[
  {
    "Title": "20x Faster TRL Fine-tuning with RapidFire AI",
    "Authors": [],
    "Publish Date": null,
    "Text": "20x Faster TRL Fine-tuning with RapidFire AI\n\nPublished November 21, 2025 Update on GitHub\n\nWhy this matters\n\nHugging Face TRL now officially integrates with RapidFire AI to accelerate your fine-tuning and post-training experiments. TRL users can now discover, install, and run RapidFire AI as the fastest way to compare multiple fine-tuning/post-training configurations to customize LLMs without major code changes and without bloating GPU requirements.\n\nWhen fine-tuning or post-training LLMs, teams often do not have the time and/or budget to compare multiple configs even though that can significantly boost eval metrics. RapidFire AI lets you launch multiple TRL configs concurrently--even on a single GPU--and compare them in near real time via a new adaptive, chunk-based scheduling and execution scheme. In internal benchmarks referenced in the TRL page, this delivers ~16\u201324\u00d7 higher experimentation throughput than sequentially comparing configs one after another, enabling you to reach much better metrics much faster.\n\nRapidFire AI establishes live three-way communication between your IDE, a metrics dashboard, and a multi-GPU execution backend\n\nWhat you get, out of the box\n\nDrop-in TRL wrappers \u2014 Use RFSFTConfig , RFDPOConfig , and RFGRPOConfig as near-zero-code replacements for TRL's SFT/DPO/GRPO configs.\n\nAdaptive chunk-based concurrent training \u2014 RapidFire AI shards the dataset into a given number of chunks and cycles configs at chunk boundaries to enable earlier apples-to-apples comparisons and also maximize GPU utilization.\n\nInteractive Control Ops (IC Ops) \u2014 From the dashboard itself, you can Stop, Resume, Delete, and Clone-Modify, possibly with Warm-Start, any runs in flight to avoid wasting resources on underperforming configs and double-down on better performing configs--no job restarts, no juggling separate GPUs or clusters, no resource bloat.\n\nClone promising configurations with modified hyperparameters, optionally warm-starting from the parent's weights, all from the live dashboard\n\nMulti-GPU orchestration \u2014 The RapidFire AI scheduler automatically places and orchestrates configs across available GPUs on chunks of data via effcient shared-memory mechanisms. You focus on your models and eval metrics, not plumbing.\n\nMLflow-based dashboard \u2014 Real-time metrics, logs, and IC Ops in one place as soon as you start your experiment. Support for more dashboards such as Trackio, W&B, and TensorBoard coming soon.\n\nHow it works\n\nRapidFire AI splits your dataset randomly into \"chunks\" and cycles LLM configurations through the GPUs at chunk boundaries. You get incremental signal on eval metrics across all configs much more quickly. The automatic checkpointing via an efficient shared-memory-based adapter/model spilling/loading mechanism keeps training smooth, stable, and consistent. Use IC Ops to adapt mid-flight to stop low-performers earlier and clone promising ones with tweaked config knobs, optionally warm-starting from the parent's weights.\n\nSequential vs. Task Parallel vs. RapidFire AI: The adaptive scheduler maximizes GPU utilization across multiple configs and GPUs. The bottom row shows IC Ops in action\u2014stopping, cloning, and modifying runs mid-flight.\n\nGetting Started\n\nInstall RapidFire AI and get running in under a minute:\n\npip install rapidfireai huggingface-cli login --token YOUR_TOKEN pip uninstall -y hf-xet rapidfireai init rapidfireai start\n\nThe dashboard launches at http://localhost:3000 where you can monitor and control all your experiments.\n\nSupported TRL trainers\n\nSFT with RFSFTConfig\n\nDPO with RFDPOConfig\n\nGRPO with RFGRPOConfig\n\nThese are designed as drop-in replacements so that you can keep your TRL mental model while gaining far more concurrency and control for your fine-tuning/post-training applications.\n\nMinimal TRL SFT example\n\nHere's what it looks like to train multiple configurations concurrently even on a single GPU:\n\nfrom rapidfireai import Experiment from rapidfireai.automl import List , RFGridSearch, RFModelConfig, RFLoraConfig, RFSFTConfig from datasets import load_dataset from transformers import AutoModelForCausalLM, AutoTokenizer dataset = load_dataset( \"bitext/Bitext-customer-support-llm-chatbot-training-dataset\" ) train_dataset = dataset[ \"train\" ].select( range ( 128 )).shuffle(seed= 42 ) def formatting_function ( row ): return { \"prompt\" : [ { \"role\" : \"system\" , \"content\" : \"You are a helpful customer support assistant.\" }, { \"role\" : \"user\" , \"content\" : row[ \"instruction\" ]}, ], \"completion\" : [{ \"role\" : \"assistant\" , \"content\" : row[ \"response\" ]}] } dataset = dataset. map (formatting_function) config_set = List ([ RFModelConfig( model_name= \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\" , peft_config=RFLoraConfig(r= 8 , lora_alpha= 16 , target_modules=[ \"q_proj\" , \"v_proj\" ]), training_args=RFSFTConfig(learning_rate= 1e-3 , max_steps= 128 , fp16= True ), ), RFModelConfig( model_name= \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\" , peft_config=RFLoraConfig(r= 32 , lora_alpha= 64 , target_modules=[ \"q_proj\" , \"v_proj\" ]), training_args=RFSFTConfig(learning_rate= 1e-4 , max_steps= 128 , fp16= True ), formatting_func=formatting_function, ) ]) experiment = Experiment(experiment_name= \"sft-comparison\" ) config_group = RFGridSearch(configs=config_set, trainer_type= \"SFT\" ) def create_model ( model_config ): model = AutoModelForCausalLM.from_pretrained( model_config[ \"model_name\" ], device_map= \"auto\" , torch_dtype= \"auto\" ) tokenizer = AutoTokenizer.from_pretrained(model_config[ \"model_name\" ]) return (model, tokenizer) experiment.run_fit(config_group, create_model, train_dataset, num_chunks= 4 , seed= 42 ) experiment.end()\n\nWhat happens when you run this?\n\nSuppose you run the above on a 2-GPU machine. Instead of training sequentially (Config 1 \u2192 wait \u2192 Config 2 \u2192 wait), both configs train concurrently:\n\nApproach Time till Comparative Decision GPU utilization Sequential (traditional) ~15 minutes 60% utilization RapidFire AI (concurrent) ~5 minutes 95%+ utilization\n\nYou can get to a comparative decision 3\u00d7 sooner on the same resources after both configs finish processing the first data chunk instead of waiting for them to see the whole dataset one after another. Open http://localhost:3000 to watch live metrics and use IC Ops to stop, clone, or tweak runs in real-time based on what you're seeing.\n\nBenchmarks: Real-World Speedups\n\nHere is what teams see on time to reach a comparable overall best training loss (across all tried configs) when switching from sequential comparisons to RapidFire AI-enabled hyperparallel experimentation:\n\nScenario Sequential Time RapidFire AI Time Speedup 4 configs, 1 GPU 120 min 7.5 min 16\u00d7 8 configs, 1 GPU 240 min 12 min 20\u00d7 4 configs, 2 GPUs 60 min 4 min 15\u00d7\n\nBenchmarks on NVIDIA A100 40GB with TinyLlama-1.1B and Llama-3.2-1B models\n\nGet Started Today\n\n\ud83d\ude80 Try it hands-on: Interactive Colab Notebook \u2014 Zero setup, runs in your browser\n\n\ud83d\udcda Full Documentation: oss-docs.rapidfire.ai \u2014 Complete guides, examples, and API reference\n\n\ud83d\udcbb GitHub: RapidFireAI/rapidfireai \u2014 Open source, production-ready\n\n\ud83d\udce6 Install via PyPI: pypi.org/project/rapidfireai \u2014 pip install rapidfireai\n\n\ud83d\udcac Join the Community: Discord \u2014 Get help, share results, request features\n\nRapidFire AI was built because the common status quo of trying one config at a time wastes both time and GPU cycles. With this official integration, every TRL user can fine-tune/post-train smarter, iterate faster, and ship better models.\n\nTry the integration and let us know: How much faster is your experimentation loop? What should we build next? We're just getting started, and your feedback shapes where we go from here.",
    "link": "https://huggingface.co/blog/rapidfireai",
    "Summary": "20x Faster TRL Fine-tuning with RapidFire AIPublished November 21, 2025 Update on GitHubWhy this mattersHugging Face TRL now officially integrates with RapidFire AI to accelerate your fine-tuning and post-training experiments.\nTRL users can now discover, install, and run RapidFire AI as the fastest way to compare multiple fine-tuning/post-training configurations to customize LLMs without major code changes and without bloating GPU requirements.\nRapidFire AI lets you launch multiple TRL configs concurrently--even on a single GPU--and compare them in near real time via a new adaptive, chunk-based scheduling and execution scheme.\nTask Parallel vs. RapidFire AI: The adaptive scheduler maximizes GPU utilization across multiple configs and GPUs.\n}, { \"role\" : \"user\" , \"content\" : row[ \"instruction\" ]}, ], \"completion\" : [{ \"role\" : \"assistant\" , \"content\" : row[ \"response\" ]}] } dataset = dataset.",
    "Keywords": [
      "trl",
      "configs",
      "ops",
      "dataset",
      "metrics",
      "faster",
      "ai",
      "gpu",
      "20x",
      "finetuning",
      "rapidfireai",
      "rapidfire",
      "min"
    ]
  },
  {
    "Title": "Continuous batching from first principles",
    "Authors": [],
    "Publish Date": null,
    "Text": "Continuous batching\n\nPublished November 25, 2025 Update on GitHub\n\nTL;DR: in this blog post, starting from attention mechanisms and KV caching, we derive continuous batching by optimizing for throughput.\n\nIf you've ever used Qwen, Claude, or any other AI chatbot, you've probably noticed something: it takes a while for the first word of the response to appear, and then words appear one-by-one on your screen with (hopefully) a regular and fast-paced frequency. That's because at the heart of it, all LLMs are just fancy next token predictors. An LLM first processes your entire prompt to produce one new token. Then it keeps adding tokens one by one, each time reading everything that came before, until it decides generation is over.\n\nThis generation process is computationally expensive: it requires passing the input through billions of parameters for each token generated. To make these models practical for real-world applications, particularly when serving many users simultaneously, researchers and engineers have developed a range of efficient inference techniques.\n\nOne of the most impactful optimizations is continuous batching, which attempts to maximize performance by processing multiple conversations in parallel and swapping them out when they are done.\n\nTo understand how continuous batching works and why it's so effective in high-load serving scenarios, we'll build up from the fundamentals of how LLMs process tokens.\n\nAttention\n\nThe attention mechanism is the central piece of how LLMs work. A language model processes text by breaking it down into pieces that we call tokens. We can conceptually think of \"tokens\" as \"words\", but sometimes a word might be composed of several tokens. For each token sequence, the network computes a prediction of what the next token should be.\n\nMany operations in the network are token-wise: each token is processed independently, and the output for a given token depends only on that token's content, not on any other tokens in the sequence. Operations like this include layer normalization or matrix multiplication. However, to create connections between words in a sentence, we need operations where tokens can influence each other.\n\nThis is where attention comes in. Attention layers are the only place where different tokens interact with each other. Understanding how a network connects tokens together means understanding attention.\n\nLet's see how this works in practice, in the case where there is only one input prompt.\n\nConsider the initial prompt I am sure this project , tokenized as 7 tokens: [<bos>, I, am, sure, this, pro, ject] . The <bos> , or \"Beginning of Sequence\", is a special token we add at the start of the prompt to tell the language model that a new conversation starts here.\n\nEach token is represented inside the network with a vector of length d (the hidden dimension). Therefore, the seven incoming tokens form a tensor x x x with shape [ 1 , 7 , d ] \\left[1, 7, d \\right] [1,7,d]. 1 is the number of sequences, or batch size, which is just one in our case. 7 is the sequence length, and d is the hidden dimension, or the size of each token representation. Going forward, we'll use n n n instead of 7 as the sequence length.\n\nInput tensor x x x is then projected by three matrices: the query projection W q W_q Wq\u200b, the key projection W k W_k Wk\u200b and the value projection W v W_v Wv\u200b. This produces three tensors Q Q Q, K K K and V V V, all of shape [ 1 , n , A ] \\left[1, n , A \\right] [1,n,A], where A A A is the dimension of the attention head. We call them the query, key and value states, respectively. This is represented on the left in the figure below.\n\nNext, tensors Q Q Q and K K K are multiplied together to measure similarity between tokens, producing a tensor of shape [ 1 , n , n ] \\left[ 1, n , n \\right] [1,n,n]. This is why we say that attention has quadratic complexity in sequence length. Computing Q K T QK^T QKT requires O ( n 2 d ) \\mathcal{O} \\left( n^2 d \\right) O(n2d) operations, so the cost is a square of n n n the sequence length. It is represented on the right in the figure above.\n\nWe then apply a boolean attention mask to Q K T QK^T QKT to control which tokens can interact, as represented in the figure below. In this figure, the attention mask is a causal mask, meaning each token only interacts with tokens that came before it. This follows the intuition that a cause must come before its consequence, hence the name causal mask. The attention mask is crucial because it dictates all token interactions in the network. Set all attention mask values to False and no token will ever interact with another in the whole network. We'll examine attention masks more closely in a few paragraphs.\n\nFinally, after applying the attention mask, we take a token-wise softmax (which is the same as saying a row-wise softmax) and multiply the result by the value projection V V V to get the output of one attention head, of shape [ 1 , n , A ] \\left[ 1, n , A \\right] [1,n,A]. We offer a visual summary of the whole process in the following figure.\n\nWe are going to use a lot of attention visualization in this post, so to simplify things, we are going to condense the figure above just a bit.\n\nWhy this matters: In continuous batching, Q Q Q, K K K, and V V V can have different numbers of tokens because, as we'll see, we'll be processing different stages (prefill and decode) at the same time. To make it more general, let's say Q Q Q has shape [ 1 , n Q , A ] \\left[1, n_Q , A \\right] [1,nQ\u200b,A], K K K has shape [ 1 , n K , A ] \\left[ 1, n_K , A \\right] [1,nK\u200b,A], and V V V has shape [ 1 , n V , A ] \\left[ 1, n_V , A \\right] [1,nV\u200b,A].\n\nThe attention scores Q K T QK^T QKT then have shape [ 1 , n Q , n K ] \\left[ 1, n_Q , n_K \\right] [1,nQ\u200b,nK\u200b], and the attention mask has the same shape since it's applied point-wise to the scores.\n\nAfter applying the attention mask and row-wise softmax, we multiply by V V V. Since we're multiplying a matrix of shape [ 1 , n Q , n K ] \\left[ 1, n_Q , n_K \\right] [1,nQ\u200b,nK\u200b] by one of shape [ 1 , n V , A ] \\left[ 1, n_V , A \\right] [1,nV\u200b,A], the inner dimensions must match: n K = n V n_K = n_V nK\u200b=nV\u200b. This means V V V and K K K always have the same length, so we can simplify our visualizations by only showing K K K.\n\nDon't worry if this seems abstract: the figures will make it concrete.\n\nFurthermore, since we know that the attention mask is applied to Q K T QK^T QKT, we know they have the same shape. Instead of representing the attention scores, we will represent the attention mask in its place. Finally, since Q Q Q, K K K and V V V are direct projections of x x x, no need to represent x x x. This gives the simplified figure where we only represent Q Q Q, K K K and the attention mask:\n\nThis representation also underlines how we can read an attention mask.\n\nWe read the mask row-by-row, which is the same as reading token-by-token: each row corresponds to one token's attention computation. A green square at position (row i, column j) means True : token j can influence token i. A white square means False : no interaction allowed.\n\nFor example, look at the third row for token \"am\". The \"I\" column is green, so \"I\" influences the computation of \"am\". The \"pro\" column is white, so \"pro\" doesn't influence \"am\" . This is causal masking at work: future tokens can't affect past ones.\n\nThe last layer of the model outputs a token prediction for each input token. In our context, generating the continuation of a single prompt, we only care about the next token prediction from the last token. The last token is \"ject\" in the figure above, and the associated prediction is \"will\".\n\nThe process we just described, where we take an entire input sequence, pass it through multiple attention layers and compute a score for the next token, is called prefill. This is because, as we'll see in a moment, much of the computation we performed can be cached and reused \u2013 hence, we are prefilling the cache. Thanks to the use of this cache, sequence generation can proceed using much less compute in a phase called decoding. In the decoding phase, generating one new token will be much faster than the initial full-sequence computation. Let's see why.\n\nTo continue generation, we begin a new forward pass, which would naively look like this:\n\nTo compute the attention scores of the new token, we still need the key and value projections of the previous tokens. So we need to repeat the matrix multiplication of the old tokens (in grey in the figure above) with W k W_k Wk\u200b and W v W_v Wv\u200b to retrieve a result that was already computed once before. In other terms, we are wasting compute. Let's see how we can avoid that.\n\nRight off the bat, we notice that the last token does not impact the attention calculation of the other tokens:\n\nThis follows the idea of the causal mask: since \"will\" comes after all previous tokens, it does not change their attention calculation. For text generation, causal attention is by far the most common, so we will focus on that case from now on. Keep in mind that non-causal attention schemes can also be used, especially when dealing with images. Considering we only need the next-token prediction for the \"will\" token, we can simplify the attention mechanism by only computing the output for this token.\n\nMoreover, we already computed the K K K and V V V states for the tokens \"<bos>\", \u2026 , \"ject\" during the previous forward pass: if they have been stored, we do not need to recompute them again. This is the KV cache: the list of key and value states created during generation. It essentially allows one to reduce the compute cost of generating token n + 1 n+1 n+1 from O ( n 2 ) \\mathcal{O} \\left( n^2 \\right) O(n2) to O ( n ) \\mathcal{O} \\left( n \\right) O(n) by avoiding recomputation of key and value projections, while paying a memory cost of O ( n ) \\mathcal{O} \\left( n \\right) O(n).\n\nIn the figure above, only the tokens in white are computed: instead of computing the keys and values for 8 tokens, we compute them for 1. You can see that through KV caching, a lot of compute is saved.\n\nYou can check this post for more visualizations of KV caching, or this one for a practical implementation example.\n\nLet's be a bit more specific about the cache size, because it's a good opportunity to examine the shapes present in our model. For a model with L \\mathcal L L attention layers and H H H attention heads with head dimension A A A, the total cache size needed to store one token will be 2 \u2217 L \u2217 A H 2 *\\mathcal L * AH 2\u2217L\u2217AH with a factor of 2 2 2 to account for both K K K and V V V.\n\nFor instance, Llama-2-7B with L = 32 \\mathcal{L}=32 L=32 layers, H = 32 H=32 H=32 heads, and A = 128 A=128 A=128 requires 2 \u00d7 32 \u00d7 128 = 8 , 192 2 \\times 32 \\times 128 = 8,192 2\u00d732\u00d7128=8,192 values per token per layer. With float16 precision, this takes 2 A H \u00d7 2 2AH \\times 2 2AH\u00d72 bytes = 16 = 16 =16 KB in memory.\n\nKV caching is useful when we want to generate the next token, which is a stage we call decoding. But it can also be useful in the prefill stage, when we process the initial prompt and have many input tokens. Especially when there are large initial prompts that don't fit in GPU memory all at once.\n\nChunked prefill\n\nUp till now, we have looked at an example of prefill where we have n = 7 n=7 n=7 tokens, but in practice initial prompts can be much longer. For instance, when using Cursor, you can add your repository to the prompt, where it acts as context: this significantly increases the prompt size. In such cases, the memory needed to store the activations for n n n tokens can be larger than the available memory on the GPU. Thus we cannot perform prefill in a single forward pass: we have to split the prefill in chunks. This is called chunked prefill, and it's going to be one of the components needed to enable efficient inference.\n\nLet's pretend that the available memory is very constrained, and that we can only pass m = 4 m=4 m=4 tokens per forward pass. If we have an initial prompt with n = 7 n = 7 n=7 tokens, we need to split it in \u2308 n / m \u2309 = 2 \\lceil n /m \\rceil = 2 \u2308n/m\u2309=2 chunks (rounding up 7/4 = 1.75 to 2). We illustrate the example below using the same n n n and m m m notations:\n\nWe can do that thanks to the KV cache. We store the KV states during the first prefill split, and during the second prefill split, we prepend the stored KV states to the new KV states. We also adapt the attention mask accordingly. Visually, it looks like we split the non-chunked prefill in the middle.\n\nThe key insight: cached KV states let us process the prompt incrementally without losing information.\n\nAlthough we showed here an example where we split the prefill into 2 chunks, chunked prefill can be used to split the prefill in any way we want, adapting flexibly to memory constraints.\n\nWe are now finally equipped with all the tools we need to understand Continuous Batching.\n\nContinuous batching\n\nIn our previous examples we have only considered the case of batch size one, i.e. we only generate tokens for one prompt at a time. In the context of evaluation or model serving, we want to generate tokens for a large number of prompts. To increase the throughput, which is the number of tokens generated per second, the best course of action is to generate tokens in parallel for a batch of several prompts.\n\nTo batch prompts together, the naive way is to add an axis to both input tensors: token sequence and attention mask. However, this comes with a constraint on the shape of the inputs: we need all prompts to have the same length, because tensors must be rectangular. To achieve this, we usually add padding on the left so the new token prediction always comes from the rightmost token. We also modify the attention mask of each prompt accordingly, as shown below:\n\nwhere the padding tokens <pad> are coloured in orange. Then we can perform the forward pass as we used to, with the added dimension of the batch size. This is called batched generation: efficient for same-length prompts, but wasteful when lengths vary.\n\nIt is illustrated below, through 4 steps of generation: one prefilling step (at the top) and 3 decoding steps (below each \"Forward pass\" lines).\n\nwhere <eos> means \"End Of Sequence\", this is a special token to indicate the model has reached the end of generation for the corresponding sequence.\n\nThe drawback of batched generation is that if one prompt finishes generation before the other one by generating an <eos> token, all further generated tokens are useless. And this goes on until the longest request of the batch finishes. Of course, we can remove the prompts that have reached an <eos> token from the batch and save some compute and memory, but saving resources is not the goal here: throughput is.\n\nInstead of just removing the finished prompt from the batch, we can replace it with a prompt that's waiting for generation. We will call this dynamic scheduling, or dynamic batching. Dynamic scheduling is great to maintain throughput while ensuring any token generated by a forward pass is relevant. But because of the way we batched prompts together, it has a major drawback: we need a lot of padding when swapping prompts. That's because the newly-inserted prompt needs to go through prefill while the other prompts are decoding one token at a time. So there is almost as much padding as there are tokens in the newly-inserted prompt.\n\nThe problem becomes even worse when batch size increases and initial prompts are long. The padding cost grows quadratically with both batch size and prompt length. If we have a batch of B B B prompts that are in decoding phase and one finishes, dynamically introducing a prompt of n n n initial tokens in the batch requires ( n \u2212 1 ) ( B \u2212 1 ) (n-1)(B-1) (n\u22121)(B\u22121) padding tokens. For instance, with B = 8 B=8 B=8 and n = 100 n=100 n=100, we'd need 99 \u00d7 7 = 693 99 \\times 7 = 693 99\u00d77=693 padding tokens!\n\nFurthermore, practical optimizations like CUDA graphs or torch.compile require static tensor shapes. This forces us to pad all prompts to a fixed maximum length, dramatically increasing the padding waste.\n\nAt this point, our main problem is padding, which is a consequence of the axis we added to batch sentences together. Thus, the ideal would be to get rid of this axis entirely, a radical rethinking of batching. If we do so, the only way to batch prompts together is to concatenate them:\n\nBut we don't want tokens from prompt 0 to interact with the tokens of prompt 1! Luckily for us, we have a way to control how tokens interact with one another: the attention mask. How we do this is displayed below:\n\nAlthough we use different tints of green to illustrate the different parts of the attention mask, this is still a boolean mask with only greens for True and white for False . This way of batching prompts together is called ragged batching (because sequence lengths are 'ragged' or uneven), and it offers the benefit of added throughput without introducing the need for padding tokens.\n\nIn the figure above, we use ragged batching to combine two full prompts together, but we can batch as many as memory allows. The only limit is m m m, the number of tokens we can fit in a batch, with m m m depending on the available memory on the GPU.\n\nRagged batching is one of the key components of continuous batching. To maximize throughput, we can combine prefill and decoding sequences following an algorithm like this:\n\nWe try to always reach our memory budget of m m m tokens per batch\n\ntokens per batch We first add all the prompts in decoding phase to the batch, each accounting for 1 token\n\nWe fill the remaining space with prefill phase prompts, relying on the flexibility of chunked prefill to split inputs as needed\n\nDynamic scheduling is the final piece that contributes to the continuous batching technique: we remove finished prompts from the batch as soon as they are done, and replace them with new chunked prompts that correspond to incoming requests.\n\nThis combination of ragged batching and dynamic scheduling is called continuous batching, and it's the technique that powers modern LLM serving systems.\n\nConclusion\n\nContinuous batching combines three key techniques to maximize throughput in LLM serving:\n\nKV caching to avoid recomputing past token representations Chunked prefill to handle variable-length prompts within memory constraints Ragged batching with dynamic scheduling to eliminate padding waste and keep the GPU fully utilized\n\nBy removing the batch dimension and using attention masks to control token interactions, continuous batching allows mixing prefill and decode phases in the same batch, dramatically improving efficiency for serving multiple requests. This is why services like ChatGPT can handle thousands of concurrent users efficiently.\n\nIn the next article in this series, we'll explore efficient KV cache management through paged attention. If you'd like to see a deep dive on other continuous batching topics, please let us know in the comments!\n\nAcknowledgement: thanks to Arthur Zucker for producing the initial concept for the figures used in this article. And thanks to Arthur Zucker, Luc Georges, Lysandre Debut, Merve Noyan and Pedro Cuenca for all providing helpful reviews.",
    "link": "https://huggingface.co/blog/continuous_batching",
    "Summary": "Going forward, we'll use n n n instead of 7 as the sequence length.\nWhy this matters: In continuous batching, Q Q Q, K K K, and V V V can have different numbers of tokens because, as we'll see, we'll be processing different stages (prefill and decode) at the same time.\nThis means V V V and K K K always have the same length, so we can simplify our visualizations by only showing K K K.Don't worry if this seems abstract: the figures will make it concrete.\nFinally, since Q Q Q, K K K and V V V are direct projections of x x x, no need to represent x x x.\nWe illustrate the example below using the same n n n and m m m notations:We can do that thanks to the KV cache.",
    "Keywords": [
      "q",
      "prompts",
      "batch",
      "k",
      "prompt",
      "batching",
      "attention",
      "principles",
      "tokens",
      "v",
      "n",
      "token",
      "continuous"
    ]
  },
  {
    "Title": "Diffusers welcomes FLUX-2",
    "Authors": [],
    "Publish Date": null,
    "Text": "Welcome FLUX.2 - BFL\u2019s new open image generation model \ud83e\udd17\n\nPublished November 25, 2025 Update on GitHub\n\n\ud83d\udea8 FLUX.2 is not meant to be a drop-in replacement of FLUX.1, but a new image generation and editing model.\n\nFLUX.2 is the recent series of image generation models from Black Forest Labs, preceded by the Flux.1 series. It is an entirely new model with aand pre-training done from scratch! In this post, we discuss the key changes introduced in FLUX.2, performing inference with it under various setups, and LoRA fine-tuning.\n\nTable of contents\n\nFLUX.2: A Brief Introduction\n\nFLUX.2 can be used for both image-guided and text-guided image generation. Furthermore, it can take multiple images as reference inputs, while producing the final output image. Below, we briefly discuss the key changes introduced in FLUX.2.\n\nText encoder\n\nFirst, instead of two text encoders as in Flux.1, it uses a single text encoder \u2014 Mistral Small 3.1. Using a single text encoder greatly simplifies the process of computing prompt embeddings. The pipeline allows for a max_sequence_length of 512. Instead of using a single-layer output for the prompt embedding, FLUX.2 stacks outputs from intermediate layers, which have been known to be more beneficial.\n\nDiT\n\nFLUX.2 follows the same general multimodel diffusion transformer (MM-DiT) + parallel DiT architecture as Flux.1. As a refresher, MM-DiT blocks first process the image latents and conditioning text in separate streams, only joining the two together for the attention operation, and are thus referred to as \u201cdouble-stream\u201d blocks. The parallel blocks then operate on the concatenated image and text streams and can be regarded as \u201csingle-stream\u201d blocks.\n\nThe key DiT changes from Flux.1 to FLUX.2 are as follows:\n\nTime and guidance information (in the form of AdaLayerNorm-Zero modulation parameters) is shared across all double-stream and single-stream transformer blocks, respectively, rather than having individual modulation parameters for each block as in Flux.1.\n\nNone of the layers in the model use bias parameters. In particular, neither the attention nor feedforward (FF) sub-blocks of either transformer block use bias parameters in any of their layers.\n\nIn Flux.1, the single-stream transformer blocks fused the attention output projection with the FF output projection. FLUX.2 single-stream blocks also fuse the attention QKV projections with the FF input projection, creating a fully parallel transformer block: Figure taken from the ViT-22B paper. Note that compared to the ViT-22B block depicted above, FLUX.2 uses a SwiGLU-style MLP activation rather than a GELU activation (and also doesn\u2019t use bias parameters).\n\nA larger proportion of the transformer blocks in FLUX.2 are single-stream blocks ( 8 double-stream blocks to 48 single-stream blocks, compared to 19 / 38 for Flux.1). This also means that single-stream blocks make up a larger proportion of the DiT parameters: Flux.1[dev]-12B has ~54% of its total parameters in the double-stream blocks, whereas FLUX.2[dev]-32B has ~24% of its parameters in the double-stream blocks (and ~73% in the single-stream blocks).\n\nMisc\n\nA new Autoencoder aka AutoencoderKLFlux2\n\nBetter way to incorporate resolution-dependent timestep schedules\n\nInference With Diffusers\n\nFLUX.2 uses a larger DiT and Mistral3 Small as its text encoder. When used together without any kind of offloading, the inference takes more than 80GB VRAM. In the following sections, we show how to perform inference with FLUX.2 in more accessible ways, under various system-level constraints.\n\nInstallation and Authentication\n\nBefore you try out the following code snippets, make sure you have installed diffusers from main and have run hf auth login .\n\npip uninstall diffusers -y && pip install git+https://github.com/huggingface/diffusers -U\n\nRegular Inference\n\nfrom diffusers import Flux2Pipeline import torch repo_id = \"black-forest-labs/FLUX.2-dev\" pipe = Flux2Pipeline.from_pretrained(repo_id, torch_dtype=torch.bfloat16) pipe.enable_model_cpu_offload() image = pipe( prompt= \"dog dancing near the sun\" , num_inference_steps= 50 , guidance_scale= 4 , height= 1024 , width= 1024 ).images[ 0 ]\n\nThe above code snippet was tested on an H100, and it isn\u2019t sufficient to run inference on it without CPU offloading. With CPU offloading enabled, this setup takes ~62GB to run.\n\nUsers who have access to Hopper-series GPUs can take advantage of Flash Attention 3 to speed up inference:\n\nfrom diffusers import Flux2Pipeline import torch repo_id = \"black-forest-labs/FLUX.2-dev\" pipe = Flux2Pipeline.from_pretrained(path, torch_dtype=torch.bfloat16) + pipe.transformer.set_attention_backend(\"_flash_3_hub\") pipe.enable_model_cpu_offload() image = pipe( prompt=\"dog dancing near the sun\", num_inference_steps=50, guidance_scale=2.5, height=1024, width=1024 ).images[0]\n\nYou can check out the supported attention backends (we have many!) here.\n\nUsing 4-bit quantization\n\nUsing bitsandbytes , we can load the transformer and text encoder models in 4-bit, allowing owners of 24GB GPUs to use the model locally. You can run this snippet on a GPU with ~20 GB of free VRAM.\n\nUnfold import torch from transformers import Mistral3ForConditionalGeneration from diffusers import Flux2Pipeline, Flux2Transformer2DModel repo_id = \"diffusers/FLUX.2-dev-bnb-4bit\" device = \"cuda:0\" torch_dtype = torch.bfloat16 transformer = Flux2Transformer2DModel.from_pretrained( repo_id, subfolder= \"transformer\" , torch_dtype=torch_dtype, device_map= \"cpu\" ) text_encoder = Mistral3ForConditionalGeneration.from_pretrained( repo_id, subfolder= \"text_encoder\" , dtype=torch_dtype, device_map= \"cpu\" ) pipe = Flux2Pipeline.from_pretrained( repo_id, transformer=transformer, text_encoder=text_encoder, torch_dtype=torch_dtype ) pipe.enable_model_cpu_offload() prompt = \"Realistic macro photograph of a hermit crab using a soda can as its shell, partially emerging from the can, captured with sharp detail and natural colors, on a sunlit beach with soft shadows and a shallow depth of field, with blurred ocean waves in the background. The can has the text `BFL Diffusers` on it and it has a color gradient that start with #FF5733 at the top and transitions to #33FF57 at the bottom.\" image = pipe( prompt=prompt, generator=torch.Generator(device=device).manual_seed( 42 ), num_inference_steps= 50 , guidance_scale= 4 , ).images[ 0 ] image.save( \"flux2_t2i_nf4.png\" )\n\nNotice that we're using a repository that contains the NF4-quantized versions of the FLUX.2 DiT and the Mistral text encoder.\n\nLocal + remote\n\nDue to the modular design of a Diffusers pipeline, we can isolate modules and work with them in sequence. We decouple the text encoder and deploy it to an Inference Endpoint. This helps us with freeing up the VRAM usage for the DiT and VAE only.\n\n\u26a0\ufe0f To use the remote text encoder, you need to have a valid token. If you are already authenticated, no further action is needed.\n\nThe example below uses a combination of local and remote inference. Additionally, we quantize the DiT with NF4 quantization through bitsandbytes .\n\nYou can run this snippet on a GPU with 18 GB of VRAM:\n\nUnfold from diffusers import Flux2Pipeline, Flux2Transformer2DModel from diffusers import BitsAndBytesConfig as DiffBitsAndBytesConfig from huggingface_hub import get_token import requests import torch import io def remote_text_encoder ( prompts: str | list [ str ] ): def _encode_single ( prompt: str ): response = requests.post( \"https://remote-text-encoder-flux-2.huggingface.co/predict\" , json={ \"prompt\" : prompt}, headers={ \"Authorization\" : f\"Bearer {get_token()} \" , \"Content-Type\" : \"application/json\" } ) assert response.status_code == 200 , f\" {response.status_code=} \" return torch.load(io.BytesIO(response.content)) if isinstance (prompts, ( list , tuple )): embeds = [_encode_single(p) for p in prompts] return torch.cat(embeds, dim= 0 ) return _encode_single(prompts).to( \"cuda\" ) repo_id = \"black-forest-labs/FLUX.2-dev\" quantized_dit_id = \"diffusers/FLUX.2-dev-bnb-4bit\" dit = Flux2Transformer2DModel.from_pretrained( quantized_dit_id, subfolder= \"transformer\" , torch_dtype=torch_dtype, device_map= \"cpu\" ) pipe = Flux2Pipeline.from_pretrained( repo_id, text_encoder= None , transformer=dit, torch_dtype=torch.bfloat16, ) pipe.enable_model_cpu_offload() print ( \"Running remote text encoder \u2601\ufe0f\" ) prompt1 = \"a photo of a forest with mist swirling around the tree trunks. The word 'FLUX.2' is painted over it in big, red brush strokes with visible texture\" prompt2 = \"a photo of a dense forest with rain. The word 'FLUX.2' is painted over it in big, red brush strokes with visible texture\" prompt_embeds = remote_text_encoder([prompt1, prompt2]) print ( \"Done \u2705\" ) out = pipe( prompt_embeds=prompt_embeds, generator=torch.Generator(device= \"cuda\" ).manual_seed( 42 ), num_inference_steps= 50 , guidance_scale= 4 , height= 1024 , width= 1024 , ) for idx, image in enumerate (out.images): image.save( f\"flux_out_ {idx} .png\" )\n\nFor GPUs with even lower VRAM, we have group_offloading , which allows GPUs with as little as 8GB of free VRAM to use this model. However, you'll need 32GB of free RAM. Alternatively, if you're willing to sacrifice some speed, you can set low_cpu_mem_usage=True to reduce the RAM requirement to just 10GB.\n\nUnfold import io import os import requests import torch from diffusers import Flux2Pipeline, Flux2Transformer2DModel repo_id = \"diffusers/FLUX.2-dev-bnb-4bit\" torch_dtype = torch.bfloat16 device = \"cuda\" def remote_text_encoder ( prompts: str | list [ str ] ): def _encode_single ( prompt: str ): response = requests.post( \"https://remote-text-encoder-flux-2.huggingface.co/predict\" , json={ \"prompt\" : prompt}, headers={ \"Authorization\" : f\"Bearer {os.environ[ 'HF_TOKEN' ]} \" , \"Content-Type\" : \"application/json\" }, ) assert response.status_code == 200 , f\" {response.status_code=} \" return torch.load(io.BytesIO(response.content)) if isinstance (prompts, ( list , tuple )): embeds = [_encode_single(p) for p in prompts] return torch.cat(embeds, dim= 0 ) return _encode_single(prompts).to( \"cuda\" ) transformer = Flux2Transformer2DModel.from_pretrained( repo_id, subfolder= \"transformer\" , torch_dtype=torch_dtype, device_map= \"cpu\" ) pipe = Flux2Pipeline.from_pretrained( repo_id, text_encoder= None , transformer=transformer, torch_dtype=torch_dtype, ) pipe.transformer.enable_group_offload( onload_device=device, offload_device= \"cpu\" , offload_type= \"leaf_level\" , use_stream= True , ) pipe.to(device) prompt = \"a photo of a forest with mist swirling around the tree trunks. The word 'FLUX.2' is painted over it in big, red brush strokes with visible texture\" prompt_embeds = remote_text_encoder(prompt) image = pipe( prompt_embeds=prompt_embeds, generator=torch.Generator(device=device).manual_seed( 42 ), num_inference_steps= 50 , guidance_scale= 4 , height= 1024 , width= 1024 , ).images[ 0 ]\n\nYou can check out other supported quantization backends here and other memory-saving techniques here.\n\nTo check how different quantizations affect an image, you can play with the playground below or access it as standlone in the FLUX.2 Quantization experiments Space\n\nMultiple images as reference\n\nFLUX.2 supports using multiple images as inputs, allowing you to use up to 10 images. However, keep in mind that each additional image will require more VRAM. You can reference the images by index (e.g., image 1, image 2) or by natural language (e.g., the kangaroo, the turtle). For optimal results, the best approach is to use a combination of both methods.\n\nUnfold import torch from transformers import Mistral3ForConditionalGeneration from diffusers import Flux2Pipeline, Flux2Transformer2DModel from diffusers.utils import load_image repo_id = \"diffusers-internal-dev/new-model-image-final-weights\" device = \"cuda:0\" torch_dtype = torch.bfloat16 pipe = Flux2Pipeline.from_pretrained( repo_id, torch_dtype=torch_dtype ) pipe.enable_model_cpu_offload() image_one = load_image( \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/flux2_blog/kangaroo.png\" ) image_two = load_image( \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/flux2_blog/turtle.png\" ) prompt = \"the boxer kangaroo from image 1 and the martial artist turtle from image 2 are fighting in an epic battle scene at a beach of a tropical island, 35mm, depth of field, 50mm lens, f/3.5, cinematic lighting\" image = pipe( prompt=prompt, image=[image_one, image_two], generator=torch.Generator(device=device).manual_seed( 42 ), num_inference_steps= 50 , guidance_scale= 2.5 , width= 1024 , height= 768 , ).images[ 0 ] image.save( f\"./flux2_t2i.png\" )\n\nMulti-image input\n\nAdvanced Prompting\n\nFLUX.2 supports advanced prompting techniques like structured JSON prompting, precise hex color control, and multi-reference image editing. Aside for the added control, this also allows for flexibility in changing specific attributes while maintaining others overall the same.\n\nFor example, let's start with this json as the base schema (taken from the official FLUX.2 prompting guide):\n\n{ \"scene\" : \"overall scene description\" , \"subjects\" : [ { \"description\" : \"detailed subject description\" , \"position\" : \"where in frame\" , \"action\" : \"what they're doing\" } ] , \"style\" : \"artistic style\" , \"color_palette\" : [ \"#hex1\" , \"#hex2\" , \"#hex3\" ] , \"lighting\" : \"lighting description\" , \"mood\" : \"emotional tone\" , \"background\" : \"background details\" , \"composition\" : \"framing and layout\" , \"camera\" : { \"angle\" : \"camera angle\" , \"lens\" : \"lens type\" , \"depth_of_field\" : \"focus behavior\" } }\n\nBuilding up on that, let's turn it into a prompt for a shot of a good old fashion walkman on a carpet (simply pass this prompt to your chosen diffusers inference example from above):\n\nprompt = \"\"\" { \"scene\": \"Professional studio product photography setup with soft-textured carpet surface\", \"subjects\": [ { \"description\": \"Old silver Walkman placed on a carpet in the middle of an empty room\", \"pose\": \"Stationary, lying flat\", \"position\": \"Center foreground on carpeted surface\", \"color_palette\": [\"brushed silver\", \"dark gray accents\"] } ], \"style\": \"Ultra-realistic product photography with commercial quality\", \"color_palette\": [\"brushed silver\", \"neutral beige\", \"soft white highlights\"], \"lighting\": \"Three-point softbox setup creating soft, diffused highlights with no harsh shadows\", \"mood\": \"Clean, professional, minimalist\", \"background\": \"Soft-textured carpet surface with subtle studio backdrop suggesting an empty room\", \"composition\": \"rule of thirds\", \"camera\": { \"angle\": \"high angle\", \"distance\": \"medium shot\", \"focus\": \"Sharp focus on metallic Walkman textures and physical controls\", \"lens-mm\": 85, \"f-number\": \"f/5.6\", \"ISO\": 200 } } \"\"\"\n\nNow, let's change the color of the carpet to a specific teal-blue shade (#367588) and add wired headphones plugged into the walkman:\n\nprompt = \"\"\" { \"scene\": \"Professional studio product photography setup with soft-textured carpet surface\", \"subjects\": [ { \"description\": \"Old silver Walkman placed on a teal-blue carpet (#367588) in the middle of an empty room, with wired headphones plugged in\", \"pose\": \"Stationary, lying flat\", \"position\": \"Center foreground on carpeted surface\", \"color_palette\": [\"brushed silver\", \"dark gray accents\", \"#367588\"] }, { \"description\": \"Wired headphones connected to the Walkman, cable loosely coiled on the carpet\", \"pose\": \"Stationary\", \"position\": \"Next to and partially in front of the Walkman on the carpet\", \"color_palette\": [\"dark gray\", \"soft black\", \"#367588\"] } ], \"style\": \"Ultra-realistic product photography with commercial quality\", \"color_palette\": [\"brushed silver\", \"#367588\", \"neutral beige\", \"soft white highlights\"], \"lighting\": \"Three-point softbox setup creating soft, diffused highlights with no harsh shadows\", \"mood\": \"Clean, professional, minimalist\", \"background\": \"Soft-textured teal-blue carpet surface (#367588) with subtle studio backdrop suggesting an empty room\", \"composition\": \"rule of thirds\", \"camera\": { \"angle\": \"high angle\", \"distance\": \"medium shot\", \"focus\": \"Sharp focus on metallic Walkman textures, wired headphones, and carpet fibers\", \"lens-mm\": 85, \"f-number\": \"f/5.6\", \"ISO\": 200 } } \"\"\"\n\nThe carpet color now matches the hex code provided, and the headphones have been with small changes to the overall scene.\n\nCheck out the official prompting guide for more examples and details.\n\nLoRA fine-tuning\n\nBeing both a text-to-image and an image-to-image model, FLUX.2 makes the perfect fine-tuning candidate for many use-cases! However, as inference alone takes more than 80GB of VRAM, LoRA fine-tuning is even more challenging to run on consumer GPUs. To squeeze in as much memory saving as we can, we utilize some of the inference optimizations described above for training as well, together with shared memory saving techniques, to substantially reduce memory consumption. To train it, you can use either the diffusers code below or Ostris' AI Toolkit.\n\nWe provide both text-to-image and image-to-image training scripts, for the purpose of this blog will focus on a text-to-image training example.\n\nMemory optimizations for fine-tuning\n\nMany of these techniques complement each other and can be used together to reduce memory consumption further. However, some techniques may be mutually exclusive, so be sure to check before launching a training run.\n\nUnfold to check details on the memory-saving techniques used: Remote Text Encoder: to leverage the remote text encoding for training, simply pass --remote_text_encoder . Note that you must either be logged in to your Hugging Face account ( hf auth login ) OR pass a token with --hub_token .\n\nto leverage the remote text encoding for training, simply pass . Note that you must either be logged in to your Hugging Face account ( ) OR pass a token with . CPU Offloading: by passing --offload the vae and text encoder to will be offloaded to CPU memory and only moved to GPU when needed.\n\nby passing the vae and text encoder to will be offloaded to CPU memory and only moved to GPU when needed. Latent Caching: Pre-encode the training images with the vae, and then delete it to free up some memory. To enable latent_caching simply pass --cache_latents .\n\nPre-encode the training images with the vae, and then delete it to free up some memory. To enable simply pass . QLoRA : Low Precision Training with Quantization - using 8-bit or 4-bit quantization. You can use the following flags: FP8 training with torchao : enable FP8 training by passing --do_fp8_training . Since we are utilizing FP8 tensor cores, we need CUDA GPUs with compute capability at least 8.9 or greater. If you're looking for memory-efficient training on relatively older cards, we encourage you to check out other trainers like SimpleTuner , ai-toolkit , etc. NF4 training with bitsandbytes : Alternatively, you can use 8-bit or 4-bit quantization with bitsandbytes by passing:- --bnb_quantization_config_path with a corresponding path to a json file containing your config. see below for more details.\n\n: Low Precision Training with Quantization - using 8-bit or 4-bit quantization. You can use the following flags: Gradient Checkpointing and Accumulation: --gradient accumulation refers to the number of updates steps to accumulate before performing a backward/update pass.by passing a value > 1 you can reduce the amount of backward/update passes and hence also memory reqs.* with --gradient checkpointing we can save memory by not storing all intermediate activations during the forward pass.Instead, only a subset of these activations (the checkpoints) are stored and the rest is recomputed as needed during the backward pass. Note that this comes at the expanse of a slower backward pass.\n\nrefers to the number of updates steps to accumulate before performing a backward/update pass.by passing a value > 1 you can reduce the amount of backward/update passes and hence also memory reqs.* with we can save memory by not storing all intermediate activations during the forward pass.Instead, only a subset of these activations (the checkpoints) are stored and the rest is recomputed as needed during the backward pass. Note that this comes at the expanse of a slower backward pass. 8-bit-Adam Optimizer: When training with AdamW (doesn't apply to prodigy ) You can pass --use_8bit_adam to reduce the memory requirements of training. Make sure to install bitsandbytes if you want to do so.\n\nPlease make sure to check out the README for prerequisites before starting training.\n\nFor this example, we\u2019ll use multimodalart/1920-raider-waite-tarot-public-domain dataset with the following configuration using FP8 training. Feel free to experiment more with the hyper-parameters and share your results \ud83e\udd17\n\naccelerate launch train_dreambooth_lora_flux2.py \\ --pretrained_model_name_or_path= \"black-forest-labs/FLUX.2-dev\" \\ --mixed_precision= \"bf16\" \\ --gradient_checkpointing \\ --remote_text_encoder \\ --cache_latents \\ --caption_column= \"caption\" \\ --do_fp8_training \\ --dataset_name= \"multimodalart/1920-raider-waite-tarot-public-domain\" \\ --output_dir= \"tarot_card_Flux2_LoRA\" \\ --instance_prompt= \"trcrd tarot card\" \\ --resolution=1024 \\ --train_batch_size=2 \\ --guidance_scale=1 \\ --gradient_accumulation_steps=1 \\ --optimizer= \"adamW\" \\ --use_8bit_adam\\ --learning_rate=1e-4 \\ --report_to= \"wandb\" \\ --lr_scheduler= \"constant_with_warmup\" \\ --lr_warmup_steps=200 \\ --checkpointing_steps=250\\ --max_train_steps=1000 \\ --rank=8\\ --validation_prompt= \"a trtcrd of a person on a computer, on the computer you see a meme being made with an ancient looking trollface, 'the shitposter' arcana, in the style of TOK a trtcrd, tarot style\" \\ --validation_epochs=25 \\ --seed= \"0\" \\ --push_to_hub\n\nLoRA finetuning Pre-trained FLUX.2 LoRA fine-tuned FLUX.2\n\nThe left image was generated using the pre-trained FLUX.2 model, and the right image was produced the LoRA.\n\nIn case your hardware isn\u2019t compatible with FP8 training, you can use QLoRA with bitsandbytes . You first need to define a config.json file like so:\n\n{ \"load_in_4bit\" : true , \"bnb_4bit_quant_type\" : \"nf4\" }\n\nAnd then pass its path to --bnb_quantization_config_path :\n\naccelerate launch train_dreambooth_lora_flux2.py \\ --pretrained_model_name_or_path= \"black-forest-labs/FLUX.2-dev\" \\ --mixed_precision= \"bf16\" \\ --gradient_checkpointing \\ --remote_text_encoder \\ --cache_latents \\ --caption_column= \"caption\" \\ **--bnb_quantization_config_path= \"config.json\" \\** --dataset_name= \"multimodalart/1920-raider-waite-tarot-public-domain\" \\ --output_dir= \"tarot_card_Flux2_LoRA\" \\ --instance_prompt= \"a tarot card\" \\ --resolution=1024 \\ --train_batch_size=2 \\ --guidance_scale=1 \\ --gradient_accumulation_steps=1 \\ --optimizer= \"adamW\" \\ --use_8bit_adam\\ --learning_rate=1e-4 \\ --report_to= \"wandb\" \\ --lr_scheduler= \"constant_with_warmup\" \\ --lr_warmup_steps=200 \\ --max_train_steps=1000 \\ --rank=8\\ --validation_prompt= \"a trtcrd of a person on a computer, on the computer you see a meme being made with an ancient looking trollface, 'the shitposter' arcana, in the style of TOK a trtcrd, tarot style\" \\ --seed= \"0\"\n\nResources",
    "link": "https://huggingface.co/blog/flux-2",
    "Summary": "FLUX.2 is the recent series of image generation models from Black Forest Labs, preceded by the Flux.1 series.\nIn this post, we discuss the key changes introduced in FLUX.2, performing inference with it under various setups, and LoRA fine-tuning.\nInstead of using a single-layer output for the prompt embedding, FLUX.2 stacks outputs from intermediate layers, which have been known to be more beneficial.\nA larger proportion of the transformer blocks in FLUX.2 are single-stream blocks ( 8 double-stream blocks to 48 single-stream blocks, compared to 19 / 38 for Flux.1).\nIn the following sections, we show how to perform inference with FLUX.2 in more accessible ways, under various system-level constraints.",
    "Keywords": [
      "import",
      "flux2",
      "image",
      "training",
      "prompt",
      "diffusers",
      "blocks",
      "welcomes",
      "text",
      "repo_id",
      "memory"
    ]
  },
  {
    "Title": "Introducing AnyLanguageModel: One API for Local and Remote LLMs on Apple Platforms",
    "Authors": [],
    "Publish Date": null,
    "Text": "Introducing AnyLanguageModel: One API for Local and Remote LLMs on Apple Platforms\n\nPublished November 20, 2025 Update on GitHub\n\nLLMs have become essential tools for building software. But for Apple developers, integrating them remains unnecessarily painful.\n\nDevelopers building AI-powered apps typically take a hybrid approach, adopting some combination of:\n\nLocal models using Core ML or MLX for privacy and offline capability\n\nCloud providers like OpenAI or Anthropic for frontier capabilities\n\nApple's Foundation Models as a system-level fallback Each comes with different APIs, different requirements, different integration patterns. It's a lot, and it adds up quickly. When I interviewed developers about building AI-powered apps, friction with model integration came up immediately. One developer put it bluntly:\n\nI thought I'd quickly use the demo for a test and maybe a quick and dirty build but instead wasted so much time. Drove me nuts.\n\nThe cost to experiment is high, which discourages developers from discovering that local, open-source models might actually work great for their use case.\n\nToday we're announcing AnyLanguageModel, a Swift package that provides a drop-in replacement for Apple's Foundation Models framework with support for multiple model providers. Our goal is to reduce the friction of working with LLMs on Apple platforms and make it easier to adopt open-source models that run locally.\n\nThe Solution\n\nThe core idea is simple: Swap your import statement, keep the same API.\n\n- import FoundationModels + import AnyLanguageModel\n\nHere's what that looks like in practice. Start with Apple's built-in model:\n\nlet model = SystemLanguageModel .default let session = LanguageModelSession (model: model) let response = try await session.respond(to: \"Explain quantum computing in one sentence\" ) print (response.content)\n\nNow try an open-source model running locally via MLX:\n\nlet model = MLXLanguageModel (modelId: \"mlx-community/Qwen3-4B-4bit\" ) let session = LanguageModelSession (model: model) let response = try await session.respond(to: \"Explain quantum computing in one sentence\" ) print (response.content)\n\nAnyLanguageModel supports a range of providers:\n\nApple Foundation Models : Native integration with Apple's system model (macOS 26+ / iOS 26+)\n\n: Native integration with Apple's system model (macOS 26+ / iOS 26+) Core ML : Run converted models with Neural Engine acceleration\n\n: Run converted models with Neural Engine acceleration MLX : Run quantized models efficiently on Apple Silicon\n\n: Run quantized models efficiently on Apple Silicon llama.cpp : Load GGUF models via the llama.cpp backend\n\n: Load GGUF models via the llama.cpp backend Ollama : Connect to locally-served models via Ollama's HTTP API\n\n: Connect to locally-served models via Ollama's HTTP API OpenAI, Anthropic, Google Gemini : Cloud providers for comparison and fallback\n\n: Cloud providers for comparison and fallback Hugging Face Inference Providers: Hundreds of cloud models powered by world-class inference providers.\n\nThe focus is on local models that you can download from the Hugging Face Hub. Cloud providers are included to lower the barrier to getting started and to provide a migration path. Make it work, then make it right.\n\nWhy Foundation Models as the Base API\n\nWhen designing AnyLanguageModel, we faced a choice: create a new abstraction that tries to capture everything, or build on an existing API. We chose the latter, using Apple's Foundation Models framework as the template.\n\nThis might seem counterintuitive. Why tie ourselves to Apple's choices? A few reasons:\n\nFoundation Models is genuinely well-designed. It leverages Swift features like macros for an ergonomic developer experience, and its abstractions around sessions, tools, and generation map well to how LLMs actually work. It's intentionally limited. Foundation Models represents something like a lowest common denominator for language model capabilities. Rather than seeing this as a weakness, we treat it as a stable foundation (hyuk hyuk). Every Swift developer targeting Apple platforms will encounter this API, so building on it directly means less conceptual overhead. It keeps us grounded. Each additional layer of abstraction takes you further from the problem you're actually solving. Abstractions are powerful, but stack too many and they become a problem in themselves.\n\nThe result is that switching between providers requires minimal code changes, and the core abstractions remain clean and predictable.\n\nPackage Traits: Include Only What You Need\n\nOne challenge with multi-backend libraries is dependency bloat. If you only want to run MLX models, you shouldn't have to pull in llama.cpp and all its dependencies.\n\nAnyLanguageModel uses Swift 6.1 package traits to solve this. You opt in to only the backends you need:\n\ndependencies: [ .package( url: \"https://github.com/mattt/AnyLanguageModel.git\" , from: \"0.4.0\" , traits: [ \"MLX\" ] ) ]\n\nAvailable traits include CoreML , MLX , and Llama (for llama.cpp / llama.swift). By default, no heavy dependencies are included. You get the base API plus cloud providers, which only require standard URLSession networking.\n\nFor Xcode projects (which don't yet support trait declarations directly), you can create a small internal Swift package that depends on AnyLanguageModel with the traits you need, then add that package as a local dependency. The README has detailed instructions.\n\nImage Support (and API Design Trade-offs)\n\nVision-language models are incredibly capable and widely used. They can describe images, extract text from screenshots, analyze charts, and answer questions about visual content. Unfortunately, Apple's Foundation Models framework doesn't currently support sending images with prompts.\n\nBuilding on an existing API means accepting its constraints. Apple will likely add image support in a future release (iOS 27, perhaps?), but vision-language models are too useful to wait for. So we've extended beyond what Foundation Models offers today.\n\nHere's an example sending an image to Claude:\n\nlet model = AnthropicLanguageModel ( apiKey: ProcessInfo .processInfo.environment[ \"ANTHROPIC_API_KEY\" ] ! , model: \"claude-sonnet-4-5-20250929\" ) let session = LanguageModelSession (model: model) let response = try await session.respond( to: \"What's in this image?\" , image: . init (url: URL (fileURLWithPath: \"/path/to/image.png\" )) )\n\nWe're taking a calculated risk here; we might design something that conflicts with Apple's eventual implementation. But that's what deprecation warnings are for. Sometimes you have to write the API for the framework that doesn't exist yet.\n\nTry It Out: chat-ui-swift\n\nTo see AnyLanguageModel in action, check out chat-ui-swift, a SwiftUI chat application that demonstrates the library's capabilities.\n\nThe app includes:\n\nApple Intelligence integration via Foundation Models (macOS 26+)\n\nHugging Face OAuth authentication for accessing gated models\n\nStreaming responses\n\nChat persistence\n\nIt's meant as a starting point: Fork it, extend it, swap in different models. See how the pieces fit together and adapt it to your needs.\n\nWhat's Next\n\nAnyLanguageModel is currently pre-1.0. The core API is stable, but we're actively working on bringing the full feature set of Foundation Models to all adapters, namely:\n\nTool calling across all providers\n\nacross all providers MCP integration for tools and elicitations\n\nfor tools and elicitations Guided generation for structured outputs\n\nfor structured outputs Performance optimizations for local inference\n\nThis library is the first step toward something larger. A unified inference API provides the scaffolding needed to build seamless agentic workflows on Apple platforms \u2014 applications where models can use tools, access system resources, and accomplish complex tasks. More on that soon. \ud83e\udd2b\n\nGet Involved\n\nWe'd love your help making this better:\n\nTry it out \u2014 Build something, kick the tires\n\n\u2014 Build something, kick the tires Share your experiences \u2014 What works? What's frustrating? We want to hear about the challenges you face integrating AI into your apps\n\n\u2014 What works? What's frustrating? We want to hear about the challenges you face integrating AI into your apps Open issues \u2014 Feature requests, bug reports, questions\n\n\u2014 Feature requests, bug reports, questions Contribute \u2014 PRs are welcome\n\nLinks\n\nWe're excited to see what you build \ud83e\uddbe",
    "link": "https://huggingface.co/blog/anylanguagemodel",
    "Summary": "Introducing AnyLanguageModel: One API for Local and Remote LLMs on Apple PlatformsPublished November 20, 2025 Update on GitHubLLMs have become essential tools for building software.\nToday we're announcing AnyLanguageModel, a Swift package that provides a drop-in replacement for Apple's Foundation Models framework with support for multiple model providers.\nWe chose the latter, using Apple's Foundation Models framework as the template.\nUnfortunately, Apple's Foundation Models framework doesn't currently support sending images with prompts.\n, model: \"claude-sonnet-4-5-20250929\" ) let session = LanguageModelSession (model: model) let response = try await session.respond( to: \"What's in this image?\"",
    "Keywords": [
      "foundation",
      "apples",
      "models",
      "apple",
      "platforms",
      "run",
      "local",
      "integration",
      "remote",
      "llms",
      "let",
      "api",
      "anylanguagemodel",
      "providers",
      "model",
      "introducing"
    ]
  },
  {
    "Title": "Easily Build and Share ROCm Kernels with Hugging Face",
    "Authors": [],
    "Publish Date": null,
    "Text": "Easily Build and Share ROCm Kernels with Hugging Face\n\nPublished November 17, 2025 Update on GitHub\n\nIntoduction\n\nCustom kernels are the backbone of high-performance deep learning, enabling GPU operations tailored precisely to your workload; whether that\u2019s image processing, tensor transformations, or other compute-heavy tasks. But compiling these kernels for the right architectures, wiring all the build flags, and integrating them cleanly into PyTorch extensions can quickly become a mess of CMake/Nix, compiler errors, and ABI issues, which is not fun. Hugging Face\u2019s kernel-builder and kernels libraries make it easy to share these kernels with the kernels-community, with support for multiple GPU and accelerator backends, including CUDA, ROCm, Metal, and XPU. This ensures your kernels are fast, portable, and seamlessly integrated with PyTorch.\n\nIn this guide, we focus exclusively on ROCm-compatible kernels and show how to build, test, and share them using kernel-builder. You\u2019ll learn how to create kernels that run efficiently on AMD GPUs, along with best practices for reproducibility, packaging, and deployment.\n\nThis ROCm-specific walkthrough is a streamlined version of the original kernel-builder guide. If you\u2019re looking for the broader CUDA-focused version, you can find it here: A Guide to Building and Scaling Production-Ready CUDA Kernels.\n\nBuild Steps\n\nWe will use the GEMM kernel from RadeonFlow_Kernels as an example. If you want to go straight to the guide, click here.\n\nAbout the kernel\n\nThis section was written by the RadeonFlow GEMM kernel authors to introduce the kernel.\n\nAuthors: ColorsWind, Zesen Liu, and Andy\n\nThe RadeonFlow GEMM kernel is a high-performance, FP8 block-wise matrix multiplication implementation optimized for the AMD Instinct MI300X GPU. GEMM (General Matrix Multiplication) is the core building block behind most deep learning workloads: given two matrices A and B, you compute their product C = A \u00d7 B. Here it\u2019s implemented in FP8, a low-precision floating-point format that trades a bit of accuracy for much higher throughput and lower memory bandwidth. This kernel was developed for the AMD Developer Challenge 2025, it was awarded the \ud83c\udfc6 Grand Prize in June 2025, recognizing its excellence in performance and innovation on AMD hardware.\n\nThe kernel operates on quantized inputs using the e4m3fnuz floating-point format and applies per-block scaling to preserve accuracy during low-precision computation. The e4m3fnuz format is an FP8 variant with 4 exponent bits and 3 mantissa bits, designed to be efficient for neural network workloads. Because FP8 has a much smaller dynamic range than FP16/FP32, we apply per-block scaling factors (a_scale and b_scale) so that each block of values is rescaled into a numerically \u201ccomfortable\u201d range before and after computation, which helps preserve accuracy despite the low precision. It takes the following arguments:\n\n(a, b, a_scale, b_scale, c)\n\nwhere a and b are the input matrices, a_scale and b_scale are the scaling factors for a and b respectively, and c is the output matrix:\n\na is K \u00d7 M in e4m3fnuz\n\nis K \u00d7 M in e4m3fnuz b is K \u00d7 N in e4m3fnuz\n\nis K \u00d7 N in e4m3fnuz a_scale is (K // 128) \u00d7 M in fp32\n\nis (K // 128) \u00d7 M in fp32 b_scale is (K // 128) \u00d7 (N // 128) in fp32\n\nis (K // 128) \u00d7 (N // 128) in fp32 c is M \u00d7 N in bf16\n\nThe kernel is precompiled for specific matrix shapes and assumes a transposed memory layout (as required by the competition). To support additional shapes or alternative memory layouts, you must modify the kernel launcher.\n\nSo now that we have a high-performance ROCm kernel, the natural question is: how do we integrate it into a real PyTorch workflow and share it with others? That\u2019s exactly what we\u2019ll cover next, using kernel-builder and kernels to structure, build, and publish the ROCm kernel.\n\nThis is a fairly technical guide, but you can still follow it step by step without understanding every detail and everything will work fine. If you\u2019re curious, you can always come back later to dig deeper into the concepts.\n\nStep 1: Project Structure\n\nThe Hugging Face Kernel Builder expects your files to be organized like this:\n\ngemm/ \u251c\u2500\u2500 build.toml \u251c\u2500\u2500 gemm \u2502 \u2514\u2500\u2500 gemm_kernel.h \u251c\u2500\u2500 flake.nix \u2514\u2500\u2500 torch-ext \u251c\u2500\u2500 torch_binding.cpp \u251c\u2500\u2500 torch_binding.h \u2514\u2500\u2500 gemm \u2514\u2500\u2500 __init__.py\n\nbuild.toml : The project manifest; it\u2019s the brain of the build process.\n\n: The project manifest; it\u2019s the brain of the build process. gemm/ : Your raw CUDA source code where the GPU magic happens.\n\n: Your raw CUDA source code where the GPU magic happens. flake.nix : The key to a perfectly reproducible build environment.\n\n: The key to a perfectly reproducible build environment. torch-ext/gemm/: The Python wrapper for the raw PyTorch operators\n\nSometimes your project might depend on other files, like tests or helper scripts, and you can add them without any issues. In our case, our project will be structured like this:\n\ngemm/ \u251c\u2500\u2500 build.toml \u251c\u2500\u2500 gemm \u2502 \u251c\u2500\u2500 gemm_kernel.h \u2502 \u251c\u2500\u2500 gemm_kernel_legacy.h \u2502 \u251c\u2500\u2500 transpose_kernel.h \u2502 \u2514\u2500\u2500 gemm_launcher.hip \u251c\u2500\u2500 include \u2502 \u251c\u2500\u2500 clangd_workaround.h \u2502 \u251c\u2500\u2500 gpu_libs.h \u2502 \u251c\u2500\u2500 gpu_types.h \u2502 \u2514\u2500\u2500 timer.h \u251c\u2500\u2500 src/utils \u2502 \u251c\u2500\u2500 arithmetic.h \u2502 \u2514\u2500\u2500 timer.hip \u251c\u2500\u2500 tests/checker \u2502 \u251c\u2500\u2500 checker.cpp \u2502 \u251c\u2500\u2500 metrics.h \u2502 \u2514\u2500\u2500 checker.h \u251c\u2500\u2500 flake.nix \u2514\u2500\u2500 torch-ext \u251c\u2500\u2500 torch_binding.cpp \u251c\u2500\u2500 torch_binding.h \u2514\u2500\u2500 gemm \u2514\u2500\u2500 __init__.py\n\nIf you look at the original files of the gemm kernel in the RadeonFlow Kernels, they are HIP source files with .cpp extensions. As a first step, you need to change these extensions to either .h or .hip depending on their content and usage:\n\nUse .h for header files containing kernel declarations, inline functions, or template code that will be included in other files\n\nfor header files containing kernel declarations, inline functions, or template code that will be included in other files Use .hip for implementation files containing HIP/GPU code that needs to be compiled separately (e.g., kernel launchers, device functions with complex implementations)\n\nIn our example, gemm_kernel.h , gemm_kernel_legacy.h , and transpose_kernel.h are header files, while gemm_launcher.hip is a HIP implementation file. This naming convention helps the kernel-builder correctly identify and compile each file type.\n\nStep 2: Configuration Files Setup\n\nThe build.toml Manifest\n\nThis file orchestrates the entire build. It tells the kernel-builder what to compile and how everything connects.\n\n[general] name = \"gemm\" universal = false [torch] src = [ \"torch-ext/torch_binding.cpp\" , \"torch-ext/torch_binding.h\" , ] [kernel.gemm] backend = \"rocm\" rocm-archs = [ \"gfx942\" , ] depends = [ \"torch\" ] src = [ \"include/clangd_workaround.h\" , \"include/gpu_libs.h\" , \"include/gpu_types.h\" , \"include/timer.h\" , \"gemm/gemm_kernel.h\" , \"gemm/gemm_kernel_legacy.h\" , \"gemm/gemm_launcher.hip\" , \"gemm/transpose_kernel.h\" , \"src/utils/arithmetic.h\" , \"src/utils/timer.hip\" , \"tests/checker/metrics.h\" , ] include = [ \"include\" ]\n\ngeneral\n\nThis section contains general project configuration settings.\n\nname (required): The name of your project. This should match your kernel name and will be used for the Python package.\n\n(required): The name of your project. This should match your kernel name and will be used for the Python package. universal (optional): the kernel is a universal kernel when set to true . A universal kernel is a pure Python package (no compiled files). Universal kernels do not use the other sections described below. A good example of a universal kernel is a Triton kernel. Default: false\n\ntorch\n\nThis section describes the Torch extension configuration. It defines the Python bindings that will expose your kernel to PyTorch.\n\nsrc (required): A list of source files and headers for the PyTorch extension. In our case, this includes the C++ binding files that create the Python interface.\n\nkernel.gemm\n\nSpecification of a kernel named \"gemm\". You can define multiple kernel sections in the same build.toml file if you have multiple kernels.\n\nbackend (required): The compute backend for the kernel. We use \"rocm\" for AMD GPU support.\n\n(required): The compute backend for the kernel. We use \"rocm\" for AMD GPU support. rocm-archs (required for ROCm): A list of ROCm architectures that the kernel should be compiled for. \"gfx942\" targets the MI300 series GPUs.\n\n(required for ROCm): A list of ROCm architectures that the kernel should be compiled for. \"gfx942\" targets the MI300 series GPUs. depends (required): A list of dependencies. We depend on \"torch\" to use PyTorch's tensor operations.\n\n(required): A list of dependencies. We depend on \"torch\" to use PyTorch's tensor operations. include (optional): Include directories relative to the project root. This helps the compiler find header files.\n\nThe flake.nix Reproducibility File\n\nTo ensure anyone can build your kernel on any machine, we use a flake.nix file. It locks the exact version of the kernel-builder and its dependencies. (You can just copy and paste this example and change the description)\n\n{ description = \"Flake for GEMM kernel\" ; inputs = { kernel-builder. url = \"github:huggingface/kernel-builder\" ; }; outputs = { self, kernel-builder, }: kernel-builder.lib.genFlakeOutputs { inherit self; path = ./.; }; }\n\nWriting the Kernel\n\nNow for the GPU code. Inside gemm/gemm_launcher.hip , we define how the GEMM kernel is launched. Depending on the configuration, we either call the new optimized gemm/gemm_kernel or fall back to the legacy implementation ( gemm/gemm_kernel_legacy ).\n\nextern \"C\" void run ( void *a, void *b, void *as, void *bs, void *c, int m, int n, int k, PerfMetrics *metrics, hipStream_t job_stream0 ) { const __FP8_TYPE *a_ptr = static_cast< const __FP8_TYPE *>(a); const __FP8_TYPE *b_ptr = static_cast< const __FP8_TYPE *>(b); __BF16_TYPE *c_ptr = static_cast<__BF16_TYPE *>(c); const float *as_ptr = static_cast< const float *>(as); const float *bs_ptr = static_cast< const float *>(bs); KernelTimerScoped timer (timers, 2LL * m * n * k, metrics ? &metrics->entries[ 0 ].time : nullptr, metrics ? &metrics->entries[ 0 ].gflops : nullptr, job_stream0) ; switch (pack_shape(m, n, k)) { DISPATCH_GEMM( 1024 , 1536 , 7168 , 256 , 128 , 128 , 4 , 2 , 512 , 4 , 16 ); DISPATCH_GEMM( 6144 , 7168 , 2304 , 256 , 128 , 128 , 4 , 2 , 512 , 1 , 16 ); default : { printf ( \"Error: Unsupported shape M=%d, K=%d, N=%d\n\n\" , m, k, n); abort (); } } }\n\nRegistering a Native PyTorch Operator\n\nThis step is key. We\u2019re not just making the function available in Python; we\u2019re turning it into a native PyTorch operator. That means it becomes a first-class part of PyTorch itself, accessible through torch.ops .\n\nThe file torch-ext/torch_binding.cpp handles this registration.\n\nextern \"C\" { struct PerfMetrics ; void run ( void *a, void *b, void *as, void *bs, void *c, int m, int n, int k, PerfMetrics *metrics, hipStream_t job_stream0) ; } void gemm (torch::Tensor &out, torch::Tensor const &a, torch::Tensor const &b, torch::Tensor const &as, torch::Tensor const &bs) { TORCH_CHECK(a.device().is_cuda(), \"Input tensor a must be on GPU device\" ); TORCH_CHECK(b.device().is_cuda(), \"Input tensor b must be on GPU device\" ); TORCH_CHECK(as.device().is_cuda(), \"Scale tensor as must be on GPU device\" ); TORCH_CHECK(bs.device().is_cuda(), \"Scale tensor bs must be on GPU device\" ); TORCH_CHECK(out.device().is_cuda(), \"Output tensor out must be on GPU device\" ); TORCH_CHECK(a.is_contiguous(), \"Input tensor a must be contiguous\" ); TORCH_CHECK(b.is_contiguous(), \"Input tensor b must be contiguous\" ); TORCH_CHECK(as.is_contiguous(), \"Scale tensor as must be contiguous\" ); TORCH_CHECK(bs.is_contiguous(), \"Scale tensor bs must be contiguous\" ); TORCH_CHECK(out.is_contiguous(), \"Output tensor out must be contiguous\" ); int M = a.size( 0 ); int K = a.size( 1 ); int N = b.size( 1 ); TORCH_CHECK(b.size( 0 ) == K, \"Matrix dimensions mismatch: a.size(1) != b.size(0)\" ); TORCH_CHECK(out.size( 0 ) == M, \"Output tensor dimension mismatch: out.size(0) != M\" ); TORCH_CHECK(out.size( 1 ) == N, \"Output tensor dimension mismatch: out.size(1) != N\" ); const hipStream_t stream = 0 ; run(a.data_ptr(), b.data_ptr(), as.data_ptr(), bs.data_ptr(), out.data_ptr(), M, N, K, nullptr, stream); } TORCH_LIBRARY_EXPAND(TORCH_EXTENSION_NAME, ops) { ops.def( \"gemm(Tensor! out, Tensor a, Tensor b, Tensor a_scale, Tensor b_scale) -> ()\" ); ops.impl( \"gemm\" , torch::kCUDA, &gemm); } REGISTER_EXTENSION(TORCH_EXTENSION_NAME)\n\nThe torch_binding.h file contains function declarations. For instance, the gemm kernel has the following declaration in torch_binding.h :\n\nvoid gemm (torch::Tensor &out, torch::Tensor const &a, torch::Tensor const &b, torch::Tensor const &as, torch::Tensor const &bs) ;\n\nSetting up the __init__.py wrapper\n\nIn torch-ext/gemm/ we need an __init__.py file to make this directory a Python package and to expose our custom operator in a user-friendly way.\n\nfrom typing import Optional import torch from ._ops import ops def gemm ( a: torch.Tensor, b: torch.Tensor, as_: torch.Tensor, bs: torch.Tensor, out: Optional [torch.Tensor] = None ) -> torch.Tensor: if out is None : M, K = a.shape K_b, N = b.shape assert K == K_b, f\"Matrix dimension mismatch: A has {K} cols, B has {K_b} rows\" out = torch.empty((M, N), dtype=torch.bfloat16, device=a.device) ops.gemm(out, a, b, as_, bs) return out\n\nStep 3: Building the Kernel\n\nThe kernel builder uses Nix for building kernels. You can build or run the kernels directly if you have Nix installed on your system. We recommend installing Nix in the following way:\n\nLinux : use the official Nix installer.\n\n: use the official Nix installer. macOS: use the Determinate Nix installer. In addition, Xcode 16.x is currently required to build kernels.\n\nGetting Started with Nix\n\nFirst of all, run this:\n\nnix flake update\n\nThis generates a flake.lock file that pins the kernel builder and all its transitive dependencies. Commit both flake.nix and flake.lock to your repository to ensure that kernel builds are reproducible.\n\nSince the kernel builder depends on many packages (e.g., every supported PyTorch version), it is recommended to enable the Hugging Face cache to avoid expensive rebuilds:\n\ncachix use huggingface\n\nOr run it once without installing cachix permanently:\n\nnix run nixpkgs\n\nBuilding Kernels with Nix\n\nA kernel that has a flake.nix file can be built with the build-and-copy command:\n\ncd Build_RadeonFlow_Kernels/gemm nix build . -L\n\nThe compiled kernel will then be in the local build/ directory.\n\nDevelopment Shell for Local Development\n\nThe kernel-builder provides shells for developing kernels. In such a shell, all required dependencies are available, as well as build2cmake for generating project files:\n\n$ nix develop $ build2cmake generate-torch build.toml $ cmake -B build-ext $ cmake --build build-ext\n\nIf you want to test the kernel as a Python package, you can do so. nix develop will automatically create a virtual environment in .venv and activate it:\n\n$ nix develop $ build2cmake generate-torch build.toml $ pip install --no-build-isolation -e .\n\nDevelopment shells are available for every build configuration. For instance, you can get a Torch 2.7 development shell with ROCm 6.3 using:\n\n$ rm -rf .venv $ nix develop .\n\nStep 4: Uploading the kernel to the Hub\n\nNow that we built our kernel, we can test it and upload it to the Hub.\n\nBuilding the Kernel for All PyTorch and ROCm Versions\n\nOne small thing we'll want to do before we share is clean up all of the development artifacts that were generated during the build process to avoid uploading unnecessary files.\n\nbuild2cmake clean build.toml\n\nTo build the kernel for all supported versions of PyTorch and ROCm, the kernel-builder tool automates the process:\n\nnix build . -L\n\nNote:\n\nThis process may take a while, as it will build the kernel for all supported versions of PyTorch and ROCm.\n\nThe output will be in the result directory.\n\nThe last step is to move the results into the expected build directory (this is where the kernels library will look for them).\n\nmkdir -p build rsync -av --delete -- chmod =Du+w,Fu+w result/ build/\n\nPushing to the Hugging Face Hub\n\nPushing the build artifacts to the Hub will make it straightforward for other developers to use your kernel.\n\nFirst, create a new repo:\n\nhf repo create gemm\n\nMake sure you are logged in to the Hugging Face Hub using huggingface-cli login.\n\nNow, in your project directory, connect your project to the new repository and push your code:\n\ngit init git remote add origin https://huggingface.co/<your-username>/gemm git pull origin main git xet install git checkout -b main git xet track \"*.so\" git add \\ build/ gemm/ include/ src/utils tests/checker \\ torch-ext/torch_binding.cpp torch-ext/torch_binding.h torch-ext/gemm \\ flake.nix flake.lock build.toml git commit -m \"feat: Created a compliant gemm kernel\" git push -u origin main\n\nFantastic! Your kernel is now on the Hugging Face Hub, ready for others to use and fully compliant with the kernels library.\n\nStep 5: Let's use it :)\n\nWith the kernels library, you don't \"install\" the kernel in the traditional sense. You load it directly from its Hub repository, which automatically registers the new operator.\n\nimport torch from kernels import get_kernel gemm = get_kernel( \"kernels-community/gemm\" ) M, N, K = 1024 , 1536 , 7168 QUANT_SIZE = 128 device = torch.device( \"cuda\" ) A_fp32 = torch.randn(M, K, device=device) B_fp32 = torch.randn(K, N, device=device) A_fp8 = A_fp32.to(torch.float8_e4m3fnuz) B_fp8 = B_fp32.to(torch.float8_e4m3fnuz) A_scale = torch.ones(K // QUANT_SIZE, M, device=device, dtype=torch.float32) B_scale = torch.ones(K // QUANT_SIZE, N // QUANT_SIZE, device=device, dtype=torch.float32) C = torch.zeros(M, N, device=device, dtype=torch.bfloat16) result = gemm.gemm(A_fp8, B_fp8, A_scale, B_scale, C)\n\nThat's it! Your ROCm kernel is now ready to use from the Hugging Face Hub.\n\nConclusion\n\nBuilding and sharing ROCm kernels with the Hugging Face is now easier than ever. With a clean, reproducible workflow powered by Nix and seamless integration into PyTorch, developers can focus on optimizing performance rather than setup. Once built, your custom kernel can be shared on the Hugging Face Hub; making it instantly accessible to the community and usable across projects with just a few lines of code. \ud83d\ude80\n\nRelated Libraries & Hub",
    "link": "https://huggingface.co/blog/build-rocm-kernels",
    "Summary": "About the kernelThis section was written by the RadeonFlow GEMM kernel authors to introduce the kernel.\nThat\u2019s exactly what we\u2019ll cover next, using kernel-builder and kernels to structure, build, and publish the ROCm kernel.\nout, Tensor a, Tensor b, Tensor a_scale, Tensor b_scale) -> ()\" ); ops.impl( \"gemm\" , torch::kCUDA, &gemm); } REGISTER_EXTENSION(TORCH_EXTENSION_NAME)The torch_binding.h file contains function declarations.\nIn addition, Xcode 16.x is currently required to build kernels.\nbuild2cmake clean build.tomlTo build the kernel for all supported versions of PyTorch and ROCm, the kernel-builder tool automates the process:nix build .",
    "Keywords": [
      "easily",
      "build",
      "gemm",
      "tensor",
      "const",
      "rocm",
      "k",
      "b",
      "face",
      "hugging",
      "m",
      "kernel",
      "n",
      "kernels",
      "share"
    ]
  },
  {
    "Title": "Open ASR Leaderboard: Trends and Insights with New Multilingual & Long-Form Tracks",
    "Authors": [],
    "Publish Date": null,
    "Text": "Open ASR Leaderboard: Trends and Insights with New Multilingual & Long-Form Tracks\n\nPublished November 21, 2025 Update on GitHub\n\nWhile everyone (and their grandma \ud83d\udc75) is spinning up new ASR models, picking the right one for your use case can feel more overwhelming than choosing your next Netflix show. As of 21 Nov 2025, there areandon the Hub \ud83e\udd2f\n\nMost benchmarks focus on short-form English transcription (<30s), and overlook other important tasks, such as (1) multilingual performance and (2) model throughput, which can a be deciding factor for long-form audio like meetings and podcasts.\n\nOver the past two years, the Open ASR Leaderboard has become a standard for comparing open and closed-source models on both accuracy and efficiency. Recently, multilingual and long-form transcription tracks have been added to the leaderboard \ud83c\udf89\n\n\ud83d\udcdd New preprint on ASR trends from the leaderboard: https://hf.co/papers/2510.06961\n\non ASR trends from the leaderboard: https://hf.co/papers/2510.06961 \ud83e\udde0 Best accuracy: Conformer encoder + LLM decoders (open-source ftw \ud83e\udd73)\n\nConformer encoder + LLM decoders (open-source ftw \ud83e\udd73) \u26a1 Fastest: CTC / TDT decoders\n\nCTC / TDT decoders \ud83c\udf0d Multilingual: Comes at the cost of single-language performance\n\nComes at the cost of single-language performance \u231b Long-form: Closed-source systems still lead (for now \ud83d\ude09)\n\nClosed-source systems still lead (for now \ud83d\ude09) \ud83e\uddd1\u200d\ud83d\udcbb Fine-tuning guides (Parakeet, Voxtral, Whisper): to continue pushing performance\n\nTakeaways from 60+ models\n\nAs of 21 Nov 2025, the Open ASR Leaderboard compares 60+ open and closed-source models from 18 organizations, across 11 datasets.\n\nIn a recent preprint, we dive into the technical setup and highlight some key trends in modern ASR. Here are the big takeaways \ud83d\udc47\n\n1. Conformer encoder \ud83e\udd1d LLM decoder tops the charts \ud83d\udcc8\n\nModels combining Conformer encoders with large language model (LLM) decoders currently lead in English transcription accuracy. For example, NVIDIA\u2019s Canary-Qwen-2.5B, IBM\u2019s Granite-Speech-3.3-8B, and Microsoft\u2019s Phi-4-Multimodal-Instruct achieve the lowest word error rates (WER), showing that integrating LLM reasoning can significantly boost ASR accuracy.\n\n\ud83d\udca1 Pro-tip: NVIDIA introduced Fast Conformer, a 2x faster variant of the Conformer, that is used in their Canary and Parakeet suite of models.\n\n2. Speed\u2013accuracy tradeoffs \u2696\ufe0f\n\nWhile highly accurate, these LLM decoders tend to be slower than simpler approaches. On the Open ASR Leaderboard, efficiency is measured using inverse real-time factor (RTFx), where higher is better.\n\nFor even faster inference, CTC and TDT decoders deliver 10\u2013100\u00d7 faster throughput, albeit with slightly higher error rates. This makes them ideal for real-time, offline, or batch transcription tasks (such as meetings, lectures, or podcasts).\n\n3. Multilingual \ud83c\udf0d\n\nOpenAI\u2019s Whisper Large v3 remains a strong multilingual baseline, supporting 99 languages. However, fine-tuned or distilled variants like Distil-Whisper and CrisperWhisper often outperform the original on English-only tasks, showing how targeted fine-tuning can improve specialization (how to fine-tune? Check out guides for Whisper, Parakeet, and Voxtral).\n\nThat said, focusing on English tends to reduce multilingual coverage \ud83d\udc49 a classic case of the tradeoff between specialization and generalization. Similarly, while self-supervised systems like Meta\u2019s Massively Multilingual Speech (MMS) and Omnilingual ASR can support 1K+ languages, they trail behind language-specific encoders in accuracy.\n\n\u2b50 While just five languages are currently benchmarked, we\u2019re planning to expand to more languages and are excited for new dataset and models contributions to multilingual ASR through GitHub pull requests.\n\n\ud83c\udfaf Alongside multilingual benchmarks, several community-driven leaderboards focus on individual languages. For example, the Open Universal Arabic ASR Leaderboard compares models across Modern Standard Arabic and regional dialects, highlighting how speech variation and diglossia challenge current systems. Similarly. the Russian ASR Leaderboard provides a growing hub for evaluating encoder-decoder and CTC models on Russian-specific phonology and morphology. These localized efforts mirror the broader multilingual leaderboard\u2019s mission to encourage dataset sharing, fine-tuned checkpoints, and transparent model comparisons, especially in languages with fewer established ASR resources.\n\n4. Long-form transcription is a different game \u23f3\n\nFor long-form audio (e.g., podcasts, lectures, meetings), closed-source systems still edge out open ones. It could be due to domain tuning, custom chunking, or production-grade optimization.\n\nAmong open models, OpenAI\u2019s Whisper Large v3 performs the best. But for throughput, CTC-based Conformers shine \ud83d\udc49 for example, NVIDIA\u2019s Parakeet CTC 1.1B achieves an RTFx of 2793.75, compared to 68.56 for Whisper Large v3, with only a moderate WER degradation (6.68 and 6.43 respectively).\n\nThe tradeoff? Parakeet is English-only, again reminding us of that multilingual and specialization tradeoff \ud83e\udee0.\n\n\u2b50 While closed systems still lead, there\u2019s huge potential for open-source innovation here. Long-form ASR remains one of the most exciting frontiers for the community to tackle next!\n\n\ud83c\udfa4 The Show Must Go On\n\nGiven how fast ASR is evolving, we\u2019re excited to see what new architectures push performance and efficiency, and how the Open ASR Leaderboard continues to serve as a transparent, community-driven benchmark for the field, and as a reference for other leaderboards (Russian, Arabic, and Speech DeepFake Detection).\n\nWe\u2019ll keep expanding the Open ASR LeaderBoard with more models, more languages, and more datasets so stay tuned \ud83d\udc40\n\n\ud83d\udc49 Want to contribute? Head on over to the GitHub repo to open a pull request \ud83d\ude80",
    "link": "https://huggingface.co/blog/open-asr-leaderboard",
    "Summary": "Open ASR Leaderboard: Trends and Insights with New Multilingual & Long-Form TracksPublished November 21, 2025 Update on GitHubWhile everyone (and their grandma \ud83d\udc75) is spinning up new ASR models, picking the right one for your use case can feel more overwhelming than choosing your next Netflix show.\nOver the past two years, the Open ASR Leaderboard has become a standard for comparing open and closed-source models on both accuracy and efficiency.\nOn the Open ASR Leaderboard, efficiency is measured using inverse real-time factor (RTFx), where higher is better.\nLong-form ASR remains one of the most exciting frontiers for the community to tackle next!\nWe\u2019ll keep expanding the Open ASR LeaderBoard with more models, more languages, and more datasets so stay tuned \ud83d\udc40\ud83d\udc49 Want to contribute?",
    "Keywords": [
      "models",
      "decoders",
      "systems",
      "multilingual",
      "trends",
      "open",
      "leaderboard",
      "insights",
      "languages",
      "tracks",
      "asr",
      "llm",
      "longform"
    ]
  },
  {
    "Title": "Building a Healthcare Robot from Simulation to Deployment with NVIDIA Isaac",
    "Authors": [],
    "Publish Date": null,
    "Text": "Building a Healthcare Robot from Simulation to Deployment with NVIDIA Isaac\n\nPublished October 29, 2025 Update on GitHub\n\nA hands-on guide to collecting data, training policies, and deploying autonomous medical robotics workflows on real hardware\n\nIntroduction\n\nSimulation has been a cornerstone in medical imaging to address the data gap. However, in healthcare robotics until now, it's often been too slow, siloed, or difficult to translate into real-world systems.\n\nNVIDIA Isaac for Healthcare, a developer framework for AI healthcare robotics, enables healthcare robotics developers in solving these challenges via offering integrated data collection, training, and evaluation pipelines that work across both simulation and hardware. Specifically, the Isaac for Healthcare v0.4 release provides healthcare developers with an end-to-end SO - ARM based starter workflow and the bring your own operating room tutorial. The SO-ARM starter workflow lowers the barrier for MedTech developers to experience the full workflow from simulation to train to deployment and start building and validating autonomous on real hardware right away.\n\nIn this post, we'll walk through the starter workflow and its technical implementation details to help you build a surgical assistant robot in less time than ever imaginable before.\n\nSO-ARM Starter Workflow; Building an Embodied Surgical Assistant\n\nThe SO-ARM starter workflow introduces a new way to explore surgical assistance tasks, and providing developers with a complete end-to-end pipeline for autonomous surgical assistance:\n\nCollect real-world and synthetic data with SO-ARM using the LeRobot\n\nFine-tune GR00t N1.5, evaluate in IsaacLab, then deploy to hardware\n\nThis workflow gives developers a safe, repeatable environment to train and refine assistive skills before moving into the Operating Room.\n\nTechnical Implementation\n\nThe workflow implements a three-stage pipeline that integrates simulation and real hardware:\n\nData Collection: Mixed simulation and real-world teleoperation demonstrations using using SO101 and LeRobot Model Training: Fine-tuning GR00T N1.5 on combined datasets with dual-camera vision Policy Deployment: Real-time inference on physical hardware with RTI DDS communication\n\nNotably, over 93% of the data used for policy training was generated synthetically in simulation, underscoring the strength of simulation in bridging the robotic data gap.\n\nSim2Real Mixed Training Approach\n\nThe workflow combines simulation and real-world data to address the fundamental challenge that training robots in the real world is expensive and limited, while pure simulation often fails to capture real-world complexities. The approach uses approximately 70 simulation episodes for diverse scenarios and environmental variations, combined with 10-20 real-world episodes for authenticity and grounding. This mixed training creates policies that generalize beyond either domain alone.\n\nHardware Requirements\n\nThe workflow requires:\n\nGPU: RT Core-enabled architecture (Ampere or later) with \u226530GB VRAM for GR00TN1.5 inference\n\nSO-ARM101 Follower: 6-DOF precision manipulator with dual-camera vision (wrist and room). The SO-ARM101 features WOWROBO vision components, including a wrist-mounted camera with a 3D-printed adapter\n\nSO-ARM101 Leader: 6-DOF Teleoperation interface for expert demonstration collection\n\nNotably, developers could run all the simulation, training and deployment (3 computers needed for physical AI) on one DGX Spark.\n\nData Collection Implementation\n\nFor real-world data collection with SO-ARM101 hardware or any other version supported in LeRobot:\n\npython lerobot-record \\ --robot.type=so101_follower \\ --robot.port=<follower_port_id> \\ --robot.cameras= \"{wrist: {type: opencv, index_or_path: 0, width: 640, height: 480, fps: 30}, room: {type: opencv, index_or_path: 2, width: 640, height: 480, fps: 30}}\" \\ --robot.id=so101_follower_arm \\ --teleop.type=so101_leader \\ --teleop.port=<leader_port_id> \\ --teleop.id=so101_leader_arm \\ --dataset.repo_id=<user>/surgical_assistance/surgical_assistance \\ --dataset.num_episodes=15 \\ --dataset.single_task= \"Prepare and hand surgical instruments to surgeon\"\n\nFor simulation-based data collection:\n\npython -m simulation.environments.teleoperation_record \\ --enable_cameras \\ --record \\ --dataset_path=/path/to/save/dataset.hdf5 \\ --teleop_device=keyboard python -m simulation.environments.teleoperation_record \\ --port=<your_leader_arm_port_id> \\ --enable_cameras \\ --record \\ --dataset_path=/path/to/save/dataset.hdf5\n\nSimulation Teleoperation Controls\n\nFor users without physical SO-ARM101 hardware, the workflow provides keyboard-based teleoperation with the following joint controls:\n\nJoint 1 (shoulder_pan): Q (+) / U (-)\n\nJoint 2 (shoulder_lift): W (+) / I (-)\n\nJoint 3 (elbow_flex): E (+) / O (-)\n\nJoint 4 (wrist_flex): A (+) / J (-)\n\nJoint 5 (wrist_roll): S (+) / K (-)\n\nJoint 6 (gripper): D (+) / L (-)\n\nR Key: Reset recording environment\n\nN Key: Mark episode as successful\n\nModel Training Pipeline\n\nAfter collecting both simulation and real-world data, convert and combine datasets for training:\n\npython -m training.hdf5_to_lerobot \\ --repo_id=surgical_assistance_dataset \\ --hdf5_path=/path/to/your/sim_dataset.hdf5 \\ --task_description= \"Autonomous surgical instrument handling and preparation\" python -m training.gr00t_n1_5.train \\ --dataset_path /path/to/your/surgical_assistance_dataset \\ --output_dir /path/to/surgical_checkpoints \\ --data_config so100_dualcam\n\nThe trained model processes natural language instructions such as \"Prepare the scalpel for the surgeon\" or \"Hand me the forceps\" and executes the corresponding robotic actions. With LeRobot latest release (0.4.0) you will be able to fine-tune Gr00t N1.5 natively in LeRobot!\n\nEnd-to-End Sim Collect\u2013Train\u2013Eval Pipelines\n\nSimulation is most powerful when it's part of a loop: collect \u2192 train \u2192 evaluate \u2192 deploy.\n\nWith v0.3, IsaacLab supports this full pipeline:\n\nGenerate Synthetic Data in Simulation\n\nTeleoperate robots using keyboard or hardware controllers\n\nCapture multi-camera observations, robot states, and actions\n\nCreate diverse datasets with edge cases impossible to collect safely in real environments\n\nTrain and Evaluate Policies\n\nDeep integration with Isaac Lab's RL framework for PPO training\n\nParallel environments (thousands of simulations simultaneously)\n\nBuilt-in trajectory analysis and success metrics\n\nStatistical validation across varied scenarios\n\nConvert Models to TensorRT\n\nAutomatic optimization for production deployment\n\nSupport for dynamic shapes and multi-camera inference\n\nBenchmarking tools to verify real-time performance\n\nThis reduces time from experiment to deployment and makes sim2real a practical part of daily development.\n\nGetting Started\n\nIsaac for Healthcare SO-ARM Starter Workflow is available now. To get started:\n\nClone the repository: git clone https://github.com/isaac-for-healthcare/i4h-workflows.git Choose a workflow: Start with the SO-ARM Starter Workflow for surgical assistance or explore other workflows Run the setup: Each workflow includes an automated setup script (e.g., tools/env_setup_so_arm_starter.sh )",
    "link": "https://huggingface.co/blog/lerobotxnvidia-healthcare",
    "Summary": "Building a Healthcare Robot from Simulation to Deployment with NVIDIA IsaacPublished October 29, 2025 Update on GitHubA hands-on guide to collecting data, training policies, and deploying autonomous medical robotics workflows on real hardwareIntroductionSimulation has been a cornerstone in medical imaging to address the data gap.\nNVIDIA Isaac for Healthcare, a developer framework for AI healthcare robotics, enables healthcare robotics developers in solving these challenges via offering integrated data collection, training, and evaluation pipelines that work across both simulation and hardware.\nSpecifically, the Isaac for Healthcare v0.4 release provides healthcare developers with an end-to-end SO - ARM based starter workflow and the bring your own operating room tutorial.\nThe SO-ARM starter workflow lowers the barrier for MedTech developers to experience the full workflow from simulation to train to deployment and start building and validating autonomous on real hardware right away.\nGetting StartedIsaac for Healthcare SO-ARM Starter Workflow is available now.",
    "Keywords": [
      "data",
      "starter",
      "surgical",
      "joint",
      "training",
      "simulation",
      "deployment",
      "building",
      "realworld",
      "isaac",
      "hardware",
      "workflow",
      "nvidia",
      "healthcare",
      "robot"
    ]
  },
  {
    "Title": "Voice Cloning with Consent",
    "Authors": [],
    "Publish Date": null,
    "Text": "Voice Cloning with Consent\n\nPublished October 28, 2025 Update on GitHub\n\nIn this blog post, we introduce the idea of a 'voice consent gate' to support voice cloning with consent. We provide an example Space and accompanying code to start the ball rolling on the idea.\n\nRealistic voice generation technology has gotten uncannily good in the past few years. In some situations, it\u2019s possible to generate a synthetic voice that sounds almost exactly like the voice of a real person. And today, what once felt like science fiction is reality: Voice cloning. With just a few seconds of recorded speech, anyone\u2019s voice can be made to say almost anything.\n\nVoice generation, and in particular the subtask of voice cloning, has notable risks and benefits. The risks of \u201cdeepfakes\u201d, such as the cloned voice of former President Biden used in robocalls, can mislead people into thinking that people have said things that they haven\u2019t said. On the other hand, voice cloning can be a powerful beneficial tool, helping people who\u2019ve lost the ability to speak communicate in their own voice again, or assisting people in learning new languages and dialects.\n\nSo how do we create meaningful use without malicious use? We\u2019re exploring one possible answer: a voice consent gate. That\u2019s a system where a voice can be cloned only when the speaker explicitly says they consent. In other words, the model won\u2019t speak in your voice unless you say it\u2019s okay.\n\nWe provide a basic demo of this idea below:\n\nEthics in Practice: Consent as System Infrastructure\n\nThe voice consent gate is a piece of infrastructure we're exploring that provides methods for ethical principles like consent to be embedded directly into AI system workflows. In our demo, this means the model only starts once the speaker\u2019s consent phrase has been both spoken and recognized, effectively making consent a prerequisite for action. This turns an abstract principle into a concrete system condition, creating a traceable, auditable interaction: an AI model can only run after an unambiguous act of consent.\n\nSuch design choices matter beyond voice cloning. They illustrate how AI systems can be built to respect autonomy by default, and how transparency and consent can be made functional, not just declarative.\n\nThe Technical Details\n\nTo create a basic voice cloning system with a voice consent gate, you need three parts:\n\nA way of generating novel consent sentences for the person whose voice will be cloned \u2013 the \u201cspeaker\u201d \u2013 to say, uniquely referencing the current consent context. An automatic speech recognition (ASR) system that recognizes the sentence conveying consent. A voice-cloning text-to-speech (TTS) system that takes as input text and the speaker's speech snippets to generate speech.\n\nOur observation: Since some voice-cloning systems can now generate speech similar to a speaker\u2019s voice using just one sentence, a sentence used for consent can also be used for voice cloning.\n\nApproach\n\nThe consent bit: To create a voice consent gate in an English voice cloning system, generate a short, natural-sounding English utterance (~20 words) for a person to read aloud that clearly states their informed consent in the current context. We recommend explicitly including a consent phrase and the model name, such as \u201cI give my consent to use the < MODEL > voice cloning model with my voice\u201d. We also recommend using an audio recording that cannot be uploaded, but that instead comes directly from a microphone, to make sure that the sentence isn\u2019t part of an earlier recording that\u2019s been manipulated. Pairing this with a novel (previously unsaid) sentence further helps to directly index the current consent context - supporting explicit, active, context-specific, informed consent. While this design reduces risks of reusing prior recordings, it\u2019s not foolproof; a person could still generate a matching phrase using another TTS system. Future iterations could explore lightweight audio provenance checks, speaker-embedding similarity, or metadata from real-time capture to help verify that the consent audio originates from the intended speaker.\n\nThe suitable-for-voice-cloning bit: Previous work on voice cloning has shown that the phrases provided by the speaker must have phonetic variety, covering diverse vowels and consonants; have a \u201cneutral\u201d or polite tone, without background noise and with the speaker in a comfortable position; and have a clear start and end (i.e., don\u2019t trim the clip mid-word).\n\nTo enact both of these aspects within the demo, we prompt a language model to create pairs of sentences: one expressing explicit consent, and another neutral sentence that adds phonetic diversity (covering different vowels, consonants, and tones). Each prompt utilizes a randomly-chosen everyday topic (like the weather, food, or music) to keep the sentences varied and comfortable to say, aiding in creating recordings that are clear, natural, and phonetically rich, while also containing an unambiguous statement of consent. This generation step is automated rather than pre-written so that each user receives a unique sentence pair, preventing reuse of the same text and ensuring that consent recordings are specific to the current session. In other words, the language model generates two fresh sentences per consent instance: one for explicit consent and one for phonetic variety. For example, the language model might generate: \u201cI give my consent to use my voice for generating audio with the model EchoVoice. The weather is bright and calm this morning.\u201d This approach ensures that every sample used for cloning contains verifiable, explicit consent, while remaining suitable as technical input for high-quality voice synthesis. (Note: It's not required that the language model be a \"large\" language model, which brings its own consent issues.)\n\nSome examples:\n\n\u201cI give my consent to use my voice for generating synthetic audio with the Chatterbox model today. My daily commute involves navigating through crowded streets on foot most days lately anyway.\u201d\n\n\u201cI give my consent to use my voice for generating audio with the model Chatterbox. After a gentle morning walk, I'm feeling relaxed and ready to speak freely now.\u201d\n\n\u201cI agree to the use of my recorded voice for audio generation with the model Chatterbox. The coffee shop outside has a pleasant aroma of freshly brewed coffee this morning.\u201d\n\nUnlocking the Voice Consent Gate\n\nOnce the speaker\u2019s input matches the generated text, the voice cloning system can start, using the speaker\u2019s consent audio as the input.\n\nThere are a few options for doing this, and we\u2019d love to hear further ideas. For now, there\u2019s:\n\nWhat we provide in the demo: Have the voice consent gate open directly to the voice cloning model, where arbitrary text can be written and generated in the speaker\u2019s voice. The model uses the consenting audio directly to learn the speaker\u2019s voice.\n\nAlternatively, it\u2019s possible to modify the code we provide in the demo to model the speaker\u2019s voice using a variety of different uploaded voice files that the speaker is consenting to \u2013 for example, when providing consent for using online recordings. Prompts and consent phrases should be altered accordingly.\n\nIt\u2019s also possible to save the consent audio to be used by a given system, for example, when the speaker is consenting to have their voice used for arbitrary utterances in the future. This can be done using the huggingface_hub upload capability. Read how to do this here. Again, prompts and consent phrases for the speaker to say should account for this context of use.\n\nCheck our demo out here! You can copy the code to suit your own use.\n\nThe code is modular so it can be sliced and diced in different ways to incorporate into your own projects. We\u2019ll be working on making this more robust and secure over time, and we\u2019re curious to hear your ideas on how to improve.\n\nHandled responsibly, this technology doesn\u2019t have to haunt us. It can instead become a respectful collaboration between humans and machines \u2014 no ghosts in the machine, just good practice. \ud83c\udf83",
    "link": "https://huggingface.co/blog/voice-consent-gate",
    "Summary": "Voice Cloning with ConsentPublished October 28, 2025 Update on GitHubIn this blog post, we introduce the idea of a 'voice consent gate' to support voice cloning with consent.\nThe Technical DetailsTo create a basic voice cloning system with a voice consent gate, you need three parts:A way of generating novel consent sentences for the person whose voice will be cloned \u2013 the \u201cspeaker\u201d \u2013 to say, uniquely referencing the current consent context.\nApproachThe consent bit: To create a voice consent gate in an English voice cloning system, generate a short, natural-sounding English utterance (~20 words) for a person to read aloud that clearly states their informed consent in the current context.\nWe recommend explicitly including a consent phrase and the model name, such as \u201cI give my consent to use the < MODEL > voice cloning model with my voice\u201d.\nFor now, there\u2019s:What we provide in the demo: Have the voice consent gate open directly to the voice cloning model, where arbitrary text can be written and generated in the speaker\u2019s voice.",
    "Keywords": [
      "speaker",
      "voice",
      "consent",
      "speakers",
      "audio",
      "system",
      "cloning",
      "sentence",
      "using",
      "model"
    ]
  },
  {
    "Title": "Building for an Open Future - our new partnership with Google Cloud",
    "Authors": [],
    "Publish Date": null,
    "Text": "Building for an Open Future - our new partnership with Google Cloud\n\nPublished November 13, 2025 Update on GitHub\n\nToday, we are happy to announce a new and deeper partnership with Google Cloud, to enable companies to build their own AI with open models.\n\n\u201cGoogle has made some of the most impactful contributions to open AI, from the OG transformer to the Gemma models. I believe in a future where all companies will build and customize their own AI. With this new strategic partnership, we\u2019re making it easy to do on Google Cloud.\u201d says Jeff Boudier, at Hugging Face.\n\n\u201cHugging Face has been the driving force enabling companies large and small all over the world to access, use and customize now more than 2 million open models, and we\u2019ve been proud to contribute over 1,000 of our models to the community\u201d, says Ryan J. Salva, Senior Director of Product Management at Google Cloud. \u201cTogether we will make Google Cloud the best place to build with open models.\u201d\n\nA Partnership for Google Cloud customers\n\nGoogle Cloud customers use open models from Hugging Face in many of its leading AI services. In Vertex AI, the most popular open models are ready to deploy in a couple clicks within Model Garden. Customers who want greater control over their AI infrastructure can find a similar model library available in GKE AI/ML, or use pre-configured environments maintained by Hugging Face. Customers also run AI inference workloads with Cloud Run GPUs, enabling serverless open model deployments.\n\nThe common thread: we work with Google Cloud to build seamless experiences fully leveraging the unique capabilities of each service to offer choice to the customers.\n\nThe Gateway to Open Models - A Fast Lane for Google Cloud Customers\n\nUsage of Hugging Face by Google Cloud customers has grown 10x over the last 3 years, and today, this translates into tens of petabytes of model downloads every month, in billions of requests.\n\nTo make sure Google Cloud customers have the best experience building with models and datasets from Hugging Face, we are working together to create a CDN Gateway for Hugging Face repositories built on top of both Hugging Face Xet optimized storage and data transfer technologies, and Google Cloud advanced storage and networking capabilities.\n\nThis CDN Gateway will cache Hugging Face models and datasets directly on Google Cloud to significantly reduce downloading times, and strengthen model supply chain robustness for Google Cloud customers. Whether you\u2019re using Vertex, GKE, Cloud Run or just building your own stack in VMs in Compute Engine, you will benefit from faster time-to-first-token and simplified model governance.\n\nA partnership for Hugging Face customers\n\nHugging Face Inference Endpoints is the easiest way to go from model to deployment in just a couple clicks. Through this deepened partnership we will bring the unique capabilities and cost performance of Google Cloud to Hugging Face customers, starting with Inference Endpoints. Expect more and newer instances available as well as price drops!\n\nWe will ensure all the fruits of our product and engineering collaboration become easily available to the 10 million AI Builders on Hugging Face. Going from a model page to deploying on Vertex Model Garden or GKE should only take a couple steps. Taking a private model securely hosted in an Enterprise organization on Hugging Face should be as easy as working with public models.\n\nTPUs, Google custom AI accelerator chips now in their seventh generation, have been steadily improving in performance and software stack maturity. We want to make sure Hugging Face users can fully benefit from the current and the next generations of TPUs when they build AI with open models. We are excited to make TPUs as easy to use as GPUs for Hugging Face models, thanks to native support in our libraries.\n\nAdditionally, this new partnership will enable Hugging Face to leverage Google industry-leading security technology to make the millions of open models on Hugging Face more secure. Powered by VirusTotal, Google Threat Intelligence and Mandiant, this joint effort aims to secure models, datasets and Spaces as you use the Hugging Face Hub daily.\n\nBuilding the open future of AI together\n\nWe want to see a future where every company can build their own AI with open models and host it within their own secure infrastructure, with full control. We are excited to make this future happen with Google Cloud. Our deep collaboration will accelerate this vision, whether you are using Vertex AI Model Garden, Google Kubernetes Engine, Cloud Run or Hugging Face Inference Endpoints.\n\nIs there something you want us to create or improve thanks to our partnership with Google? Let us know in the comments!",
    "link": "https://huggingface.co/blog/google-cloud",
    "Summary": "Building for an Open Future - our new partnership with Google CloudPublished November 13, 2025 Update on GitHubToday, we are happy to announce a new and deeper partnership with Google Cloud, to enable companies to build their own AI with open models.\nWith this new strategic partnership, we\u2019re making it easy to do on Google Cloud.\u201d says Jeff Boudier, at Hugging Face.\n\u201cTogether we will make Google Cloud the best place to build with open models.\u201dA Partnership for Google Cloud customersGoogle Cloud customers use open models from Hugging Face in many of its leading AI services.\nThis CDN Gateway will cache Hugging Face models and datasets directly on Google Cloud to significantly reduce downloading times, and strengthen model supply chain robustness for Google Cloud customers.\nAdditionally, this new partnership will enable Hugging Face to leverage Google industry-leading security technology to make the millions of open models on Hugging Face more secure.",
    "Keywords": [
      "partnership",
      "models",
      "future",
      "cloud",
      "building",
      "customers",
      "face",
      "hugging",
      "open",
      "ai",
      "google",
      "model"
    ]
  },
  {
    "Title": "Streaming datasets: 100x More Efficient",
    "Authors": [],
    "Publish Date": null,
    "Text": "Streaming datasets: 100x More Efficient\n\nPublished October 27, 2025 Update on GitHub\n\nWe boosted load_dataset('dataset', streaming=True) , streaming datasets without downloading them with one line of code! Start training on multi-TB datasets immediately, without complex setups, downloading, no \"disk out of space\", or 429 \u201cstop requesting!\u201d errors.\n\nIt's super fast! Outrunning our local SSDs when training on 64xH100 with 256 workers downloading data. We've improved streaming to have 100x fewer requests, \u2192 10\u00d7 faster data resolution \u2192 2x sample/sec, \u2192 0 worker crashes at 256 concurrent workers.\n\nLoading data, especially at the terabyte scale, is a major pain in any machine learning workflow. We suffered this while training SmolLM3, at one point we had to wait 3 hours before each run to download enough data.\n\nStreaming has always been possible in the datasets library, but large scale training with massive datasets remained a challenge. That changes today \ud83d\udd25. We spent a few months improving the backend, focusing on streaming datasets to make it faster and more efficient.\n\nWhat did we do exactly? \u2935\ufe0f\n\nStreaming: The Same Easy API\n\nFirst things first: our changes are backwards compatible. You can still stream any dataset from the Hub with the same simple streaming=True flag. It's as easy as ever. \ud83d\ude80\n\nfrom datasets import load_dataset dataset = load_dataset( \"HuggingFaceM4/FineVisionMax\" , split= \"train\" , streaming= True ) print ( next ( iter (dataset)))\n\nThousands of AI developers around the world use datasets daily; they should just get improved performance with zero extra work.\n\nThe Challenge: Streaming at Scale\n\nStreaming was a lifesaver to quickly understand a dataset, but to train models, people were usually downloading the data locally, or using a cloud storage service such as S3. That's what we were doing for training SmolVLM, we had all of our data on S3 and were streaming directly from it.\n\nWe wanted to change that, so we decided to use streaming from the Hub when we were developing nanoVLM. Soon we found a big issue: our test run generated over 100,000 requests in under a minute, which got our IP blocked by the Hub! \ud83d\ude05 This happened because every DataLoader worker was initializing the dataset independently. As we dug deeper, we found that this creates a storm of redundant requests, many of which are unnecessary. Our changes ultimately reduced startup requests by a factor of 100. In total, our improvements delivered:\n\nData files resolution time: 10x faster\n\nStartup requests: Up to 100x more efficient\n\nStreaming speed: Up to 2x faster\n\nIn-flight requests: Up to 2x more efficient\n\nUnder the Hood: What We Improved\n\nSo, what changed? We focused on two phases: startup and streaming.\n\n1. Startup\u26a1\ufe0f The initial resolution of data files was creating a ton of requests. We made two major changes:\n\nPersistent Data Files Cache: We are now caching the list of data files across all DataLoader workers. The first worker resolves the file list from the Hub. All others workers read directly from this local cache, virtually eliminating startup requests and slashing resolution time. No more request storms!\n\nOptimized Resolution Logic: We also minimized the number of API calls required for that initial worker to fetch the file list. We now bundle the necessary requests as efficiently as possible, reducing latency even further.\n\n2. Streaming \ud83c\udfce\ufe0f To improve throughput during streaming itself, we've introduced two new features:\n\nPrefetching for Parquet: We enabled prefetching for Parquet datasets. This means that while your model is processing the current chunk of data, the datasets library is already fetching the next chunk in the background. This keeps the data pipeline full and ensures your GPU is never left waiting for data.\n\nConfigurable Buffering: Advanced users can now fine-tune streaming performance for their specific hardware and network setup. We've exposed options to configure the buffer's block size and the prefetch volume, giving you maximum control to optimize I/O.\n\nThis is how we can increase the minimum request size when streaming from 32MiB (default) to 128MiB and configure prefetching:\n\nimport pyarrow import pyarrow.dataset fragment_scan_options = pyarrow.dataset.ParquetFragmentScanOptions( cache_options=pyarrow.CacheOptions( prefetch_limit= 1 , range_size_limit= 128 << 20 ), ) ds = load_dataset(parquet_dataset_id, streaming= True , fragment_scan_options=fragment_scan_options)\n\nTogether, these improvements can double your data throughput, allowing you to train faster and more efficiently.\n\nHow are we faster than plain S3: Xet\n\nHugging Face uses Xet: a dedupe-based storage which enables fast deduped uploads and downloads. Unlike traditional remote storage, data transfers are faster on Xet because duplicated data is only transferred once. For example: uploading a large scale dataset to Hugging Face leverages Xet which accelerates uploads. Once the dataset is uploaded, it can be streamed right away.\n\nDeduplication for Parquet is enabled through Parquet Content Defined Chunking (CDC). Thanks to Parquet CDC and Xet deduplication, uploading datasets on Hugging Face is faster than on any traditional remote storage.\n\nThis is supported by our pyspark_huggingface package, a Spark Data Source to read/write HF datasets. It includes Parquet CDC and Xet support, accelerating data transfers on HF dramatically.\n\nNeed a custom streaming pipeline ?\n\nSome data file formats are not supported in datasets , and sometimes there is a need for more control, so we made it easy to build custom streaming pipelines. This has been battle-tested in the LeRobot library to sample video frames, and in the WebDataset library to stream TAR archives.\n\nWe improved the HfFileSystem in the huggingface_hub library to efficiently read files from remote Hugging Face dataset repositories and stream data:\n\nfrom huggingface_hub import HfFileSystem path = f\"hf://datasets/ {dataset_id} / {path_in_repo} \" with HfFileSystem(). open (path) as f:\n\nPassing a HfFileSystem to a torch DataLoader reuses the cached results from .ls() and .glob() which eliminates the need for additional requests when listing data files.\n\nPush streaming to the limit\n\nWe're now using these streaming enhancements in nanoVLM to train the next generation of SmolVLMs. With these tweaks, we achieve better performance from streaming than from training on our cluster's hierarchical hard disk setup. In fact, streaming is now as fast as reading the data from local SSDs! Previously, transferring data to local SSDs was the process that used to delay our trainings by three-hours. For more details, check out our GitHub.\n\nGet Started and See the Difference\n\nThese powerful new features landed in the datasets and huggingface_hub libraries. To take advantage of them, simply update your libraries and check out the documentation:\n\npip install --upgrade datasets huggingface_hub\n\nTo celebrate this, we preconcatenated and shuffled all the data sources in FineVision into FineVisionMax. You can use this single combined dataset to train your VLM \u2013 no need to handle multiple datasets manually!\n\nfrom datasets import load_dataset dataset = load_dataset( \"HuggingFaceM4/FineVisionMax\" , split= \"train\" , streaming= True ) print ( next ( iter (dataset)))\n\nAnd you can see how we do it at scale in nanoVLM!\n\nHappy streaming! \ud83e\udd17",
    "link": "https://huggingface.co/blog/streaming-datasets",
    "Summary": "Streaming datasets: 100x More EfficientPublished October 27, 2025 Update on GitHubWe boosted load_dataset('dataset', streaming=True) , streaming datasets without downloading them with one line of code!\nWe've improved streaming to have 100x fewer requests, \u2192 10\u00d7 faster data resolution \u2192 2x sample/sec, \u2192 0 worker crashes at 256 concurrent workers.\nStreaming has always been possible in the datasets library, but large scale training with massive datasets remained a challenge.\nWe spent a few months improving the backend, focusing on streaming datasets to make it faster and more efficient.\nStreaming \ud83c\udfce\ufe0f To improve throughput during streaming itself, we've introduced two new features:Prefetching for Parquet: We enabled prefetching for Parquet datasets.",
    "Keywords": [
      "data",
      "100x",
      "parquet",
      "training",
      "requests",
      "efficient",
      "train",
      "dataset",
      "xet",
      "faster",
      "datasets",
      "streaming"
    ]
  },
  {
    "Title": "huggingface_hub v1.0: Five Years of Building the Foundation of Open Machine Learning",
    "Authors": [],
    "Publish Date": null,
    "Text": "huggingface_hub v1.0: Five Years of Building the Foundation of Open Machine Learning\n\nPublished October 27, 2025 Update on GitHub\n\nhuggingface_hub\n\nAfter five years of development,has reached v1.0 - a milestone that marks the library's maturity as the Python package poweringand providing core functionality for accessing over 2 million public models, 0.5 million public datasets, and 1 million public Spaces. This release introduces breaking changes designed to support the next decade of open machine learning, driven by a global community of almost 300 contributors and millions of users.\n\n\ud83d\ude80 We highly recommend upgrading to v1.0 to benefit from major performance improvements and new capabilities.\n\npip install --upgrade huggingface_hub\n\nMajor changes in this release include the migration to httpx as the backend library, a completely redesigned hf CLI (which replaces the deprecated huggingface-cli ) featuring a Typer-based interface with a significantly expanded feature set, and full adoption of hf_xet for file transfers, replacing the legacy hf_transfer . You can find the full release notes here.\n\nWe\u2019ve worked hard to ensure that huggingface_hub v1.0.0 remains backward compatible. In practice, most ML libraries should work seamlessly with both v0.x and v1.x versions. The main exception is transformers , which explicitly requires huggingface_hub v0.x in its v4 releases and v1.x in its upcoming v5 release. For a detailed compatibility overview across libraries, refer to the table in this issue.\n\nThe Story Behind the Library\n\nEvery major library has a story. For huggingface_hub , it began with a simple idea: what if sharing machine learning models could be as easy as sharing code on GitHub?\n\nIn the early days of the Hugging Face Hub, researchers and practitioners faced a common frustration. Training a state-of-the-art model required significant compute resources and expertise. Once trained, these models often lived in isolation, stored on local machines and shared via (broken) Google Drive links. The AI community was duplicating work, wasting resources, and missing opportunities for collaboration.\n\nThe Hugging Face Hub emerged as the answer to this challenge. Initially, it was primarily used to share checkpoints compatible with the transformers library. All the Python code for interacting with the Hub lived within this library, making it inaccessible for other libraries to reuse.\n\nIn late 2020, we shipped huggingface_hub v0.0.1 with a simple mission: extract the internal logic from transformers and create a dedicated library that would unify how to access and share machine learning models and datasets on the Hugging Face Hub. Initially, the library was as straightforward as a Git wrapper for downloading files and managing repositories. Five years and 35+ releases later, huggingface_hub has evolved far beyond its origins.\n\nLet's trace that journey.\n\nThe Foundation Years (2020-2021)\n\nThe early releases established the basics. Version 0.0.8 introduced our first APIs, wrapping Git commands to interact with repositories. Version 0.0.17 brought token-based authentication, enabling secure access to private repositories and uploads. These were humble beginnings, but they laid the groundwork for everything that followed.\n\nThe Great Shift: Git to HTTP (2022)\n\nIn June 2022, version 0.8.1 marked a pivotal moment: we introduced the HTTP Commit API. Instead of requiring Git and Git LFS installations, users could now upload files directly through HTTP requests. The new create_commit() API simplified workflows dramatically, especially for large model files that are cumbersome to use with Git LFS. In addition, a git-aware cache file layout was introduced. All libraries (not only transformers, but third party ones as well) would now share the same cache, with explicit versioning and file deduplication.\n\nThis wasn't just a technical improvement. It was a philosophical shift. We were no longer building a Git wrapper for transformers; we were building purpose-built infrastructure for machine learning artifacts that could power any library in the ML ecosystem.\n\nAn Expanding API Surface (2022\u20132024)\n\nAs the Hub grew from a model repository into a full platform, huggingface_hub kept pace with an expanding API surface. Core repository primitives matured: listing trees, browsing refs and commits, reading files or syncing folders, managing tags, branches, and release cycles. Repository metadata and webhooks rounded up the offering so teams could react to changes in real time.\n\nIn parallel, Spaces emerged as a simple yet powerful way to host and share interactive ML demos directly on the Hub. Over time, huggingface_hub gained full programmatic control to deploy and manage Spaces (hardware requests, secrets, environment configuration, uploads). To deploy models on production-scale infrastructure, Inference Endpoints were integrated as well. Finally, the Jobs API came later (Q3 2025) to complete our compute offering.\n\nThe social and community layers became first-class citizens too: from APIs for pull requests and comments, to user and organization info, repository likes, following and followers, all the way through Collections to curate and share sets of related resources. Everyday ergonomics improved too: seamless authentication in Colab, resumable downloads, reliable uploads of large-scale folders, and more.\n\nThen came version 0.28.0 and the Inference Providers ecosystem. Instead of a single inference backend, we partnered with multiple serverless providers (Together AI, SambaNova, Replicate, Cerebras, Groq, and more) to serve one API with transparent routing. We adopted a pay-per-request inference architecture that matched how people actually wanted to work.\n\nVersion 0.30.0 introduced Xet, a groundbreaking new protocol for storing large objects in Git repositories. Unlike Git LFS, which deduplicates at the file level, Xet operates at the chunk level (64KB chunks). When you update a large file in a dataset or a model, only the changed chunks are uploaded or downloaded, not the entire file.\n\nThe migration was massive, starting with 20 petabytes across over 500,000 repositories. Yet it happened transparently, with full backward compatibility. One year later, all 77PB+ over 6,000,000 repositories have been migrated to the Xet backend, allowing for much faster (and smarter!) uploads and downloads. This happened with no user intervention, and no disruption to existing workflows \ud83d\udd25\n\nMeasuring Growth and Impact\n\nMeasuring the growth and impact of an open-source library is a tricky task. Numbers tell a story of their own:\n\n113.5 million monthly downloads , 1.6 billion total (October 2025).\n\n, total (October 2025). Powers access to 2M+ public models, 500k+ public datasets, 1M+ public Spaces, and about twice as much when accounting for private repos.\n\npublic models, public datasets, public Spaces, and about twice as much when accounting for private repos. Used by 60k+ users daily, 550k+ monthly\n\nusers daily, monthly Trusted by 200k+ companies from startups to Fortune 500\n\nBut the real scale becomes clear when you look at the ecosystem. huggingface_hub is a dependency for over 200,000 repositories on GitHub and 3,000 packages on PyPI, powering everything from major third-party frameworks like Keras, LangChain, PaddleOCR, ChatTTS, YOLO, Google Generative AI, Moshi, NVIDIA NeMo, and Open Sora, to countless smaller libraries and tools across the ML landscape. Our own ecosystem (transformers, diffusers, datasets, sentence-transformers, lighteval, gradio, peft, trl, smolagents, timm, lerobot, etc.) benefits from this foundation as well.\n\nThe remarkable part? Most of the third-party integrations happened organically, and we played no role in them. The Hugging Face Hub empowers the ML community in countless ways, yet we're continually humbled by how far it has gone and how widely it's used.\n\nBuilding for the Next Decade\n\nVersion 1.0 isn't just about reaching a milestone. It's about building the foundation for the next decade of open machine learning. The breaking changes we've made aren't arbitrary; they're strategic decisions that position huggingface_hub to scale with the explosive growth of AI while maintaining the reliability that millions of developers depend on.\n\nModern HTTP Infrastructure with httpx and hf_xet\n\nThe most significant architectural change in v1.0 is our migration from requests to httpx . This isn't just dependency churn. It's a fundamental upgrade that brings the library into the modern era of HTTP.\n\nWhy httpx? The benefits are substantial: native HTTP/2 support for better connection efficiency and true thread safety that enables safe connection reuse across multiple threads. Most importantly, httpx provides a unified API for both synchronous and asynchronous operations, eliminating the subtle behavioral differences that existed between our sync and async inference clients.\n\nThe migration was designed to be as transparent as possible. Most users won't need to change anything. For those with custom HTTP backends, we've provided clear migration paths from configure_http_backend() to set_client_factory() and set_async_client_factory() .\n\nAdditionally, hf_xet is now the default package for uploading and downloading files to and from the Hub, replacing the previously optional hf_transfer , which has now been fully removed.\n\nAgents Made Simple with MCP and Tiny-Agents\n\nVersion 0.32.0 introduced Model Context Protocol (MCP) integration and tiny-agents, fundamentally changing how developers build AI agents. What once required complex framework integration now takes approximately 70 lines of Python.\n\nThe MCPClient provides a standardized way for AI agents to interact with tools, while the tiny-agents CLI lets you run agents directly from the Hub. Connect to local or remote MCP servers, use any Gradio Space as a tool, and build conversational agents that feel natural and responsive.\n\nAll of this is built on top of our existing InferenceClient and the dozens of Inference Providers it supports. We do believe Agents are the future, and huggingface_hub is there to provide the building blocks that enable AI builders to play with them.\n\nA Fully-Featured CLI for Modern Workflows\n\nThe CLI has evolved from a simple command-line tool into a comprehensive interface for ML operations. The streamlined hf command replaces the legacy huggingface-cli with a modern resource-action pattern:\n\nhf auth login for authentication\n\nfor authentication hf download and hf upload for file transfers\n\nand for file transfers hf repo for repository management\n\nfor repository management hf cache ls and hf cache rm for cache management\n\nand for cache management hf jobs run for cloud compute\n\nThe CLI comes with a sandboxed installer, making it easy to upgrade without breaking existing dev environments:\n\n# On macOS or Linux curl -LsSf https://hf.co/cli/install.sh | sh # or on Windows powershell -ExecutionPolicy ByPass -c \"irm https://hf.co/cli/install.ps1 | iex\"\n\nWith autocompletion support and an installer that works across platforms, the CLI now feels as polished as any modern developer tool.\n\nCleaning House for the Future\n\nVersion 1.0 removes legacy patterns that were holding us back. The Git-based Repository class is gone. HTTP-based methods like upload_file() and create_commit() are simpler, more reliable, and better suited for modern workflows. The HfFolder token management has been replaced with explicit login() , logout() , and get_token() functions. The old InferenceApi class has been superseded by the more feature-complete InferenceClient . hf_transfer has been fully replaced by hf_xet binary package.\n\nThese changes weren't made lightly. Most deprecations were announced months in advance with clear warnings and migration guidance. The result is a cleaner, more maintainable codebase that can focus on forward-looking features rather than supporting deprecated patterns.\n\nThe Migration Guide\n\nWe understand that breaking changes are disruptive. That's why we've invested heavily in making the migration as smooth as possible. Our comprehensive migration guide provides step-by-step instructions for every change with explanations of why each change was necessary.\n\nMost importantly, we've maintained backward compatibility wherever possible. HfHubHttpError , for example, inherits from both the old requests and new httpx base HTTPError classes, ensuring that error handling continues to work across versions. With this release, we're fully committing to the future and we will focus exclusively on v1.0 and beyond, ensuring we can deliver the performance, features, and tools the community needs to interact with the Hugging Face Hub. Previous v0.* versions will remain available on PyPI, but they will only receive vulnerability updates.\n\nWe\u2019ve worked hard to ensure that huggingface_hub v1.0.0 remains backward compatible. In practice, most ML libraries should work seamlessly with both v0.x and v1.x versions. The main exception is transformers , which explicitly requires huggingface_hub v0.x in its v4 releases and v1.x in its upcoming v5 release. For a detailed compatibility overview across libraries, refer to the table in this issue.\n\nAcknowledgments\n\nTo our 280+ contributors who built this library through code, documentation, translations, and community support, thank you!\n\nWe\u2019re also deeply grateful to the entire Hugging Face community for their feedback, bug reports, and suggestions that have shaped this library.\n\nFinally, a huge thank you to our users -from individual developers to large enterprises- for trusting huggingface_hub to power your workflows. Your support drives us to keep improving and innovating.\n\nPlease star us on GitHub \u2b50 to show your support and help us continue building the foundation of open machine learning. It has been five years, but it is still only the beginning!",
    "link": "https://huggingface.co/blog/huggingface-hub-v1",
    "Summary": "Initially, it was primarily used to share checkpoints compatible with the transformers library.\nVersion 0.30.0 introduced Xet, a groundbreaking new protocol for storing large objects in Git repositories.\npublic models, public datasets, public Spaces, and about twice as much when accounting for private repos.\nIt's about building the foundation for the next decade of open machine learning.\nPlease star us on GitHub \u2b50 to show your support and help us continue building the foundation of open machine learning.",
    "Keywords": [
      "machine",
      "foundation",
      "transformers",
      "building",
      "hf",
      "library",
      "hub",
      "repository",
      "repositories",
      "open",
      "git",
      "migration",
      "learning",
      "huggingface_hub",
      "v10",
      "public"
    ]
  },
  {
    "Title": "LeRobot v0.4.0: Supercharging OSS Robot Learning",
    "Authors": [],
    "Publish Date": null,
    "Text": "LeRobot v0.4.0: Supercharging OSS Robot Learning\n\nPublished October 24, 2025 Update on GitHub\n\nWe're thrilled to announce a series of significant advancements across LeRobot, designed to make open-source robot learning more powerful, scalable, and user-friendly than ever before! From revamped datasets to versatile editing tools, new simulation environments, and a groundbreaking plugin system for hardware, LeRobot is continuously evolving to meet the demands of cutting-edge embodied AI.\n\nLeRobot v0.4.0 delivers a major upgrade for open-source robotics, introducing scalable Datasets v3.0, powerful new VLA models like PI0.5 and GR00T N1.5, and a new plugin system for easier hardware integration. The release also adds support for LIBERO and Meta-World simulations, simplified multi-GPU training, and a new Hugging Face Robot Learning Course.\n\nDatasets: Ready for the Next Wave of Large-Scale Robot Learning\n\nWe've completely overhauled our dataset infrastructure with LeRobotDataset v3.0, featuring a new chunked episode format and streaming capabilities. This is a game-changer for handling massive datasets like OXE (Open X Embodiment) and Droid, bringing unparalleled efficiency and scalability.\n\nWhat's New in Datasets v3.0?\n\nChunked Episodes for Massive Scale: Our new format supports datasets at the OXE-level (> 400GB), enabling unprecedented scalability.\n\nEfficient Video Storage + Streaming: Enjoy faster loading times and seamless streaming of video data.\n\nUnified Parquet Metadata: Say goodbye to scattered JSONs! All episode metadata is now stored in unified, structured Parquet files for easier management and access.\n\nFaster Loading & Better Performance: Experience significantly reduced dataset initialization times and more efficient memory usage.\n\nWe've also provided a conversion script to easily migrate your existing v2.1 datasets to the new v3.0 format, ensuring a smooth transition. Read more about it in our previous blog post. Open-source robotics keeps leveling up!\n\nNew Feature: Dataset Editing Tools!\n\nWorking with LeRobot datasets just got a whole lot easier! We've introduced a powerful set of utilities for flexible dataset editing.\n\nWith our new lerobot-edit-dataset CLI, you can now:\n\nDelete specific episodes from existing datasets.\n\nSplit datasets by fractions or episode indices.\n\nAdd or remove features with ease.\n\nMerge multiple datasets into one unified set.\n\nlerobot-edit-dataset \\ --repo_id lerobot/pusht_merged \\ --operation.type merge \\ --operation.repo_ids \"['lerobot/pusht_train', 'lerobot/pusht_val']\" lerobot-edit-dataset \\ --repo_id lerobot/pusht \\ --new_repo_id lerobot/pusht_after_deletion \\ --operation.type delete_episodes \\ --operation.episode_indices \"[0, 2, 5]\"\n\nThese tools streamline your workflow, allowing you to curate and optimize your robot datasets like never before. Check out the docs for more details!\n\nSimulation Environments: Expanding Your Training Grounds\n\nWe're continuously expanding LeRobot's simulation capabilities to provide richer and more diverse training environments for your robotic policies.\n\nLIBERO Support\n\nLeRobot now officially supports LIBERO, one of the largest open benchmarks for Vision-Language-Action (VLA) policies, boasting over 130 tasks! This is a huge step toward building the go-to evaluation hub for VLAs, enabling easy integration and a unified setup for evaluating any VLA policy.\n\nCheck out the LIBERO dataset and our docs to get started!\n\nMeta-World Integration\n\nWe've integrated Meta-World, a premier benchmark for testing multi-task and generalization abilities in robotic manipulation, featuring over 50 diverse manipulation tasks. This integration, along with our standardized use of gymnasium \u2265 1.0.0 and mujoco \u2265 3.0.0 , ensures deterministic seeding and a robust simulation foundation.\n\nTrain your policies with the Meta-World dataset today!\n\nCodebase: Powerful Tools For Everyone\n\nWe're making robot control more flexible and accessible, enabling new possibilities for data collection and model training.\n\nThe New Pipeline for Data Processing\n\nGetting data from a robot to a model (and back!) is tricky. Raw sensor data, joint positions, and language instructions don't match what AI models expect. Models need normalized, batched tensors on the right device, while your robot hardware needs specific action commands.\n\nWe're excited to introduce Processors: a new, modular pipeline that acts as a universal translator for your data. Think of it as an assembly line where each ProcessorStep handles one specific job\u2014like normalizing, tokenizing text, or moving data to the GPU.\n\nYou can chain these steps together into a powerful pipeline to perfectly manage your data flow. We've even created two distinct types to make life easier:\n\nPolicyProcessorPipeline : Built for models. It expertly handles batched tensors for high-performance training and inference.\n\n: Built for models. It expertly handles batched tensors for high-performance training and inference. RobotProcessorPipeline : Built for hardware. It processes individual data points (like a single observation or action) for real-time robot control.\n\nobs = robot.get_observation() obs_processed = preprocess(obs) action = model.select_action(obs_processed) action_processed = postprocess(action) robot.send_action(action_processed)\n\nThis system makes it simple to connect any policy to any robot, ensuring your data is always in the perfect format for every step of the way. Learn more about it in our Introduction to Processors documentation.\n\nMulti-GPU Training Made Easy\n\nTraining large robot policies just got a lot faster! We've integrated Accelerate directly into our training pipeline, making it incredibly simple to scale your experiments across multiple GPUs with just one command:\n\naccelerate launch \\ --multi_gpu \\ --num_processes= $NUM_GPUs \\ $( which lerobot-train) \\ --dataset.repo_id= ${HF_USER} /my_dataset \\ --policy.repo_id= ${HF_USER} /my_trained_policy \\ --policy.type= $POLICY_TYPE \\\n\nWhether you're fine-tuning a policy or running large-scale experiments, LeRobot now handles all the complexities of distributed training for you. This means you can drastically reduce training time, cutting it in half with 2 GPUs, down to a third with 3 GPUs, and beyond.\n\nCheck out the documentation to accelerate your robot learning!\n\nPolicies: Unleashing Open-World Generalization\n\nPI0 and PI0.5\n\nIn a major milestone for open-source robotics, we've integrated pi0 and pi0.5 policies by Physical Intelligence into LeRobot! These Vision-Language-Action (VLA) models represent a significant leap towards addressing open-world generalization in robotics. But what makes \u03c0\u2080.\u2085 revolutionary?\n\nOpen-World Generalization: Designed to adapt to entirely new environments and situations, generalizing across physical, semantic, and environmental levels.\n\nCo-training on Heterogeneous Data: Learns from a diverse mix of multimodal web data, verbal instructions, subtask commands, and multi-environment robot data.\n\nPhysical Intelligence Collaboration: Huge thanks to the Physical Intelligence team for their groundbreaking work!\n\nYou can find the models on the Hugging Face Hub: pi0.5_base, pi0_base, and their Libero-tuned counterparts. For more details, checkout the Physical Intelligence Reasearch\n\nGR00T N1.5\n\nIn another exciting development, we've integrated NVIDIA's GR00T N1.5 into LeRobot, thanks to a fantastic collaboration with the NVIDIA robotics team! This open foundation model is a powerhouse for generalized robot reasoning and skills. As a cross-embodiment model, it takes multimodal input (like language and images) to perform complex manipulation tasks in diverse environments, marking another major leap in generalized robotics. But what makes GR00T N1.5 a game-changer?\n\nGeneralized Reasoning & Skills: Designed as a cross-embodiment foundation model, GR00T N1.5 excels at generalized reasoning and manipulation tasks, with improved language-following ability.\n\nExpansive Heterogeneous Training: It learns from a massive dataset combining real captured humanoid data, synthetic data generated by NVIDIA Isaac GR00T Blueprint, and internet-scale video data.\n\nNVIDIA Collaboration: We're thrilled to partner with the NVIDIA team to bring this state-of-the-art model to the open-source LeRobot community!\n\nYou can find the model on the Hugging Face Hub: GR00T-N1.5-3B. For more details, check out the NVIDIA research page and the official GitHub repository.\n\nThe native integration of these policies in lerobot is a huge step forward in making robot learning as open and reproducible as it can be. Try them out today, share your runs, and let's push forward the frontier of embodied AI together!\n\nRobots: A New Era of Hardware Integration with the Plugin System\n\nBig news for hardware enthusiasts! We've launched a brand-new plugin system to revolutionize how you integrate third-party hardware with LeRobot. Now, connecting any robot, camera, or teleoperator is as simple as a pip install , eliminating the need to modify the core library.\n\nKey Benefits\n\nExtensibility: Develop and integrate custom hardware in separate Python packages.\n\nScalability: Supports a growing ecosystem of devices without bloating the core library.\n\nCommunity-Friendly: Lowers the barrier to entry for community contributions, fostering a more collaborative environment.\n\nLearn how to create your own plugin in our documentation.\n\npip install lerobot_teleoperator_my_awesome_teleop lerobot-teleoperate --teleop.type=my_awesome_teleop\n\nReachy 2 Integration\n\nThanks to our new plugin system, we've also added Reachy 2 from Pollen Robotics to LeRobot! Reachy 2 is available for both real robot control and simulation, enabling you to experiment with teleoperation and autonomous demos right away.\n\nPhone Integration\n\nThanks to our powerful new pipeline system, you can now teleoperate your follower arm right from your phone (iOS/Android). The phone acts as a teleoperator device, and our RobotProcessor pipeline handles all the transformations, allowing you to drive robots in different action spaces (like end-effector space) with ease. Check out the examples!\n\nThe Hugging Face Robot Learning Course\n\nWe're launching a comprehensive, self-paced, and entirely open-source course designed to make robot learning accessible to everyone! If you're curious about how real-world robots learn, this is the perfect place to start.\n\nIn this course, you\u2019ll learn how to:\n\nUnderstand the fundamentals of classical robotics.\n\nUse generative models for imitation learning (VAEs, diffusion, etc.).\n\nApply Reinforcement Learning to real-world robots.\n\nExplore the latest generalist robot policies like PI0 and SmolVLA.\n\nJoin the Hugging Face Robotics organization to follow along and start your journey!\n\nDeep Dive: The Modern Robot Learning Tutorial\n\nFor those who want to go deeper, we've also published a hands-on tutorial on the most recent advancements in robotics. This guide provides self-contained explanations, re-derives modern techniques from first principles, and includes ready-to-use code examples using LeRobot and Hugging Face.\n\nThe tutorial itself is hosted in a Space and it features practical examples using LeRobot, with all models and datasets on the Hugging Hub. You can also check out our paper for a detailed overview.\n\nFinal thoughts from the team\n\nBeyond these major features, this release is packed with numerous bug fixes, documentation improvements, updated dependencies, more examples and better infrastructure to make your experience with LeRobot smoother and more reliable.\n\nWe want to extend a huge thank you to everyone in the community for your invaluable contributions, feedback, and support. We're incredibly excited about the future of open-source robotics and can't wait to work with you on what's next!\n\nStay tuned for more to come \ud83e\udd17 Get started here! \u2013 The LeRobot team \u2764\ufe0f",
    "link": "https://huggingface.co/blog/lerobot-release-v040",
    "Summary": "LeRobot v0.4.0: Supercharging OSS Robot LearningPublished October 24, 2025 Update on GitHubWe're thrilled to announce a series of significant advancements across LeRobot, designed to make open-source robot learning more powerful, scalable, and user-friendly than ever before!\nThe release also adds support for LIBERO and Meta-World simulations, simplified multi-GPU training, and a new Hugging Face Robot Learning Course.\nCo-training on Heterogeneous Data: Learns from a diverse mix of multimodal web data, verbal instructions, subtask commands, and multi-environment robot data.\nThe native integration of these policies in lerobot is a huge step forward in making robot learning as open and reproducible as it can be.\nThe Hugging Face Robot Learning CourseWe're launching a comprehensive, self-paced, and entirely open-source course designed to make robot learning accessible to everyone!",
    "Keywords": [
      "oss",
      "data",
      "models",
      "training",
      "lerobot",
      "weve",
      "robotics",
      "hardware",
      "v040",
      "datasets",
      "supercharging",
      "learning",
      "robot"
    ]
  },
  {
    "Title": "Building the Open Agent Ecosystem Together: Introducing OpenEnv",
    "Authors": [],
    "Publish Date": null,
    "Text": "Building the Open Agent Ecosystem Together: Introducing OpenEnv\n\nPublished October 23, 2025 Update on GitHub\n\nWith tools like TRL TorchForge and verl , the open-source community has shown how to scale AI across complex compute infrastructure. But compute is only one side of the coin. The other side is the developer community; the people and tools that make agentic systems possible. That\u2019s why Meta and Hugging Face are partnering to launch the OpenEnv Hub : a shared and open community hub for agentic environments.\n\nAgentic environments define everything an agent needs to perform a task: the tools, APIs, credentials, execution context, and nothing else. They bring clarity, safety, and sandboxed control to agent behavior.\n\nThese environments can be used for both training and deployment, and serve as the foundation for scalable agentic development.\n\nThe Problem\n\nModern AI agents can act autonomously across thousands of tasks. However, a large language model isn\u2019t enough to get those tasks to actually run \u2014 it needs access to the right tools. Exposing millions of tools directly to a model isn\u2019t reasonable (or safe). Instead, we need agentic environments: secure, semantically clear sandboxes that define exactly what\u2019s required for a task, and nothing more. These environments handle the critical details:\n\nClear semantics about what a task needs\n\nSandboxed execution and safety guarantees\n\nSeamless access to authenticated tools and APIs\n\nThe Solution\n\nTo supercharge this next wave of agentic development, Meta-PyTorch and Hugging Face are partnering to launch a Hub for Environments: a shared space where developers can build, share, and explore OpenEnv-compatible environments for both training and deployment. The figure below shows how OpenEnv fits in the new post-training stack being developed by Meta, with integrations for other libraries like TRL, SkyRL, and Unsloth underway:\n\nStarting next week, developers can:\n\nVisit the new Environment Hub on Hugging Face where we will seed some initial environments\n\nInteract with environments directly as a Human Agent\n\nEnlist a model to solve tasks within the environment\n\nInspect which tools the environment exposes and how it defines its observations\n\nEvery environment uploaded to the Hub that conforms to the OpenEnv specification automatically gains this functionality \u2014 making it fast and easy to validate and iterate before running full RL training.\n\nAlongside this, we\u2019re releasing the OpenEnv 0.1 Spec (RFC) to gather community feedback and help shape the standard.\n\nThe RFCs\n\nIn the current state of the repository, environment creators can create environments using step() , reset() , close() APIs (part of RFCs below). A few examples on how to create such environments can be seen here. Environment users can play with local Docker based environments for all environments already available in the repo. Following RFCs are under review:\n\nRFC 001: Establish architecture for how the core components like Environment, Agent, Task, etc. are related\n\nRFC 002: Propose basic env interface, packaging, isolation and communication w/ environment.\n\nRFC 003: Propose encapsulation of MCP tools through environment abstraction and isolation boundaries\n\nRFC 004: Extend tool support to cover unified action schema covering tool calling agents as well as CodeAct paradigm.\n\nUse cases\n\nRL Post training: pull in environments across collections and use them to train RL agents with TRL, TorchForge+Monarch, VeRL etc.\n\nEnvironment creation: build an environment and ensure that it interops with popular RL tools in the ecosystem, share with collaborators, etc.\n\nReproduction of SOTA methods: easily replicate methods like those from FAIR's Code World Model by integrating environments for agentic coding and software engineering.\n\nDeployment: users can create an environment, train on the same environment and then use the same for inference too (the full pipeline)\n\nWhat\u2019s Next\n\nThis is just the beginning. We\u2019re integrating the OpenEnv Hub with Meta\u2019s new TorchForge RL library, and collaborating with other open-source RL projects such as verl, TRL, and SkyRL to expand compatibility. Join us at the PyTorch Conference on Oct 23 for a live demo and walkthrough of the spec, and stay tuned for our upcoming community meetup on environments, RL post-training, and agentic development.\n\n\ud83d\udc49 Explore the OpenEnv Hub on Hugging Face and start building the environments that will power the next generation of agents.\n\n\ud83d\udc49 Check out the 0.1 spec which can be found implemented in the OpenEnv project \u2192 we welcome ideas and contributions to making it better!\n\n\ud83d\udc49 Engage on Discord and talk with the community about RL, environments and agentic development\n\n\ud83d\udc49 Try it out yourself - We created a comprehensive notebook that walks you through an end to end example and of course you can easily pip install the package via PyPI. This notebook walks you through the abstractions we\u2019ve built, along with an example of how to use existing integrations and how to add yours - Try it out in Google Colab!\n\n\ud83d\udc49 Check out supporting platforms - Unsloth, TRL, Lightning.AI\n\nLet's build the future of open agents together, one environment at a time \ud83d\udd25!",
    "link": "https://huggingface.co/blog/openenv",
    "Summary": "Building the Open Agent Ecosystem Together: Introducing OpenEnvPublished October 23, 2025 Update on GitHubWith tools like TRL TorchForge and verl , the open-source community has shown how to scale AI across complex compute infrastructure.\nThat\u2019s why Meta and Hugging Face are partnering to launch the OpenEnv Hub : a shared and open community hub for agentic environments.\nAgentic environments define everything an agent needs to perform a task: the tools, APIs, credentials, execution context, and nothing else.\nInstead, we need agentic environments: secure, semantically clear sandboxes that define exactly what\u2019s required for a task, and nothing more.\n\ud83d\udc49 Explore the OpenEnv Hub on Hugging Face and start building the environments that will power the next generation of agents.",
    "Keywords": [
      "trl",
      "agents",
      "ecosystem",
      "agentic",
      "environment",
      "building",
      "agent",
      "hub",
      "openenv",
      "open",
      "rl",
      "environments",
      "tools",
      "community",
      "introducing"
    ]
  },
  {
    "Title": "Hugging Face and VirusTotal collaborate to strengthen AI security",
    "Authors": [],
    "Publish Date": null,
    "Text": "Hugging Face and VirusTotal collaborate to strengthen AI security\n\nPublished October 22, 2025 Update on GitHub\n\nWe\u2019re excited to announce a new collaboration between Hugging Face and VirusTotal , the world\u2019s leading threat-intelligence and malware analysis platform. This collaboration enhances the security of files shared across the Hugging Face Hub, helping protect the machine learning community from malicious or compromised assets.\n\nTL;DR - Starting today, every one of the 2.2M+ public model and datasets repositories on the Hugging Face Hub is being continuously scanned with VirusTotal.\n\nWhy this matters\n\nAI models are powerful but they\u2019re also complex digital artifacts that can include large binary files, serialized data, and dependencies that sometimes carry hidden risks. As of today HF Hub hosts 2.2 Million Public model artifacts. As we continue to grow into the world\u2019s largest open platform for Machine Learning models and datasets, ensuring that shared assets remain safe is essential.\n\nThreats can take many forms:\n\nMalicious payloads disguised as model files or archives\n\nFiles that have been compromised before upload\n\nBinary assets linked to known malware campaigns\n\nDependencies or serialized objects that execute unsafe code when loaded\n\nBy collaborating with VirusTotal, we\u2019re adding an extra layer of protection and visibility by enabling files shared through Hugging Face to be checked against one of the largest and most trusted malware intelligence databases in the world.\n\nHow the collaboration works\n\nWhenever you visit a repository page or a file or directory page, the Hub will automatically retrieve VirusTotal information about the corresponding files. Example\n\nHere\u2019s what happens:\n\nWe compare the file hash against VirusTotal\u2019s threat-intelligence database.\n\nIf a file hash has been previously analyzed by VirusTotal, its status (clean or malicious) is retrieved.\n\nNo raw file contents are shared with VirusTotal maintaining user privacy and compliance with Hugging Face\u2019s data protection principles.\n\nResults include metadata such as detection counts, known-bad relationships, or associated threat-campaign intelligence where relevant.\n\nThis provides valuable context to users and organizations before they download or integrate files from the Hub.\n\nBenefits for the community\n\nTransparency: Users can see if files have been previously flagged or analyzed in VirusTotal\u2019s ecosystem.\n\nSafety: Organizations can integrate VirusTotal checks into their CI/CD or deployment workflows to help prevent the spread of malicious assets.\n\nEfficiency: Leveraging existing VirusTotal intelligence reduces the need for repeated or redundant scanning.\n\nTrust: Together, we\u2019re making the Hugging Face Hub a more secure, reliable place to collaborate on open-source AI.\n\nJoin us\n\nIf you\u2019d like to learn more about this integration or explore ways to contribute to a safer open-source AI ecosystem, reach out to security@huggingface.co.\n\nTogether, we can make AI collaboration not just open but secure by design.",
    "link": "https://huggingface.co/blog/virustotal",
    "Summary": "Hugging Face and VirusTotal collaborate to strengthen AI securityPublished October 22, 2025 Update on GitHubWe\u2019re excited to announce a new collaboration between Hugging Face and VirusTotal , the world\u2019s leading threat-intelligence and malware analysis platform.\nThis collaboration enhances the security of files shared across the Hugging Face Hub, helping protect the machine learning community from malicious or compromised assets.\nTL;DR - Starting today, every one of the 2.2M+ public model and datasets repositories on the Hugging Face Hub is being continuously scanned with VirusTotal.\nNo raw file contents are shared with VirusTotal maintaining user privacy and compliance with Hugging Face\u2019s data protection principles.\nTrust: Together, we\u2019re making the Hugging Face Hub a more secure, reliable place to collaborate on open-source AI.",
    "Keywords": [
      "files",
      "strengthen",
      "collaboration",
      "collaborate",
      "hub",
      "face",
      "hugging",
      "file",
      "malware",
      "ai",
      "security",
      "shared",
      "model",
      "virustotal"
    ]
  },
  {
    "Title": "Sentence Transformers is joining Hugging Face!",
    "Authors": [],
    "Publish Date": null,
    "Text": "Sentence Transformers is joining Hugging Face!\n\nPublished October 22, 2025 Update on GitHub\n\nToday, we are announcing that Sentence Transformers is transitioning from Iryna Gurevych\u2019s Ubiquitous Knowledge Processing (UKP) Lab at the TU Darmstadt to Hugging Face. Hugging Face's Tom Aarsen has already been maintaining the library since late 2023 and will continue to lead the project. At its new home, Sentence Transformers will benefit from Hugging Face's robust infrastructure, including continuous integration and testing, ensuring that it stays up-to-date with the latest advancements in Information Retrieval and Natural Language Processing.\n\nSentence Transformers (a.k.a. SentenceBERT or SBERT) is a popular open-source library for generating high-quality embeddings that capture semantic meaning. Since its inception by Nils Reimers in 2019, Sentence Transformers has been widely adopted by researchers and practitioners for various natural language processing (NLP) tasks, including semantic search, semantic textual similarity, clustering, and paraphrase mining. After years of development and training by and for the community, over 16,000 Sentence Transformers models are publicly available on the Hugging Face Hub, serving more than a million monthly unique users.\n\n\"Sentence Transformers has been a huge success story and a culmination of our long-standing research on computing semantic similarities for the whole lab. Nils Reimers has made a very timely discovery and has produced not only outstanding research outcomes, but also a highly usable tool. This continues to impact generations of students and practitioners in natural language processing and AI. I would also like to thank all the users and especially the contributors, without whom this project would not be what it is today. And finally, I would like to thank Tom and Hugging Face for taking the project into the future.\"\n\nProf. Dr. Iryna Gurevych, Director of the Ubiquitous Knowledge Processing Lab, TU Darmstadt\n\n\"We're thrilled to officially welcome Sentence Transformers into the Hugging Face family! Over the past two years, it\u2019s been amazing to see this project grow to massive global adoption, thanks to the incredible foundation from the UKP Lab and the amazing community around it. This is just the beginning: we\u2019ll keep doubling down on supporting its growth and innovation, while staying true to the open, collaborative spirit that made it thrive in the first place.\"\n\nClem Delangue, co-founder & CEO, Hugging Face\n\nSentence Transformers will remain a community-driven, open-source project, with the same open-source license (Apache 2.0) as before. Contributions from researchers, developers, and enthusiasts are welcome and encouraged. The project will continue to prioritize transparency, collaboration, and broad accessibility.\n\nProject History\n\nThe Sentence Transformers library was introduced in 2019 by Dr. Nils Reimers at the Ubiquitous Knowledge Processing (UKP) Lab at Technische Universit\u00e4t Darmstadt, under the supervision of Prof. Dr. Iryna Gurevych. Motivated by the limitations of standard BERT embeddings for sentence-level semantic tasks, Sentence-BERT used a Siamese network architecture to produce semantically meaningful sentence embeddings that could be efficiently compared using cosine similarity. Thanks to its modular, open-source design and strong empirical performance on tasks such as semantic textual similarity, clustering, and information retrieval, the library quickly became a staple in the NLP research toolkit, spawning a range of follow-up work and real-world applications that rely on high-quality sentence representations.\n\nIn 2020, multilingual support was added to the library, extending sentence embeddings to more than 400 languages. In 2021, with contributions from Nandan Thakur and Dr. Johannes Daxenberger, the library expanded to support pair-wise sentence scoring using Cross Encoder and Sentence Transformer models. Sentence Transformers was also integrated with the Hugging Face Hub (v2.0). For over four years, the UKP Lab team maintained the library as a community-driven open-source project and provided continued research-driven innovation. During this period, the project's development was supported by grants to Prof. Gurevych by the German Research Foundation (DFG), German Federal Ministry of Education and Research (BMBF), and Hessen State Ministry for Higher Education, Research and the Arts (HMWK).\n\nIn late 2023, Tom Aarsen from Hugging Face took over maintainership of the library, introducing modernized training for Sentence Transformer models (v3.0), as well as improvements of Cross Encoder (v4.0) and Sparse Encoder (v5.0) models.\n\nAcknowledgements\n\nThe Ubiquitous Knowledge Processing (UKP) Lab at Technische Universit\u00e4t Darmstadt, led by Prof. Dr. Iryna Gurevych, is internationally recognized for its research in natural language processing (NLP) and machine learning. The lab has a long track record of pioneering work in representation learning, large language models, and information retrieval, with numerous publications at leading conferences and journals. Beyond Sentence Transformers, the UKP Lab has developed a number of widely used datasets, benchmarks, and open-source tools that support both academic research and real-world applications.\n\nHugging Face would like to thank the UKP Lab and all past and present contributors, especially Dr. Nils Reimers and Prof. Dr. Iryna Gurevych, for their dedication to the project and for entrusting us with its maintenance and now stewardship. We also extend our gratitude to the community of researchers, developers, and practitioners who have contributed to the library's success through model contributions, bug reports, feature requests, documentation improvements, and real-world applications. We are excited to continue building on the strong foundation laid by the UKP Lab and to work with the community to further advance the capabilities of Sentence Transformers.\n\nGetting Started\n\nFor those new to Sentence Transformers or looking to explore its capabilities:",
    "link": "https://huggingface.co/blog/sentence-transformers-joins-hf",
    "Summary": "Sentence Transformers is joining Hugging Face!\nPublished October 22, 2025 Update on GitHubToday, we are announcing that Sentence Transformers is transitioning from Iryna Gurevych\u2019s Ubiquitous Knowledge Processing (UKP) Lab at the TU Darmstadt to Hugging Face.\nAfter years of development and training by and for the community, over 16,000 Sentence Transformers models are publicly available on the Hugging Face Hub, serving more than a million monthly unique users.\nProf. Dr. Iryna Gurevych, Director of the Ubiquitous Knowledge Processing Lab, TU Darmstadt\"We're thrilled to officially welcome Sentence Transformers into the Hugging Face family!\nSentence Transformers was also integrated with the Hugging Face Hub (v2.0).",
    "Keywords": [
      "joining",
      "project",
      "research",
      "library",
      "face",
      "hugging",
      "ukp",
      "processing",
      "transformers",
      "lab",
      "sentence"
    ]
  },
  {
    "Title": "Supercharge your OCR Pipelines with Open Models",
    "Authors": [],
    "Publish Date": null,
    "Text": "Supercharge your OCR Pipelines with Open Models\n\nPublished October 21, 2025 Update on GitHub\n\nTL;DR: The rise of powerful vision-language models has transformed document AI. Each model comes with unique strengths, making it tricky to choose the right one. Open-weight models offer better cost efficiency and privacy. To help you get started with them, we\u2019ve put together this guide.\n\nIn this guide, you\u2019ll learn:\n\nThe landscape of current models and their capabilities\n\nWhen to fine-tune models vs. use models out-of-the-box\n\nKey factors to consider when selecting a model for your use case\n\nHow to move beyond OCR with multimodal retrieval and document QA\n\nBy the end, you\u2019ll know how to choose the right OCR model, start building with it, and gain deeper insights into document AI. Let\u2019s go!\n\nBrief Introduction to Modern OCR\n\nOptical Character Recognition (OCR) is one of the earliest and longest running challenges in computer vision. Many of AI\u2019s first practical applications focused on turning printed text into digital form.\n\nWith the surge of vision-language models (VLMs), OCR has advanced significantly. Recently, many OCR models have been developed by fine-tuning existing VLMs. But today\u2019s capabilities extend far beyond OCR: you can retrieve documents by query or answer questions about them directly. Thanks to stronger vision features, these models can also handle low-quality scans, interpret complex elements like tables, charts, and images, and fuse text with visuals to answer open-ended questions across documents.\n\nModel Capabilities\n\nTranscription\n\nRecent models transcribe texts into a machine-readable format.\n\nThe input can include:\n\nHandwritten text\n\nVarious scripts like Latin, Arabic, and Japanese characters\n\nMathematical expressions\n\nChemical formulas\n\nImage/Layout/Page number tags\n\nOCR models convert them into machine-readable text that comes in many different formats like HTML, Markdown and more.\n\nHandling complex components in documents\n\nOn top of text, some models can also recognize:\n\nImages\n\nCharts\n\nTables\n\nSome models know where images are inside the document, extract their coordinates, and insert them appropriately between texts. Other models generate captions for images and insert them where they appear. This is especially useful if you are feeding the machine-readable output into an LLM. Example models are OlmOCR by AllenAI, or PaddleOCR-VL by PaddlePaddle.\n\nModels use different machine-readable output formats, such as DocTags, HTML or Markdown (explained in the next section Output Formats). The way a model handles tables and charts often depends on the output format they are using. Some models treat charts like images: they are kept as is. Other models convert charts into markdown tables or JSON, e.g., a bar chart can be converted as follows.\n\nSimilarly for tables, cells are converted into a machine-readable format while retaining context from headings and columns.\n\nOutput formats\n\nDifferent OCR models have different output formats. Briefly, here are the common output formats used by modern models.\n\nDocTag: DocTag is an XML-like format for documents that expresses location, text format, component-level information, and more. Below is an illustration of a paper parsed into DocTags. This format is employed by the open Docling models.\n\nHTML: HTML is one of the most popular output formats used for document parsing as it properly encodes structure and hierarchical information.\n\nHTML is one of the most popular output formats used for document parsing as it properly encodes structure and hierarchical information. Markdown: Markdown is the most human-readable format. It\u2019s simpler than HTML but not as expressive. For example, it can\u2019t represent split-column tables.\n\nMarkdown is the most human-readable format. It\u2019s simpler than HTML but not as expressive. For example, it can\u2019t represent split-column tables. JSON: JSON is not a format that models use for the entire output, but it can be used to represent information in tables or charts.\n\nThe right model depends on how you plan to use its outputs:\n\nDigital reconstruction : To reconstruct documents digitally, choose a model with a layout-preserving format (e.g., DocTags or HTML).\n\n: To reconstruct documents digitally, choose a model with a layout-preserving format (e.g., DocTags or HTML). LLM input or Q&A : If the use case involves passing outputs to LLM, pick a model that outputs Markdown and image captions, since they\u2019re closer to natural language.\n\n: If the use case involves passing outputs to LLM, pick a model that outputs Markdown and image captions, since they\u2019re closer to natural language. Programmatic use: If you want to pass your outputs to a program (like data analysis), opt for a model that generates structured outputs like JSON.\n\nLocality Awareness\n\nDocuments can have complex structures, like multi-column text blocks and floating figures. Older OCR models handled these documents by detecting words and then the layout of pages manually in post-processing to have the text rendered in reading order, which is brittle. Modern OCR models, on the other hand, incorporate layout metadata to help preserve reading order and accuracy. This metadata is called \u201canchor\u201d, it can come in bounding boxes. This process is also called as \u201cgrounding/anchoring\u201d because it helps with reducing hallucination.\n\nModel Prompting\n\nOCR models can either take in images and an optional text prompt, this depends on the model architecture and the pre-training setup.\n\nSome OCR models support prompt-based task switching, e.g. granite-docling can parse an entire page with the prompt \u201cConvert this page to Docling\u201d while it can also take prompts like \u201cConvert this formula to LaTeX\u201d along with a page full of formulas.\n\nOther models, however, are trained only for parsing entire pages, and they are conditioned to do this through a system prompt.\n\nFor instance, OlmOCR by AllenAI takes a long conditioning prompt. Like many others, OlmOCR is technically an OCR fine-tuned version of a VLM (Qwen2.5VL in this case), so you can prompt for other tasks, but its performance will not be on par with the OCR capabilities.\n\nCutting-edge Open OCR Models\n\nWe\u2019ve seen an incredible wave of new models this past year. Because so much work is happening in the open, these players build on and benefit from each other\u2019s work. A great example is AllenAI\u2019s release of OlmOCR, which not only released a model but also the dataset used to train it. With these, others can build upon them in new directions. The field is incredibly active, but it\u2019s not always obvious which model to use.\n\nComparing Latest Models\n\nTo make things a bit easier, we\u2019re putting together a non-exhaustive comparison of some of our current favorite models. All of the models below are layout-aware and can parse tables, charts, and math equations. The full list of languages each model supports are detailed in their model cards, so make sure to check them if you\u2019re interested. All models below have open-source license except for Chandra having OpenRAIL license and Nanonets license being unclear. The average scores are taken from model cards of Chandra, OlmOCR, evaluated on OlmOCR Benchmark, which is English-only. Many of the models in this collection have been fine-tuned from Qwen2.5-VL or Qwen3-VL, so we also provide Qwen3-VL model below as well.\n\nModel Name Output formats Features Model Size Multilingual? Average Score on OlmOCR Benchmark Nanonets-OCR2-3B structured Markdown with semantic tagging (plus HTML tables, etc.) Captions images in the documents\n\nSignature & watermark extraction\n\nHandles checkboxes, flowcharts, and handwriting 4B \u2705Supports English, Chinese, French, Arabic and more. N/A PaddleOCR-VL Markdown, JSON, HTML tables and charts Handles handwriting, old documents\n\nAllows prompting\n\nConverts tables & charts to HTML\n\nExtracts and inserts images directly 0.9B \u2705Supports 109 languages N/A dots.ocr Markdown, JSON Grounding\n\nExtracts and inserts images\n\nHandles handwriting 3B \u2705Multilingual with language info not available 79.1 \u00b1 1.0 OlmOCR-2 Markdown, HTML, LaTeX Grounding\n\nOptimized for large-scale batch processing 8B \u274eEnglish-only 82.3 \u00b1 1.1 Granite-Docling-258M DocTags Prompt-based task switching\n\nAbility to prompt element locations with location tokens\n\nRich output 258M \u2705Supports English, Japanese, Arabic and Chinese. N/A DeepSeek-OCR Markdown, HTML Supports general visual understanding\n\nCan parse and re-render all charts, tables, and more into HTML\n\nHandles handwriting\n\nMemory-efficient, solves text through image 3B \u2705Supports nearly 100 languages 75.4 \u00b1 1.0 Chandra Markdown, HTML, JSON Grounding\n\nExtracts and inserts images as is 9B \u2705Supports 40+ languages 83.1 \u00b1 0.9 Qwen3-VL Vision Language Model can output in all formats Can recognize ancient text\n\nHandles handwriting\n\nExtracts and inserts images as is 9B \u2705Supports 32 languages N/A\n\nWhile Qwen3-VL itself is a powerful and versatile vision-language model post-trained for document understanding and other tasks, it isn\u2019t optimized for a single, universal OCR prompt. In contrast, the other models were fine-tuned using one or a few fixed prompts specifically designed for OCR tasks. So to use Qwen3-VL, we recommend experimenting with prompts.\n\nHere\u2019s a small demo for you to try some of the latest models and compare their outputs.\n\nEvaluating Models\n\nBenchmarks\n\nThere\u2019s no single best model, as every problem has different needs. Should tables be rendered in Markdown or HTML? Which elements should we extract? How should we quantify text accuracy and error rates? \ud83d\udc40\n\nWhile there are many evaluation datasets and tools, many don\u2019t answer these questions. So we suggest using the following benchmarks:\n\nOmniDocBenchmark: This widely used benchmark stands out for its diverse document types: books, magazines, and textbooks. Its evaluation criteria are well designed, accepting tables in both HTML and Markdown formats. A novel matching algorithm evaluates the reading order, and formulas are normalized before evaluation. Most metrics rely on edit distance or tree edit distance (tables). Notably, the annotations used for evaluation are not solely human-generated but are acquired through SoTA VLMs or conventional OCR methods. OlmOCR-Bench: OlmOCR-Bench takes a different approach: they treat the evaluation as a set of unit tests. For example, table evaluation is done by checking the relation between selected cells of a given table. They use PDFs from public sources, and annotations are done using a wide range of closed-source VLMs. This benchmark is quite successful to evaluate on the English language. CC-OCR (Multilingual): Compared to the previous benchmarks, CC-OCR is less preferred when picking models, due to lower document quality and diversity. However, it\u2019s the only benchmark that contains evaluation beyond English and Chinese! While the evaluation is far from perfect (images are photos with few words), it\u2019s still the best you can do for multilingual evaluation.\n\nWhen testing different OCR models, we've found that the performance across different document types, languages, etc., varies a lot. Your domain may not be well represented in existing benchmarks! To make effective use of this new generation of VLM-based OCR models we suggest aiming to collect a dataset of representative examples of your task domain and testing a few different models to compare their performance.\n\nMost OCR models are small, having between 3B and 7B parameters; you can even find models with fewer than 1B parameters, like PaddleOCR-VL. However, the cost also depends on the availability of optimized implementations for specialized inference frameworks. For example, OlmOCR-2 comes with vLLM and SGLang implementations, and the cost per million pages is 178 dollars (assuming on H100 for $2.69/hour). DeepSeek-OCR can process 200k+ pages per day on a single A100 with 40GB VRAM. With napkin math, we see that the cost per million pages is more or less similar to OlmOCR (although it depends on your A100 provider). If your use case remains unaffected, you can also opt for quantized versions of the models. The cost of running open-source models heavily depends on the hourly cost of the instance and the optimizations the model includes, but it\u2019s guaranteed to be cheaper than many closed-source models out there on a larger scale.\n\nOpen OCR Datasets\n\nWhile the past year has seen a surge in open OCR models, this hasn't been matched by as many open training and evaluation datasets. An exception is AllenAI's olmOCR-mix-0225, which has been used to train at least 72 models on the Hub \u2013 likely more, since not all models document their training data.\n\nSharing more datasets could unlock even greater advances in open OCR models. There are several promising approaches for creating these datasets:\n\nSynthetic data generation (e.g., isl_synthetic_ocr)\n\n(e.g., isl_synthetic_ocr) VLM-generated transcriptions filtered manually or through heuristics\n\nfiltered manually or through heuristics Using existing OCR models to generate training data for new, potentially more efficient models in specific domains\n\nto generate training data for new, potentially more efficient models in specific domains Leveraging existing corrected datasets like the Medical History of British India Dataset, which contains extensively human-corrected OCR for historic documents\n\nIt's worth noting that many such datasets exist but remain unused. Making them more readily available as 'training-ready' datasets carries a considerable potential for the open-source community.\n\nTools to Run Models\n\nWe have received many questions about getting started with OCR models, so here are a few ways you can use local inference tools and host remotely with Hugging Face.\n\nLocally\n\nMost cutting-edge models come with vLLM support and transformers implementation. You can get more info about how to serve each from the models\u2019 own cards. For convenience, we show how to infer locally using vLLM here. The code below can differ from model to model, but for most models it looks like the following.\n\nvllm serve nanonets/Nanonets-OCR2-3B\n\nAnd then you can query as follows using e.g. OpenAI client.\n\nfrom openai import OpenAI import base64 client = OpenAI(base_url= \"http://localhost:8000/v1\" ) model = \"nanonets/Nanonets-OCR2-3B\" def encode_image ( image_path ): with open (image_path, \"rb\" ) as image_file: return base64.b64encode(image_file.read()).decode( \"utf-8\" ) def infer ( img_base64 ): response = client.chat.completions.create( model=model, messages=[ { \"role\" : \"user\" , \"content\" : [ { \"type\" : \"image_url\" , \"image_url\" : { \"url\" : f\"data:image/png;base64, {img_base64} \" }, }, { \"type\" : \"text\" , \"text\" : \"Extract the text from the above document as if you were reading it naturally.\" , }, ], } ], temperature= 0.0 , max_tokens= 15000 ) return response.choices[ 0 ].message.content img_base64 = encode_image(your_img_path) print (infer(img_base64))\n\nTransformers\n\nTransformers provides standard model definitions for easy inference and fine-tuning. Models available in transformers come with either official transformers implementation (model definitions within the library) or \u201cremote code\u201d implementations. Latter is defined by the model owners to enable easy loading of models into transformers interface, so you don\u2019t have to go through the model implementation. Below is an example loading Nanonets model using transformers implementation.\n\nfrom transformers import AutoProcessor, AutoModelForImageTextToText model = AutoModelForImageTextToText.from_pretrained( \"nanonets/Nanonets-OCR2-3B\" , torch_dtype= \"auto\" , device_map= \"auto\" , attn_implementation= \"flash_attention_2\" ) model. eval () processor = AutoProcessor.from_pretrained( \"nanonets/Nanonets-OCR2-3B\" ) def infer ( image_url, model, processor, max_new_tokens= 4096 ): prompt = \"\"\"Extract the text from the above document as if you were reading it naturally. Return the tables in html format. Return the equations in LaTeX representation. If there is an image in the document and image caption is not present, add a small description of the image inside the <img></img> tag; otherwise, add the image caption inside <img></img>. Watermarks should be wrapped in brackets. Ex: <watermark>OFFICIAL COPY</watermark>. Page numbers should be wrapped in brackets. Ex: <page_number>14</page_number> or <page_number>9/22</page_number>. Prefer using \u2610 and \u2611 for check boxes.\"\"\" image = Image. open (image_path) messages = [ { \"role\" : \"system\" , \"content\" : \"You are a helpful assistant.\" }, { \"role\" : \"user\" , \"content\" : [ { \"type\" : \"image\" , \"image\" : image_url}, { \"type\" : \"text\" , \"text\" : prompt}, ]}, ] text = processor.apply_chat_template(messages, tokenize= False , add_generation_prompt= True ) inputs = processor(text=[text], images=[image], padding= True , return_tensors= \"pt\" ).to(model.device) output_ids = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample= False ) generated_ids = [output_ids[ len (input_ids):] for input_ids, output_ids in zip (inputs.input_ids, output_ids)] output_text = processor.batch_decode(generated_ids, skip_special_tokens= True , clean_up_tokenization_spaces= True ) return output_text[ 0 ] result = infer(image_path, model, processor, max_new_tokens= 15000 ) print (result)\n\nMLX\n\nMLX is an open-source machine learning framework for Apple Silicon. MLX-VLM is built on top of MLX to serve vision language models easily. You can explore all the OCR models available in MLX format here. They also come in quantized versions.\n\nYou can install MLX-VLM as follows.\n\npip install -U mlx-vlm\n\nwget https://huggingface.co/datasets/merve/vlm_test_images/resolve/main/throughput_smolvlm.png python -m mlx_vlm.generate --model ibm-granite/granite-docling-258M-mlx --max-tokens 4096 --temperature 0.0 --prompt \"Convert this chart to JSON.\" --image throughput_smolvlm.png\n\nRemotely\n\nInference Endpoints for Managed Deployment\n\nYou can deploy OCR models compatible with vLLM or SGLang on Hugging Face Inference Endpoints, either from a model repository \u201cDeploy\u201d option or directly through Inference Endpoints interface. Inference Endpoints serve the cutting-edge models in a fully managed environment with GPU acceleration, auto-scaling, and monitoring without manually managing the infrastructure.\n\nHere is a simple method of deploying nanonets using vLLM as the inference engine.\n\nNavigate to the model repository nanonets/Nanonets-OCR2-3B Click on the \u201cDeploy\u201d button and select the \u201cHF Inference Endpoints\u201d\n\nConfigure the deployment setup within seconds\n\nAfter the endpoint is created, you can consume it using the OpenAI client snippet we provided in the previous section.\n\nYou can learn more about it here.\n\nHugging Face Jobs for Batch Inference\n\nFor many OCR applications, you want to do efficient batch inference, i.e., running a model across thousands of images as cheaply and efficiently as possible. A good approach is to use vLLM's offline inference mode. As discussed above, many recent VLM-based OCR models are supported by vLLM, which efficiently batches images and generates OCR outputs at scale.\n\nTo make this even easier, we've created uv-scripts/ocr, a collection of ready-to-run OCR scripts that work with Hugging Face Jobs. These scripts let you run OCR on any dataset without needing your own GPU. Simply point the script at your input dataset, and it will:\n\nProcess all images in a dataset column using many different open OCR models\n\nAdd OCR results as a new markdown column to the dataset\n\nPush the updated dataset with OCR results to the Hub\n\nFor example, to run OCR on 100 images:\n\nhf jobs uv run --flavor l4x1 \\ https://huggingface.co/datasets/uv-scripts/ocr/raw/main/nanonets-ocr.py \\ your-input-dataset your-output-dataset \\ --max-samples 100\n\nThe scripts handle all the vLLM configuration and batching automatically, making batch OCR accessible without infrastructure setup.\n\nGoing Beyond OCR\n\nIf you are interested in document AI, not just OCR, here are some of our recommendations.\n\nVisual Document Retrievers\n\nVisual document retrieval is to retrieve the most relevant top-k documents when given a text query. If you have previously worked with retriever models, the difference is that you search directly on a stack of PDFs. Aside from using them standalone, you can also build multimodal RAG pipelines by combining them with a vision language model (find how to do so here). You can find all of them on Hugging Face Hub.\n\nThere are two types of visual document retrievers, single-vector and multi-vector models. Single-vector models are more memory efficient and less performant; meanwhile, multi-vector models are more memory hungry and more performant. Most of these models often come with vLLM and transformers integrations, so you can index documents using them and then do a search easily using a vector DB.\n\nUsing Vision Language Models for Document Question Answering\n\nIf you have a task at hand that only requires answering questions based on documents, you can use some of the vision language models that had document tasks in their training tasks. We\u2019ve observed users trying to convert documents into text and passing the output to LLMs, but if your document has a complex layout, and your converted document outputs charts and so on in HTML, or images are captioned incorrectly, the LLM will miss out. Instead, feed your document and query to one of the advanced vision language models like Qwen3-VL not to miss out on any context.\n\nWrapping up\n\nIn this blog post, we wanted to give you an overview of how to pick your OCR model, existing cutting-edge models and capabilities, and the tools to get you started with OCR.\n\nIf you want to learn more about OCR and vision language models, we encourage you to read the resources below.",
    "link": "https://huggingface.co/blog/ocr-open-models",
    "Summary": "Supercharge your OCR Pipelines with Open ModelsPublished October 21, 2025 Update on GitHubTL;DR: The rise of powerful vision-language models has transformed document AI.\nMost OCR models are small, having between 3B and 7B parameters; you can even find models with fewer than 1B parameters, like PaddleOCR-VL.\nOpen OCR DatasetsWhile the past year has seen a surge in open OCR models, this hasn't been matched by as many open training and evaluation datasets.\nSharing more datasets could unlock even greater advances in open OCR models.\nAs discussed above, many recent VLM-based OCR models are supported by vLLM, which efficiently batches images and generates OCR outputs at scale.",
    "Keywords": [
      "markdown",
      "models",
      "images",
      "ocr",
      "pipelines",
      "html",
      "supercharge",
      "open",
      "document",
      "using",
      "tables",
      "text",
      "model"
    ]
  },
  {
    "Title": "Google Cloud C4 Brings a 70% TCO improvement on GPT OSS with Intel and Hugging Face",
    "Authors": [],
    "Publish Date": null,
    "Text": "Google Cloud C4 Brings a 70% TCO improvement on GPT OSS with Intel and Hugging Face\n\nPublished October 16, 2025 Update on GitHub\n\nC4\n\nIntel and Hugging Face collaborated to demonstrate the real-world value of upgrading to Google\u2019s latestVirtual Machine (VM) running on Intel\u00ae Xeon\u00ae 6 processors (codenamed Granite Rapids (GNR)). We specifically wanted to benchmark improvements in the text generation performance of OpenAI GPT OSS Large Language Model(LLM).\n\nThe results are in, and they are impressive, demonstrating a 1.7x improvement in Total Cost of Ownership(TCO) over the previous-generation Google C3 VM instances. The Google Cloud C4 VM instance further resulted in:\n\n1.4x to 1.7x TPOT throughput/vCPU/dollar\n\nLower price per hour over C3 VM\n\nIntroduction\n\nGPT OSS is a common name for an open-source Mixture of Experts (MoE) model released by OpenAI. An MoE model is a deep neural network architecture that uses specialized \u201cexpert\u201d sub-networks and a \u201cgating network\u201d to decide which experts to use for a given input. MoE models allow you to scale your model capacity efficiently without linearly scaling compute costs. They also allow for specialization, where different \u201cexperts\u201d learn different skills, allowing them to adapt to diverse data distributions.\n\nEven with very large parameters, only a small subset of experts is activated per token, making CPU inference viable.\n\nIntel and Hugging Face collaborated to merge an expert execution optimization (PR #40304) to eliminate redundant computation where every expert processed all tokens to transformers. This optimization directed each expert to run only on the tokens it is routed to, removing FLOPs waste and improving utilization.\n\nBenchmark Scope & Hardware\n\nWe benchmarked GPT OSS under a controlled, repeatable generation workload to isolate architectural differences (GCP C4 VMs on Intel Xeon 6 processors (GNR) vs GCP C3 VMs on 4th Gen Intel Xeon Processors (SPR)) and MoE execution efficiency. The focus is steady\u2011state decoding (per\u2011token latency) and end\u2011to\u2011end normalized throughput with increasing batch size while keeping sequence lengths fixed. All runs use static KV cache and SDPA attention for determinism.\n\nConfiguration Summary\n\nModel: unsloth/gpt-oss-120b-BF16\n\nPrecision: bfloat16\n\nTask: Text generation\n\nInput length: 1024 tokens (left\u2011padded)\n\nOutput length: 1024 tokens\n\nBatch sizes: 1, 2, 4, 8, 16, 32, 64\n\nEnabled features: Static KV cache SDPA attention backend\n\nReported metrics: Throughput (Total generated tokens per second aggregated over the batch)\n\n\n\nHardware Under Test\n\nInstance Architecture vCPUs C3 4th Gen Intel Xeon processor (SPR) 172 C4 Intel Xeon 6 processor (GNR) 144\n\nCreate instance\n\nC3\n\nVisit Google Cloud Console and click on create a VM under your project. Follow the steps below to create a 176 vCPU instance.\n\npick C3 in the Machine configuration and specify Machine type as c3-standard-176 . You also need to set the CPU platform and turn on all-core turbo to make performance more stable: configure OS and storage tab as below: keep other configurations as default click Create button\n\nC4\n\nVisit Google Cloud Console and click on create a VM under your project. Follow the below steps to create a 144 vCPU instance.\n\npick C4 in the Machine configuration tab and specify Machine type as c4-standard-144 . You can also set the CPU platform and turn on all-core turbo to make performance more stable: configure OS and storage tab as we need for C3. keep other configurations as default click Create button\n\nSet up the environment\n\nLogin the instance with SSH and then install docker. Follow the steps below to set up the environment easily. For reproducibility, we list the versions and commits we are using in the commands.\n\n$ git clone https://github.com/huggingface/transformers.git $ cd transformers/ $ git checkout 26b65fb5168f324277b85c558ef8209bfceae1fe $ cd docker/transformers-intel-cpu/ $ sudo docker build . -t <your_docker_image_tag> $ sudo docker run -it --rm --privileged -v /home/<your_home_folder>:/workspace <your_docker_image_tag> /bin/bash\n\nWe are in container now, do following steps.\n\n$ pip install git+https://github.com/huggingface/transformers.git@26b65fb5168f324277b85c558ef8209bfceae1fe $ pip install torch==2.8.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n\nBenchmark Procedure\n\nFor each batch size we\n\nBuild a fixed-length 1024\u2011token left\u2011padded batch. Run a single warm\u2011up round. set max_new_tokens=1024 and measure total latency, then get t h r o u g h p u t = ( O U T P U T _ T O K E N S \u2217 b a t c h _ s i z e ) / t o t a l _ l a t e n c y throughput = (OUTPUT\\_TOKENS * batch\\_size) / total\\_latency t h ro ug h p u t = ( O U TP U T _ TO K ENS \u2217 ba t c h _ s i ze ) / t o t a l _ l a t e n cy .\n\nRun numactl -l python benchmark.py for the following codes.\n\nimport os import time import torch from datasets import load_dataset from transformers import AutoModelForCausalLM, AutoTokenizer INPUT_TOKENS = 1024 OUTPUT_TOKENS = 1024 def get_inputs ( tokenizer, batch_size ): dataset = load_dataset( \"ola13/small-the_pile\" , split= \"train\" ) tokenizer.padding_side = \"left\" selected_texts = [] for sample in dataset: input_ids = tokenizer(sample[ \"text\" ], return_tensors= \"pt\" ).input_ids if len (selected_texts) == 0 and input_ids.shape[- 1 ] >= INPUT_TOKENS: selected_texts.append(sample[ \"text\" ]) elif len (selected_texts) > 0 : selected_texts.append(sample[ \"text\" ]) if len (selected_texts) == batch_size: break return tokenizer(selected_texts, max_length=INPUT_TOKENS, padding= \"max_length\" , truncation= True , return_tensors= \"pt\" ) def run_generate ( model, inputs, generation_config ): inputs[ \"generation_config\" ] = generation_config model.generate(**inputs) pre = time.time() model.generate(**inputs) latency = (time.time() - pre) return latency def benchmark ( model, tokenizer, batch_size, generation_config ): inputs = get_inputs(tokenizer, batch_size) generation_config.max_new_tokens = 1 generation_config.min_new_tokens = 1 prefill_latency = run_generate(model, inputs, generation_config) generation_config.max_new_tokens = OUTPUT_TOKENS generation_config.min_new_tokens = OUTPUT_TOKENS total_latency = run_generate(model, inputs, generation_config) decoding_latency = (total_latency - prefill_latency) / (OUTPUT_TOKENS - 1 ) throughput = OUTPUT_TOKENS * batch_size / total_latency return prefill_latency, decoding_latency, throughput if __name__ == \"__main__\" : model_id = \"unsloth/gpt-oss-120b-BF16\" tokenizer = AutoTokenizer.from_pretrained(model_id) model_kwargs = { \"dtype\" : torch.bfloat16} model = AutoModelForCausalLM.from_pretrained(model_id, **model_kwargs) model.config._attn_implementation= \"sdpa\" generation_config = model.generation_config generation_config.do_sample = False generation_config.cache_implementation= \"static\" for batch_size in [ 1 , 2 , 4 , 8 , 16 , 32 , 64 ]: print ( f\"---------- Run generation with batch size = {batch_size} ----------\" , flush= True ) prefill_latency, decoding_latency, throughput = benchmark(model, tokenizer, batch_size, generation_config) print ( f\"throughput = {throughput} \" , flush= True )\n\nResults\n\nNormalized Throughput per vCPU\n\nAcross batch sizes up to 64, Intel Xeon 6 processor\u2011powered C4 consistently outperforms C3 with a 1.4x to 1.7\u00d7 throughput per-vCPU. The formula is:\n\nnormalized_throughput_per_vCPU = throughput_C4 / vCPUs_C4 throughput_C3 / vCPUs_C3 \\text{normalized\\_throughput\\_per\\_vCPU} = \\frac{\\text{throughput\\_C4} / \\text{vCPUs\\_C4}} {\\text{throughput\\_C3} / \\text{vCPUs\\_C3}} normalized_throughput_per_vCPU=throughput_C3/vCPUs_C3throughput_C4/vCPUs_C4\u200b\n\nCost & TCO\n\nAt batch size 64, C4 provides 1.7\u00d7 the per\u2011vCPU throughput of C3 ; with near parity in price per vCPU (hourly cost scales linearly with vCPU count), this yields a 1.7\u00d7 TCO advantage ( C3 would require 1.7\u00d7 the spend for the same generated token volume).\n\nPer\u2011vCPU throughput ratio: throughput_C4 / vCPUs_C4 throughput_C3 / vCPUs_C3 = 1.7 \u21d2 TCO_C3 TCO_C4 \u2248 1.7 \\frac{\\text{throughput\\_C4} / \\text{vCPUs\\_C4}}{\\text{throughput\\_C3} / \\text{vCPUs\\_C3}} = 1.7 \\Rightarrow \\frac{\\text{TCO\\_C3}}{\\text{TCO\\_C4}} \\approx 1.7 throughput_C3/vCPUs_C3throughput_C4/vCPUs_C4\u200b=1.7\u21d2TCO_C4TCO_C3\u200b\u22481.7\n\nConclusion",
    "link": "https://huggingface.co/blog/gpt-oss-on-intel-xeon",
    "Summary": "Google Cloud C4 Brings a 70% TCO improvement on GPT OSS with Intel and Hugging FacePublished October 16, 2025 Update on GitHubC4Intel and Hugging Face collaborated to demonstrate the real-world value of upgrading to Google\u2019s latestVirtual Machine (VM) running on Intel\u00ae Xeon\u00ae 6 processors (codenamed Granite Rapids (GNR)).\nWe specifically wanted to benchmark improvements in the text generation performance of OpenAI GPT OSS Large Language Model(LLM).\nIntel and Hugging Face collaborated to merge an expert execution optimization (PR #40304) to eliminate redundant computation where every expert processed all tokens to transformers.\nBenchmark Scope & HardwareWe benchmarked GPT OSS under a controlled, repeatable generation workload to isolate architectural differences (GCP C4 VMs on Intel Xeon 6 processors (GNR) vs GCP C3 VMs on 4th Gen Intel Xeon Processors (SPR)) and MoE execution efficiency.\nPer\u2011vCPU throughput ratio: throughput_C4 / vCPUs_C4 throughput_C3 / vCPUs_C3 = 1.7 \u21d2 TCO_C3 TCO_C4 \u2248 1.7 \\frac{\\text{throughput\\_C4} / \\text{vCPUs\\_C4}}{\\text{throughput\\_C3} / \\text{vCPUs\\_C3}} = 1.7 \\Rightarrow \\frac{\\text{TCO\\_C3}}{\\text{TCO\\_C4}} \\approx 1.7 throughput_C3/vCPUs_C3throughput_C4/vCPUs_C4\u200b=1.7\u21d2TCO_C4TCO_C3\u200b\u22481.7Conclusion",
    "Keywords": [
      "oss",
      "throughput",
      "batch_size",
      "17",
      "output_tokens",
      "c4",
      "tco",
      "cloud",
      "face",
      "hugging",
      "xeon",
      "c3",
      "google",
      "gpt",
      "improvement",
      "generation_config",
      "model",
      "intel"
    ]
  },
  {
    "Title": "Unlock the power of images with AI Sheets",
    "Authors": [],
    "Publish Date": null,
    "Text": "Unlock the power of images with AI Sheets\n\nPublished October 21, 2025 Update on GitHub\n\nAnalyzing your images with AI Sheets\n\nWe are excited to release a massive update to Hugging Face AI Sheets, the open-source tool for building, transforming, and enriching data with open AI models. AI Sheets leverages Inference Providers, which means you can use thousands of open models powered by the best inference providers on the planet.\n\nThe first version of AI Sheets made structuring and enriching textual content a breeze. Now, we're adding vision to AI Sheets.\n\nImages are everywhere\u2014product photos, receipts, screenshots, diagrams, charts, logos. These documents contain structured information waiting to be extracted, analyzed, and transformed. Today, you can finally work with visual content directly in AI Sheets: view images, analyze them, extract information, generate new ones, and even edit them in real-time \u2014all in the same workflow.\n\nYour images have stories to tell\n\nImages contain valuable information\u2014product catalogs, support tickets, research archives, receipts, documents. Now you can upload images directly or use datasets with images, and use vision models to extract, analyze, and structure the information inside them.\n\nWhat you can do:\n\nDescribe and categorize images - Generate captions for product photos, classify document types, or tag images by content\n\n- Generate captions for product photos, classify document types, or tag images by content Extract structured data - Pull line items from receipts, data from charts, or text from scanned documents\n\n- Pull line items from receipts, data from charts, or text from scanned documents Add context and metadata - Automatically label images with relevant attributes, quality scores, or custom annotations\n\nJust like text columns, you can iterate on prompts, manually edit outputs, and use thumbs-up to teach the model what you want. Your feedback becomes few-shot examples for better results.\n\nExample: From receipts to structured expenses\n\nImagine you're back from a trip with a stack of receipts. Upload them to AI Sheets and create a column with a prompt like: Extract the merchant name, date, total amount, and expense category from this receipt\n\nAI Sheets processes each receipt and gives you a clean table with all the details extracted. You can edit any mistakes, validate good results with thumbs-up, and regenerate to improve the rest. Export the final dataset as CSV or Parquet for your expense tracking tool.\n\nOr maybe you're digitizing handwritten recipes from old family notebooks. Create columns to extract ingredients, cooking time, and cuisine type\u2014turning your personal archive into a searchable, structured dataset.\n\nGenerate and transform text and images in the same flow\n\nNeed visuals for your content? AI Sheets can generate and edit images directly in your spreadsheet using AI models, keeping your entire content creation workflow in one place.\n\nWhat you can do:\n\nGenerate images from text - Create social media graphics, thumbnails, or illustrations that match your content\n\nEdit and transform existing images - Modify uploaded images or generated visuals\u2014change styles, add elements, adjust compositions\n\nCreate variations at scale - Generate multiple versions or styles to test what resonates with your audience\n\nBuild visual content libraries - Produce consistent branded assets across large content campaigns\n\n\n\nExample: Creating a content calendar with visuals\n\nImagine you're planning a month of social media posts about healthy recipes. You have a spreadsheet with post titles and descriptions, but no images yet.\n\nCreate an image column with a prompt like: Generate an appetizing food photo for: {{title}}. Style: bright, overhead shot, natural lighting.\n\nAI Sheets generates a unique image for each post. Not quite right? Create another column to edit them: Transform the image to have a rustic wooden background and add fresh herbs as garnish.\n\nYou can iterate on generation and editing prompts and try different approaches. Your entire content calendar\u2014copy and visuals\u2014lives in one spreadsheet, ready to schedule or export.\n\nStep-by-step guide\n\nNow let\u2019s see AI Sheets in action. We will use open models to unlock the knowledge within handwritten recipes like the ones you could find from your grandma.\n\nUpload your data\n\nWe have a folder with photos that we can simply upload to the app.\n\nThe result is a spreadsheet like this:\n\nUnderstanding AI actions\n\nEach column in your spreadsheet can be transformed, extracted from, queried, and anything you can imagine using AI actions.\n\nTo see this in action, click on the overlay on top of any column:\n\nImage columns come with image operations like extracting text, asking the image, object detection, colorization, adding text, and any custom action you can think of.\n\nText columns include summarization, keyword extraction, translation, and custom actions.\n\nA prompt and a model define every AI action. Let\u2019s see what we can do with our handwritten recipes dataset!\n\nExtract text from images.\n\nAI Sheets comes with a template to extract text from images:\n\nThe result of this action is an AI-generated column with the transcribed text. Let\u2019s see an example:\n\nFor the above image, the extracted text is as follows:\n\nMEMORANDUM: From To 1 Box Duncan Hines Yellow Cake Mix 1 Box instant lemon pudding 2/3 cups water 1/2 cup Mozola oil 4 eggs Lemon flavoring to taste. Put in mixing bowl and beat for 10 min. and REMEMBER... for Quality PRINTING CALL OR WRITE Gatling & Pierce PRINTERS TELEPHONE 332-2579 22 YEARS OF SERVICE IN NORTHEASTERN CAROLINA\n\nNot bad! But we see it has included printed text for the header and footer, and we\u2019re interested in the recipe text. The reason this text is included is that we have used the default template for text extraction, which is as follows:\n\nExtract and transcribe all visible text from the image, including signs, labels, documents, or any written content\n\nLet\u2019s now try a custom prompt.\n\nHere is the extracted recipe details:\n\n- 1 box Duncan Hines Yellow Cake Mix\n\n- 1 box instant lemon pudding\n\n- 2/3 cups water\n\n- 1/2 cup Mazola oil\n\n- 4 eggs\n\n- Lemon flavoring to taste\n\n- Put in mixing bowl and beat for 10 minutes\n\nThis is great! But what about more complex images? By default, AI Sheets uses models with a good balance of speed and accuracy, but you can experiment with thousands of models. The above example uses the default vision language model Qwen/Qwen2.5-VL-7B-Instruct .\n\nLet\u2019s test a SoTA reasoning model, Qwen/Qwen3-VL-235B-A22B-Reasoning , with a more challenging image.\n\nHere\u2019s the comparison between the models:\n\nQwen/Qwen2.5-VL-7B-Instruct Qwen/Qwen3-VL-235B-A22B-Reasoning in large bowl combine meat, onion, bread crumbs 1/2 nutmeg & cheese - as you add sprinkle around. Then blend - Last sprinkle blend again Bake in large pan for 10-15 min. at 350. Let stand 5 min before serving. in lg bowl combine meat, onion, bread crumbs 1/4 nutmeg & cheese - as you add sprinkle around. then blend - last spinach blend again. Bake in lg pan for 50-60 min. @ 350 - let stand 5 min before serving\n\nBoth models produce very similar outputs, but with two subtle but important details (in bold): the temperature and a key ingredient: spinach.\n\nClean, transform, and enrich text\n\nOnce we are satisfied with the extracted text, we can further transform and enrich it. We need to perform an AI action with the new column as follows:\n\nWe now have a beautifully structured HTML page for each recipe:\n\nEdit and transform images.\n\nFinally, AI Sheets integrates image-to-image models like Qwen-Image-Edit. This means you can run AI actions to transform and enrich your images.\n\nFor example, let\u2019s say you want to give your recipes and old-looking style, you need to go to the column and use the B&W template like so:\n\nResult:\n\nExport your dataset\n\nOnce you're happy with your new dataset, export it to the Hub! You can export it to an organization, your personal profile or make it private if you don't want to share it with the community.\n\nYou can check out the dataset we have just created.\n\nYou can try AI Sheets without installing or downloading and deploying it locally from the GitHub repo. To run locally and get the most out of it, we recommend you subscribe to PRO and get 20x monthly inference usage.\n\nIf you have questions or suggestions, let us know in the Community tab or by opening an issue on GitHub.",
    "link": "https://huggingface.co/blog/aisheets-unlock-images",
    "Summary": "Unlock the power of images with AI SheetsPublished October 21, 2025 Update on GitHubAnalyzing your images with AI SheetsWe are excited to release a massive update to Hugging Face AI Sheets, the open-source tool for building, transforming, and enriching data with open AI models.\nNow, we're adding vision to AI Sheets.\nAI Sheets can generate and edit images directly in your spreadsheet using AI models, keeping your entire content creation workflow in one place.\nBy default, AI Sheets uses models with a good balance of speed and accuracy, but you can experiment with thousands of models.\nYou can try AI Sheets without installing or downloading and deploying it locally from the GitHub repo.",
    "Keywords": [
      "images",
      "models",
      "image",
      "column",
      "receipts",
      "content",
      "ai",
      "text",
      "sheets",
      "transform",
      "unlock",
      "power"
    ]
  },
  {
    "Title": "Get your VLM running in 3 simple steps on Intel CPUs",
    "Authors": [],
    "Publish Date": null,
    "Text": "Get your VLM running in 3 simple steps on Intel CPUs\n\nPublished October 15, 2025 Update on GitHub\n\nWith the growing capability of large language models (LLMs), a new class of models has emerged: Vision Language Models (VLMs) . These models can analyze images and videos to describe scenes, create captions, and answer questions about visual content.\n\nWhile running AI models on your own device can be difficult as these models are often computationally demanding, it also offers significant benefits: including improved privacy since your data stays on your machine, and enhanced speed and reliability because you're not dependent on an internet connection or external servers. This is where tools like Optimum Intel and OpenVINO come in, along with a small, efficient model like SmolVLM. In this blog post, we'll walk you through three easy steps to get a VLM running locally, with no expensive hardware or GPUs required (though you can run all the code samples from this blog post on Intel GPUs).\n\nDeploy your model with Optimum\n\nSmall models like SmolVLM are built for low-resource consumption, but they can be further optimized. In this blog post we will see how to optimize your model, to lower memory usage and speedup inference, making it more efficient for deployment on devices with limited resources.\n\nTo follow this tutorial, you need to install optimum and openvino , which you can do with:\n\npip install optimum-intel[openvino] transformers==4.52.*\n\nStep 1: Convert your model\n\nFirst, you will need to convert your model to the OpenVINO IR. There are multiple options to do it:\n\nYou can use the Optimum CLI\n\noptimum-cli export openvino -m HuggingFaceTB/SmolVLM2-256M-Video-Instruct smolvlm_ov/\n\nOr you can convert it on the fly when loading your model:\n\nfrom optimum.intel import OVModelForVisualCausalLM model_id = \"HuggingFaceTB/SmolVLM2-256M-Video-Instruct\" model = OVModelForVisualCausalLM.from_pretrained(model_id) model.save_pretrained( \"smolvlm_ov\" )\n\nStep 2: Quantization\n\nNow it\u2019s time to optimize your model. Quantization reduces the precision of the model weights and/or activations, leading to smaller, faster models. Essentially, it's a way to map values from a high-precision data type, such as 32-bit floating-point numbers (FP32), to a lower-precision format, typically 8-bit integers (INT8). While this process offers several key benefits, it can also impact in a potential loss of accuracy.\n\nOptimum supports two main post-training quantization methods:\n\nLet\u2019s explore each of them.\n\nOption 1: Weight Only Quantization\n\nWeight-only quantization means that only the weights are quantized but activations remain in their original precisions. As a result, the model becomes smaller and more memory-efficient, improving loading times. But since activations are not quantized, inference speed gains are limited. Weight-only quantization is a simple first step since it usually doesn\u2019t result in significant accuracy degradation.\n\nSince OpenVINO 2024.3, if the model's weight have been quantized, the corresponding activations will also be quantized at runtime, leading to additional speedup depending on the device.\n\nIn order to run it, you will need to create a quantization configuration OVWeightQuantizationConfig as follows:\n\nfrom optimum.intel import OVModelForVisualCausalLM, OVWeightQuantizationConfig q_config = OVWeightQuantizationConfig(bits= 8 ) q_model = OVModelForVisualCausalLM.from_pretrained(model_id, quantization_config=q_config) q_model.save_pretrained( \"smolvlm_int8\" )\n\nor equivalently using the CLI:\n\noptimum-cli export openvino -m HuggingFaceTB/SmolVLM2-256M-Video-Instruct --weight-format int8 smolvlm_int8/\n\nOption 2: Static Quantization\n\nWith Static Quantization, both weights and activations are quantized before inference. To achieve the best estimate for the activation quantization parameters, we perform a calibration step. During this step, a small representative dataset is fed through the model. In our case, we will use 50 samples of the contextual dataset and will apply static quantization on the vision encoder while weight-only quantization will be applied on the rest of the model. Experiments show that applying static quantization on the vision encoder provides a noticeable performance improvement without significant accuracy degradation. Since the vision encoder is called only once per generation, the overall performance gain from applying static quantization on this component is lower than the gain achieved by optimizing more frequently used components like the language model. Nevertheless, this approach can be beneficial in certain scenarios. For example, when short answers are needed, especially with multiple images as input.\n\nfrom optimum.intel import OVModelForVisualCausalLM, OVPipelineQuantizationConfig, OVQuantizationConfig, OVWeightQuantizationConfig q_config = OVPipelineQuantizationConfig( quantization_configs={ \"lm_model\" : OVWeightQuantizationConfig(bits= 8 ), \"text_embeddings_model\" : OVWeightQuantizationConfig(bits= 8 ), \"vision_embeddings_model\" : OVQuantizationConfig(bits= 8 ), }, dataset=dataset, num_samples=num_samples, ) q_model = OVModelForVisualCausalLM.from_pretrained(model_id, quantization_config=q_config) q_model.save_pretrained( \"smolvlm_static_int8\" )\n\nQuantizing activations adds small errors that can build up and affect accuracy, so careful testing afterward is important. More information and examples can be found in our documentation.\n\nStep 3: Run inference\n\nYou can now run inference with your quantized model:\n\ngenerated_ids = q_model.generate(**inputs, max_new_tokens= 100 ) generated_texts = processor.batch_decode(generated_ids, skip_special_tokens= True ) print (generated_texts[ 0 ])\n\nIf you have a recent Intel laptop, Intel AI PC, or Intel discrete GPU, you can load the model on GPU by adding device=\"gpu\" when loading your model:\n\nmodel = OVModelForVisualCausalLM.from_pretrained(model_id, device= \"gpu\" )\n\nWe also created a space so you can play with the original model and its quantized variants obtained by respectively applying weight-only quantization and mixed quantization. This demo runs on 4th Generation Intel Xeon (Sapphire Rapids) processors.\n\nTo reproduce our results, check out our notebook.\n\nEvaluation and Conclusion\n\nWe ran a benchmark to compare the performance of the PyTorch, OpenVINO, and OpenVINO 8-bit WOQ versions of the original model. The goal was to evaluate the impact of weight-only quantization on latency and throughput on Intel CPU hardware. For this test, we used a single image as input.\n\nWe measured the following metrics to evaluate the model's performance:\n\nTime To First Token (TTFT) : Time it takes to generate the first output token.\n\nTime Per Output Token (TPOT): Time it takes to generate each subsequent output tokens.\n\nEnd-to-End Latency : Total time it takes to generate the output all output tokens.\n\nDecoding Throughput: Number of tokens per second the model generates during the decoding phase.\n\nHere are the results on Intel CPU:\n\nConfiguration Time To First Token (TTFT) Time Per Output Token (TPOT) End-to-End Latency Decoding Throughput pytorch 5.150 1.385 25.927 0.722 openvino 0.420 0.021 0.738 47.237 openvino-8bit-woq 0.247 0.016 0.482 63.928\n\nThis benchmark demonstrates how small, optimized multimodal models, like SmolVLM2-256M, perform on Intel CPUs across different configurations. According to the tests, the PyTorch version shows high latency, with a time to first token (TTFT) of over 5s with a decoding throughput of 0.7 tokens/s. Simply converting the model with Optimum and running it on OpenVINO drastically reduces the time to first token (TTFT) to 0.42s (x12 speedup) and raises throughput to 47 tokens/s (x65). Applying 8-bit weight-only quantization further reduces TTFT (x1.7) and increases throughput (x1.4), while also reducing model size and improving efficiency.\n\nPlatform configuration Platform Configuration for performance claims above: System Board: MSI B860M GAMING PLUS WIFI (MS-7E42)\n\nCPU: Intel\u00ae Core\u2122 Ultra 7 265K\n\nSockets/Physical Cores: 1/20 (20 threads)\n\nHyperThreading/Turbo Settings: Disabled\n\nMemory: 64 GB DDR5 @ 6400 MHz\n\nTDP: 665W\n\nBIOS: American Megatrends International, LLC. 2.A10\n\nBIOS Release Date: 28.11.2024\n\nOS: Ubuntu 24.10\n\nKernel: 6.11.0\u201325-generic\n\nOpenVINO Version: 2025.2.0\n\ntorch: 2.8.0\n\ntorchvision: 0.23.0+cpu\n\noptimum-intel: 1.25.2\n\ntransformers: 4.53.3\n\nBenchmark Date: 15.05.2025\n\nBenchmarked by: Intel Corporation Performance may vary by use, configuration, and other factors. See the platform configuration below.\n\nUseful Links & Resources",
    "link": "https://huggingface.co/blog/openvino-vlm",
    "Summary": "Get your VLM running in 3 simple steps on Intel CPUsPublished October 15, 2025 Update on GitHubWith the growing capability of large language models (LLMs), a new class of models has emerged: Vision Language Models (VLMs) .\nThis is where tools like Optimum Intel and OpenVINO come in, along with a small, efficient model like SmolVLM.\nThe goal was to evaluate the impact of weight-only quantization on latency and throughput on Intel CPU hardware.\nWe measured the following metrics to evaluate the model's performance:Time To First Token (TTFT) : Time it takes to generate the first output token.\nTime Per Output Token (TPOT): Time it takes to generate each subsequent output tokens.",
    "Keywords": [
      "throughput",
      "models",
      "quantized",
      "quantization",
      "cpus",
      "activations",
      "running",
      "vlm",
      "steps",
      "openvino",
      "simple",
      "output",
      "token",
      "model",
      "intel"
    ]
  },
  {
    "Title": "SOTA OCR with Core ML and dots.ocr",
    "Authors": [],
    "Publish Date": null,
    "Text": "SOTA OCR with Core ML and dots.ocr\n\nPublished October 2, 2025 Update on GitHub\n\nEvery year our hardware is a little more powerful, our models a little smarter for each parameter. In 2025, it is more feasible than ever to run truly competitive models on-device. dots.ocr , a 3B parameter OCR model from RedNote, surpasses Gemini 2.5 Pro in OmniDocBench , making OCR a truly no compromises on-device use case. Running models on-device is certainly appealing to developers: no smuggling API keys, zero cost, and no network required. However, if we want these models to run on-device, we need to be mindful of the limited compute and power budgets.\n\nEnter the Neural Engine, Apple's custom AI accelerator that has shipped with every Apple device since 2017. This accelerator is designed for high performance whilst sipping battery power. Some of our testing has found the Neural Engine to be 12x more power efficient than CPU, and 4x more power efficient than GPU.\n\nWhilst this all sounds very appealing, unfortunately the Neural Engine is only accessible through Core ML, Apple's closed source ML framework. Furthermore, even just converting a model from PyTorch to Core ML can present some challenges, and without a preconverted model or some knowledge of the sharp edges it can be arduous for developers. Luckily, Apple also offers MLX, a more modern and flexible ML framework that targets the GPU (not the Neural Engine), and can be used in conjunction with Core ML.\n\nIn this three part series, we will provide a reasoning trace of how we converted dots.ocr to run on-device, using a combination of CoreML and MLX . This process should be applicable to many other models, and we hope that this will help highlight the ideas and tools needed for developers looking to run their own models on-device.\n\nTo follow along, clone the repo. You'll need uv and hf installed to run the setup command:\n\n./boostrap.sh\n\nIf you just want to skip ahead and use the converted model, you can download it here.\n\nConversion\n\nConverting from PyTorch to CoreML is a two step process:\n\nCapturing your PyTorch execution graph (via torch.jit.trace or, the more modern approach of torch.export ). Compiling this converted graph to an .mlpackage using coremltools .\n\nWhilst we do have a few knobs we can tweak for step 2, most of our control is in step 1, the graph we feed to coremltools .\n\nFollowing the programmers litany of make it work, make it right, make it fast , we will first focus on getting the conversion working on GPU, in FLOAT32, and with static shapes. Once we have this working, we can dial down the precision and try and move to the Neural Engine.\n\nDots.OCR consists of two key components: A 1.2B parameter vision encoder trained from scratch, based on the NaViT architecture, and a Qwen2.5-1.5B backbone. We will be using CoreML to run the vision encoder, and MLX to run the LM backbone.\n\nStep 0: Understand and simplify the model\n\nIn order to convert a model, it's best to understand the structure and function before getting started. Looking at the original vision modelling file here, we can see that the vision encoder is similar to the QwenVL family. Like many vision encoders, the vision encoder for dots works on a patch basis, in this case 14x14 patches. The dots vision encoder is capable of processing videos and batches of images. This gives us an opportunity to simplify by only processing a single image at a time. This approach is frequent in on-device apps, where we convert a model that provides the essential functions and iterate if we want to process multiple images.\n\nWhen kicking off the conversion process, it's best to start with a minimal viable model. This means removing any bells and whistles that are not strictly necessary for the model to function. In our case, dots has many different attention implementations available for both the vision encoder and the LM backbone. CoreML has lots of infrastructure oriented around the scaled_dot_product_attention operator, which they introduced in iOS 18. We can simplify the model by removing all of the other attention implementations and just focusing on simple sdpa (not the memory efficient variant) for now, commit here.\n\nOnce we've done this, we see a scary warning message when we load the model:\n\nSliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n\nThe model doesn't require Sliding Window Attention to function, so we can happily move on.\n\nStep 1: A simple harness\n\nUsing torch.jit.trace is still the most mature method for converting models to CoreML. We usually encapsulate this in a simple harness that allows you to modify the compute units used and the precision selected.\n\nYou can check out the initial harness here. If we run the following on the original code implementation:\n\nuv run convert.py --precision FLOAT32 --compute_units CPU_AND_GPU\n\nWe should bump into the first (of many) issues.\n\nStep 2: Bug hunting\n\nIt is rare that a model will convert first time. Often, you will need to progressively make changes further and further down the execution graph until you reach the final node.\n\nOur first issue is the following error:\n\nERROR - converting 'outer' op (located at: 'vision_tower/rotary_pos_emb/192' ): In op \"matmul\" , when x and y are both non-const, their dtype need to match, but got x as int32 and y as fp32\n\nLuckily this error gives us quite a bit of information. We can look at the VisionRotaryEmbedding layer and see the following code:\n\ndef forward ( self, seqlen: int ) -> torch.Tensor: seq = torch.arange(seqlen, device=self.inv_freq.device, dtype=self.inv_freq.dtype) freqs = torch.outer(seq, self.inv_freq) return freqs\n\nAlthough torch.arange has a dtype argument, coremltools ignores this for arange and always outputs int32 . We can simply add a cast after the arange to fix this issue, commit here.\n\nAfter fixing this, running the conversion again leads us to our next issue at repeat_interleave :\n\nERROR - converting 'repeat_interleave' op (located at: 'vision_tower/204' ): Cannot add const [None]\n\nWhilst this error is less informative, we only have a single call to repeat_interleave in our vision encoder:\n\ncu_seqlens = torch.repeat_interleave(grid_thw[:, 1 ] * grid_thw[:, 2 ], grid_thw[:, 0 ]).cumsum( dim= 0 , dtype=grid_thw.dtype if torch.jit.is_tracing() else torch.int32, )\n\ncu_seqlens is used for masking variable length sequences in flash_attention_2 . It's derived from the grid_thw tensor, which represents time , height and width . Since we are only processing a single image, we can simply remove this call, commit here.\n\nOnto the next! This time, we get a more cryptic error:\n\nERROR - converting '_internal_op_tensor_inplace_fill_' op (located at: 'vision_tower/0/attn/301_internal_tensor_assign_1' ): _internal_op_tensor_inplace_fill does not support dynamic index\n\nThis is again due to the masking logic to handle variable length sequences. Since we are only processing a single image (not a video or batch of images), we don't really need attention masking at all! Therefore, we can just use a mask of all True . To prepare ourselves for the Neural Engine conversion, we also switch from using a boolean mask to a float mask of all zeros, as the Neural Engine does not support bool tensors commit here\n\nWith all of this done, the model should now successfully convert to CoreML! However, when we run the model, we get the following error:\n\nerror: 'mps.reshape' op the result shape is not compatible with the input shape\n\nThis reshape could be in multiple places! Luckily, we can use a previous warning message to help us track down the issue:\n\nTracerWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won 't change the number of iterations executed (and might lead to errors or silently give incorrect results). for t, h, w in grid_thw:\n\nMost ML compilers do not like dynamic control flow. Luckily for us, as we are only processing a single image, we can simply remove the loop and process the single h, w pair, commit here.\n\nAnd there we have it! If we run the conversion again, we should see that the model successfully converts and matches the original PyTorch precision:\n\nMax difference: 0.006000518798828125, Mean difference: 1.100682402466191e-05\n\nStep 3: Benchmarking\n\nNow that we've got the model working, let's evaluate the size and performance. The good news is the model is working, the bad news is that it's over 5GB! This is completely untenable for on device deployment! To benchmark the computation time, we can use the built in XCode tooling by calling:\n\nopen DotsOCR_FLOAT32.mlpackage\n\nwhich will launch the XCode inspector for the model. After clicking + Performance Report and launching a report on all compute devices, you should see something like the following:\n\nOver a second for a single forward pass of the vision encoder! We have lots of more work.",
    "link": "https://huggingface.co/blog/dots-ocr-ne",
    "Summary": "Whilst this all sounds very appealing, unfortunately the Neural Engine is only accessible through Core ML, Apple's closed source ML framework.\nLuckily, Apple also offers MLX, a more modern and flexible ML framework that targets the GPU (not the Neural Engine), and can be used in conjunction with Core ML.\nIn this three part series, we will provide a reasoning trace of how we converted dots.ocr to run on-device, using a combination of CoreML and MLX .\nDots.OCR consists of two key components: A 1.2B parameter vision encoder trained from scratch, based on the NaViT architecture, and a Qwen2.5-1.5B backbone.\nLike many vision encoders, the vision encoder for dots works on a patch basis, in this case 14x14 patches.",
    "Keywords": [
      "models",
      "ocr",
      "core",
      "engine",
      "run",
      "vision",
      "single",
      "sota",
      "encoder",
      "dotsocr",
      "neural",
      "ml",
      "ondevice",
      "model"
    ]
  },
  {
    "Title": "Introducing RTEB: A New Standard for Retrieval Evaluation",
    "Authors": [],
    "Publish Date": null,
    "Text": "Introducing RTEB: A New Standard for Retrieval Evaluation\n\nPublished October 1, 2025 Update on GitHub\n\nWe\u2019re excited to introduce the beta version of the Retrieval Embedding Benchmark (RTEB) , a new benchmark designed to reliably evaluate the retrieval accuracy of embedding models for real-world applications. Existing benchmarks struggle to measure true generalization, while RTEB addresses this with a hybrid strategy of open and private datasets. Its goal is simple: to create a fair, transparent, and application-focused standard for measuring how models perform on data they haven\u2019t seen before.\n\nThe performance of many AI applications, from RAG and agents to recommendation systems, is fundamentally limited by the quality of search and retrieval. As such, accurately measuring the retrieval quality of embedding models is a common pain point for developers. How do you really know how well a model will perform in the wild?\n\nThis is where things get tricky. The current standard for evaluation often relies on a model's \"zero-shot\" performance on public benchmarks. However, this is, at best, an approximation of a model's true generalization capabilities. When models are repeatedly evaluated against the same public datasets, a gap emerges between their reported scores and their actual performance on new, unseen data.\n\nPerformance Discrepancy Between Public and Closed Datasets\n\nTo address these challenges, we developed RTEB, a benchmark built to provide a reliable standard for evaluating retrieval models.\n\nWhy Existing Benchmarks Fall Short\n\nWhile the underlying evaluation methodology and metrics (such as NDCG@10) are well-known and robust, the integrity of existing benchmarks is often set back by the following issues:\n\nThe Generalization Gap. The current benchmark ecosystem inadvertently encourages \"teaching to the test.\" When training data sources overlap with evaluation datasets, a model's score can become inflated, undermining a benchmark's integrity. This practice, whether intentional or not, is evident in the training datasets of several models. This creates a feedback loop where models are rewarded for memorizing test data rather than developing robust, generalizable capabilities.\n\nBecause of the above, models with a lower zero-shot score[1] may perform very well on the benchmark, without generalizing to new problems. For this reason, models with slightly lower benchmark performance and a higher zero-shot score are often recommended instead.\n\nMisalignment with Today\u2019s AI Applications. Many benchmarks are poorly aligned with the enterprise use cases that developers are building today. They often rely on academic datasets or on retrieval tasks derived from QA datasets, which, while useful in their own right, were not designed to evaluate retrieval and can fail to capture the distributional biases and complexities encountered in real-world retrieval scenarios. Benchmarks which do not possess these issues are often too narrow, focusing on a single domain like code retrieval, making them unsuitable for evaluating general-purpose models.\n\nIntroducing RTEB\n\nToday, we\u2019re excited to introduce the Retrieval Embedding Benchmark (RTEB). Its goal is to create a new, reliable, high-quality benchmark that measures the true retrieval accuracy of embedding models.\n\nA Hybrid Strategy for True Generalization\n\nTo combat benchmark overfitting, RTEB implements a hybrid strategy using both open and private datasets:\n\nOpen Datasets: The corpus, queries, and relevance labels are fully public. This ensures transparency and allows any user to reproduce the results.\n\nThe corpus, queries, and relevance labels are fully public. This ensures transparency and allows any user to reproduce the results. Private Datasets: These datasets are kept private, and evaluation is handled by the MTEB maintainers to ensure impartiality. This setup provides a clear, unbiased measure of a model\u2019s ability to generalize to unseen data. For transparency, we provide descriptive statistics, a dataset description, and sample (query, document, relevance) triplets for each private dataset.\n\nThis hybrid approach encourages the development of models with broad, robust generalization. A model with a significant performance drop between the open and the private datasets would suggest overfitting, providing a clear signal to the community. This is already apparent with some models, which show a notable drop in performance on RTEB's private datasets.\n\nBuilt for Real-World Domains\n\nRTEB is designed with a particular emphasis on enterprise use cases. Instead of a complex hierarchy, it uses simple groups for clarity. A single dataset can belong to multiple groups (e.g., a German law dataset exists in both the \"law\" and \"German\" groups).\n\nMultilingual in Nature: The benchmark datasets cover 20 languages, from common ones like English or Japanese to rarer languages such as Bengali or Finnish.\n\nThe benchmark datasets cover 20 languages, from common ones like English or Japanese to rarer languages such as Bengali or Finnish. Domain-Specific Focus: The benchmark includes datasets from critical enterprise domains like law, healthcare, code, and finance.\n\nThe benchmark includes datasets from critical enterprise domains like law, healthcare, code, and finance. Efficient Dataset Sizes: Datasets are large enough to be meaningful (at least 1k documents and 50 queries) without being so large that they make evaluation time-consuming and expensive.\n\nDatasets are large enough to be meaningful (at least 1k documents and 50 queries) without being so large that they make evaluation time-consuming and expensive. Retrieval-First Metric: The default leaderboard metric is NDCG@10, a gold-standard measure for the quality of ranked search results.\n\nA complete list of the datasets can be found below. We plan to continually update both the open as well as closed portion with different categories of datasets and actively encourage participation from the community; please open an issue on the MTEB repository on GitHub if you would like to suggest other datasets.\n\nRTEB Datasets Open Dataset Dataset Groups Open/Closed Dataset URL Repurposed from QA Description and Reason for Inclusion AILACasedocs english, legal Open https://huggingface.co/datasets/mteb/AILA_casedocs No This dataset comprises approximately 3,000 Supreme Court of India case documents and is designed to evaluae the retrieval of relevant prior cases for given legal situations. It includes 50 queries, each outlining a specific scenario. We include this dataset in the benchmark because the documents are reasonably challenging, the queries are non-synthetic, and the labels are of high quality. AILAStatutes english, legal Open https://huggingface.co/datasets/mteb/AILA_statutes No The dataset comprises descriptions of 197 Supreme Court of India statutes, designed to facilitate the retrieval of relevant prior statutes for given legal situations. It includes 50 queries, each outlining a specific scenario. We include this dataset in the benchmark because the documents are reasonably challenging, the queries are non-synthetic, and the labels are of high quality. LegalSummarization english, legal Open https://huggingface.co/datasets/mteb/legal_summarization No The dataset comprises 446 pairs of legal text excerpts and their corresponding plain English summaries, sourced from reputable websites dedicated to clarifying legal documents. The summaries have been manually reviewed for quality, ensuring that the data is clean and suitable for evaluating legal retrieval. LegalQuAD german, legal Open https://huggingface.co/datasets/mteb/LegalQuAD No The corpus consists of 200 real-world legal documents and the query set consists of 200 questions pertaining to legal documents. FinanceBench english, finance Open https://huggingface.co/datasets/virattt/financebench Yes The FinanceBench dataset is derived from the PatronusAI/financebench-test dataset, containing only the PASS examples processed into a clean format for question-answering tasks in the financial domain. FinanceBench-rtl has been repurposed for retrieval. HC3Finance english, finance Open https://huggingface.co/datasets/Hello-SimpleAI/HC3 No The HC3 dataset comprises tens of thousands of comparison responses from both human experts and ChatGPT across various domains, including open-domain, financial, medical, legal, and psychological areas. The data collection process involved sourcing publicly available question-answering datasets and wiki texts, ensuring that the human answers were either expert-provided or high-quality user responses, thereby minimizing mislabeling and enhancing the dataset's reliability. FinQA english, finance Open https://huggingface.co/datasets/ibm/finqa Yes FinQA is a large-scale dataset with 2.8k financial reports for 8k Q&A pairs to study numerical reasoning with structured and unstructured evidence. HumanEval code Open https://huggingface.co/datasets/openai/openai_humaneval Yes The HumanEval dataset released by OpenAI includes 164 programming problems with a handwritten function signature, docstring, body, and several unit tests for each problem. The dataset was handcrafted by engineers and researchers at OpenAI. MBPP code Open https://huggingface.co/datasets/google-research-datasets/mbpp Yes The MBPP dataset consists of around 1,000 crowd-sourced Python programming problems, designed to be solvable by entry level programmers, covering programming fundamentals, standard library functionality, and so on. Each problem consists of a task description, code solution and 3 automated test cases. As described in the paper, a subset of the data has been hand-verified by the dataset authors to ensure quality. MIRACLHardNegatives Open https://huggingface.co/datasets/mteb/miracl-hard-negatives No MIRACL (Multilingual Information Retrieval Across a Continuum of Languages) is a multilingual retrieval dataset that focuses on search across 18 different languages. The hard negative version has been created by pooling the 250 top documents per query from BM25, e5-multilingual-large and e5-mistral-instruct. APPS code, english Open https://huggingface.co/datasets/codeparrot/apps Yes APPS is a benchmark for code generation with 10000 problems. It can be used to evaluate the ability of language models to generate code from natural language specifications. To create the APPS dataset, the authors manually curated problems from open-access sites where programmers share problems with each other, including Codewars, AtCoder, Kattis, and Codeforces. DS1000 code, english Open https://huggingface.co/datasets/xlangai/DS-1000 Yes DS-1000 is a code generation benchmark with a thousand data science problems spanning seven Python libraries, such as NumPy and Pandas. It employs multi-criteria evaluation metrics, including functional correctness and surface-form constraints, resulting in a high-quality dataset with only 1.8% incorrect solutions among accepted Codex-002 predictions. WikiSQL code, english Open https://huggingface.co/datasets/Salesforce/wikisql Yes WikiSQL is a dataset comprising 80,654 hand-annotated examples of natural language questions and corresponding SQL queries across 24,241 tables from Wikipedia. ChatDoctor_HealthCareMagic english, healthcare Open https://huggingface.co/datasets/lavita/ChatDoctor-HealthCareMagic-100k No The ChatDoctor-HealthCareMagic-100k dataset comprises 112,000 real-world medical question-and-answer pairs, providing a substantial and diverse collection of authentic medical dialogues. There is a slight risk to this dataset since there are grammatical inconsistencies in many of the questions and answers, but this can potentially help separate strong healthcare retrieval models from weak ones. HC3 Medicine english, healthcare Open https://huggingface.co/datasets/Hello-SimpleAI/HC3 No The HC3 dataset comprises tens of thousands of comparison responses from both human experts and ChatGPT across various domains, including open-domain, financial, medical, legal, and psychological areas. The data collection process involved sourcing publicly available question-answering datasets and wiki texts, ensuring that the human answers were either expert-provided or high-quality user responses, thereby minimizing mislabeling and enhancing the dataset's reliability. HC3 French OOD french, healthcare Open https://huggingface.co/datasets/almanach/hc3_french_ood No The HC3 dataset comprises tens of thousands of comparison responses from both human experts and ChatGPT across various domains, including open-domain, financial, medical, legal, and psychological areas. The data collection process involved sourcing publicly available question-answering datasets and wiki texts, ensuring that the human answers were either expert-provided or high-quality user responses, thereby minimizing mislabeling and enhancing the dataset's reliability. JaQuAD japanese Open https://huggingface.co/datasets/SkelterLabsInc/JaQuAD Yes The JaQuAD dataset comprises 39,696 human-annotated question-answer pairs based on Japanese Wikipedia articles, with 88.7% of the contexts sourced from curated high-quality articles. Cure english, healthcare Open https://huggingface.co/datasets/clinia/CUREv1 No TripClick english, healthcare Open https://huggingface.co/datasets/irds/tripclick No FreshStack english Open https://huggingface.co/papers/2504.13128 No Closed Dataset Dataset Groups Open/Closed Dataset URL Comments Repurposed from QA Description and Reason for Inclusion _GermanLegal1 german, legal Closed Yes This dataset is derived from real-world judicial decisions and employs a combination of legal citation matching and BM25 similarity. The BM25 baseline poses a slight risk as it biases the data outside of citation matching. A subset of the dataset was manually verified to ensure correctness and quality. _JapaneseLegal1 japanese, legal Closed No This dataset comprises 8.75K deduplicated law records retrieved from the official Japanese government website e-Gov, ensuring authoritative and accurate content. Record titles are used as queries, while record bodies are used as documents. _FrenchLegal1 french, legal Closed No This dataset comprises case laws from the French court \"Conseil d'Etat,\" systematically extracted from the OPENDATA/JADE repository, focusing on tax-related cases. Queries are the title of each document, ensuring that the labels are clean. _EnglishFinance1 english, finance Closed Yes This retrieval dataset has been repurposed for retrieval from TAT-QA, a large-scale QA dataset using tabular and textual content. _EnglishFinance4 english, finance Closed No This dataset is a combination of Stanford's Alpaca and FiQA with another 1.3k pairs custom generated using GPT3.5, and then further cleaned to ensure that the data quality is high. _EnglishFinance2 english, finance Closed Yes This dataset is a finance-domain dataset that is composed of questions for each conversation turn based on simulated conversation flow. The curation is done by expert annotators, ensuring a reasonably high data quality. The questions are repurposed as queries, while the conversation block is repurposed as documents for retrieval. _EnglishFinance3 english, finance Closed Yes This dataset is a collection of question-answer pairs curated to address various aspects of personal finance. _Code1 code Closed No We extracted functions from GIthub repos. With syntactic parsing, doc strings and function signature are obtained from the functions. Only functions with docstrings are kept. Doc strings are used as queries, with function signature (which includes function name and argument names) removed to making the task harder. Each language is a subset with separate corpus. _JapaneseCode1 code, japanese Closed No This is a subset of the CoNaLa challenge with Japanese questions. _EnglishHealthcare1 english, healthcare Closed Yes This dataset comprises 2,019 question-answer pairs annotated by 15 experts, each holding at least a Master's degree in biomedical sciences. A medical doctor led the annotation team, verifying each question-answer pair to ensure data quality. _GermanHealthcare1 german, healthcare Closed No This dataset comprises of 465 German-language medical dialogues between patients and healthcare assistants, each entry containing detailed patient descriptions and corresponding professional responses. We have manually verified a subset of the dataset for accuracy and data quality. _German1 german Closed No This dataset is a dialogue summarization dataset derived from multiple public corpora, which have cleaned and preprocessed into a unified format. Each dialogue has been manually summarized and labeled with topics by annotators, ensuring high-quality and clean data. Dialog summaries are used as queries, while full dialogues are used as documents. _French1 french Closed Yes This dataset comprises over 4118 French trivia question-answer pairs, each accompanied by relevant Wikipedia context. We have manually verified a subset of the dataset for accuracy and data quality.\n\nLaunching RTEB: A Community Effort\n\nRTEB is launching today in beta. We believe building a robust benchmark is a community effort, and we plan to evolve RTEB based on feedback from developers and researchers alike. We encourage you to share your thoughts, suggest new datasets, find issues in existing datasets and help us build a more reliable standard for everyone. Please feel free to join the discussion or open an issue in the MTEB repository on Github.\n\nLimitations and Future Work\n\nTo highlight areas for improvement we want to be transparent about RTEB's current limitations and our plans for the future.\n\nBenchmark Scope: RTEB is focused on realistic, retrieval-first use cases. Highly challenging synthetic datasets are not a current goal but could be added in the future.\n\nRTEB is focused on realistic, retrieval-first use cases. Highly challenging synthetic datasets are not a current goal but could be added in the future. Modality: The benchmark currently evaluates text-only retrieval. We plan to incorporate text-image and other multimodal retrieval tasks in future releases.\n\nThe benchmark currently evaluates text-only retrieval. We plan to incorporate text-image and other multimodal retrieval tasks in future releases. Language Coverage: We are actively working to expand our language coverage, particularly for major languages like Chinese and Arabic, as well as more low-resource languages. If you know of high-quality datasets that fits these criteria please let us know.\n\nWe are actively working to expand our language coverage, particularly for major languages like Chinese and Arabic, as well as more low-resource languages. If you know of high-quality datasets that fits these criteria please let us know. Repurposing of QA dataset : About 50% of the current retrieval datasets are repurposed from QA datasets, which might lead to issues such as a strong lexical overlap between the question and the context, favoring models that rely on keyword matching over true semantic understanding.\n\n: About 50% of the current retrieval datasets are repurposed from QA datasets, which might lead to issues such as a strong lexical overlap between the question and the context, favoring models that rely on keyword matching over true semantic understanding. Private datasets: To test for generalization, we utilize private datasets that are only accessible to MTEB maintainers. To maintain fairness, all maintainers commit to not publishing models trained on these datasets and only testing on these private datasets through public channels, ensuring no company or individual receives unfair advantages.\n\nOur goal is for RTEB to become a community-trusted standard for retrieval evaluation.\n\nThe RTEB leaderboard is available today on Hugging Face as a part of the new Retrieval section on the MTEB leaderboard. We invite you to check it out, evaluate your models, and join us in building a better, more reliable benchmark for the entire AI community.",
    "link": "https://huggingface.co/blog/rteb",
    "Summary": "Introducing RTEB: A New Standard for Retrieval EvaluationPublished October 1, 2025 Update on GitHubWe\u2019re excited to introduce the beta version of the Retrieval Embedding Benchmark (RTEB) , a new benchmark designed to reliably evaluate the retrieval accuracy of embedding models for real-world applications.\nPerformance Discrepancy Between Public and Closed DatasetsTo address these challenges, we developed RTEB, a benchmark built to provide a reliable standard for evaluating retrieval models.\nIntroducing RTEBToday, we\u2019re excited to introduce the Retrieval Embedding Benchmark (RTEB).\n_EnglishFinance1 english, finance Closed Yes This retrieval dataset has been repurposed for retrieval from TAT-QA, a large-scale QA dataset using tabular and textual content.\nOur goal is for RTEB to become a community-trusted standard for retrieval evaluation.",
    "Keywords": [
      "data",
      "english",
      "models",
      "benchmark",
      "closed",
      "dataset",
      "standard",
      "open",
      "datasets",
      "legal",
      "evaluation",
      "rteb",
      "retrieval",
      "introducing"
    ]
  },
  {
    "Title": "Accelerating Qwen3-8B Agent on Intel\u00ae Core\u2122 Ultra with Depth-Pruned Draft Models",
    "Authors": [],
    "Publish Date": null,
    "Text": "Accelerating Qwen3-8B Agent on Intel\u00ae Core\u2122 Ultra with Depth-Pruned Draft Models\n\nPublished September 29, 2025 Update on GitHub\n\nTL;DR:\n\nQwen3-8B is one of the most exciting recent releases\u2014a model with native agentic capabilities, making it a natural fit for the AIPC.\n\nWith OpenVINO.GenAI, we\u2019ve been able to accelerate generation by ~1.3\u00d7 using speculative decoding with a lightweight Qwen3-0.6B draft.\n\nBy using speculative decoding and applying a simple pruning process to the draft, we pushed the speedup even further to ~1.4\u00d7\n\nWe wrapped this up by showing how these improvements can be used to run a fast, local AI Agent with \ud83e\udd17 smolagents\n\nQwen3\n\nQwen3-8B is part of the latest Qwen family, trained with explicit agentic behaviors. It supports tool invocation, multi-step reasoning, and long-context handling capabilities, that make it well-suited for complex agent workflows. When integrated with frameworks like Hugging Face \ud83e\udd17smolagents, QwenAgent, or AutoGen, it enables a wide range of agentic applications built around tool use and reasoning. Unlike single-turn chatbots, agentic applications rely on reasoning models that produce \u201cthinking aloud\u201d traces, intermediate steps that expand token usage, making inference speed critical to responsiveness. The combination of optimized inference and built-in agentic intelligence makes Qwen3-8B a compelling foundation for next-gen AI agents.\n\nAccelerating Qwen3-8B on Intel\u00ae Core\u2122 Ultra with Speculative Decoding\n\nWe started by benchmarking the 4-bit optimized OpenVINO version of Qwen3-8B on an Intel Lunar Lake integrated GPU, establishing this as our baseline for further acceleration\n\nSpeculative decoding is a method to speed up auto-regressive generation. It works by using a smaller, faster model as a draft to propose multiple tokens in a single forward pass, which are then validated by the larger target model in one forward pass. In our setup, Qwen3-8B served as the target model while Qwen3-0.6B was used as the draft. This approach delivered an average of 1.3\u00d7 speedup over the baseline.\n\nfrom openvino_genai import LLMPipeline, draft_model target_path = \"/path/to/target/Qwen3-8B-int4-ov\" draft_path = \"/path/to/draft/Qwen3-0.6B-int8-ov\" device = \"GPU\" model = LLMPipeline(target_path, device, draft_model=draft_model(draft_path, device)) streamer = lambda x: print (x, end= \"\" , flush= True ) model.generate( \"What is speculative decoding and how does it improve inference speed?\" , max_new_tokens= 100 , streamer=streamer)\n\nBefore initializing the LLMPipeline , make sure both the target and draft models are converted to OpenVINO. You can either download pre-converted models from the provided links or follow these instructions to convert your own.\n\nPushing Performance Further\n\nThe speculative decoding speedup depends on the average number of generated tokens per forward step of the target, \u03b3 \\gamma \u03b3, the speculation window size, and the ratio between the target and draft models' latency c c c. A smaller, faster (though less accurate) draft can often deliver greater acceleration. This inspired us to shrink the draft model while still preserving its quality, i.e. E ( # g e n e r a t e d _ t o k e n s ) E(\\# generated\\_tokens) E(#generated_tokens).\n\nS p e e d u p = E ( # g e n e r a t e d _ t o k e n s ) \u03b3 c + 1 Speedup = \\frac{E(\\# generated\\_tokens)}{\\gamma c + 1} Speedup=\u03b3c+1E(#generated_tokens)\u200b\n\nOur recent work shows that model depth (number of layers) is a major contributor to inference latency. We drew inspiration from recent work on layer-wise compression[1]. In our approach, we identify blocks of layers that contribute little, measured using angular distance, and remove them. After pruning, we apply fine-tuning to recover accuracy. Using this method, we pruned 6 out of 28 layers from the Qwen3-0.6B draft model. To recover the quality of the pruned draft model, we further finetuned it using synthetic data generated by Qwen3-8B. The data was produced by generating responses to 500k prompts from BAAI/Infinity-Instruct dataset.\n\nThe resulting pruned draft model delivered ~1.4x speedup compared to the baseline, an improvement over the ~1.3\u00d7 gain achieved with the original draft. This outcome aligns with theoretical expectations - reducing draft latency improves the over-all speedup, enabling faster and more efficient inference.\n\nThis demonstrates how pruning + speculative decoding can unlock faster and more efficient inference\u2014making local AI agents even more practical.\n\nCheck out the notebook and the Qwen3-0.6B depth-pruned draft model to reproduce our results step by step\n\nIntegration with \ud83e\udd17smolagents\n\nTo showcase the real-world potential, we deployed our optimized setup with the \ud83e\udd17smolagents library. With this integration, developers can plug in Qwen3-8B (paired with our pruned draft) to build agents that call APIs and external tools, write and execute code, handle long-context reasoning and run efficiently on Intel\u00ae Core\u2122 Ultra. The benefits aren\u2019t limited to Hugging Face, this model pairing can also be used seamlessly with frameworks like AutoGen or QwenAgent, further strengthening the agentic ecosystem.\n\nIn our demo, we assigned the accelerated Qwen3-based agent a task: \ud83d\udc49 Summarize the key features of the Qwen3 model series and present them in a slide deck.\n\nHere\u2019s how it worked: 1. The agent used a web search tool to gather up-to-date information. 2. It then switched to the Python interpreter to generate slides with the python-pptx library. This simple workflow highlights just a fraction of the possibilities unlocked when accelerated Qwen3 models meet frameworks like \ud83e\udd17smolagents, bringing practical, efficient AI agents to life on AI PC. Try it here \ud83d\ude80\n\nReferences\n\n[1] Gromov, A., Tirumala, K., Shapourian, H., Glorioso, P., & Roberts, D. A. (2025, January 22). The unreasonable ineffectiveness of the deeper layers. Poster presented at ICLR 2025. https://arxiv.org/abs/2403.17887",
    "link": "https://huggingface.co/blog/intel-qwen3-agent",
    "Summary": "With OpenVINO.GenAI, we\u2019ve been able to accelerate generation by ~1.3\u00d7 using speculative decoding with a lightweight Qwen3-0.6B draft.\n, max_new_tokens= 100 , streamer=streamer)Before initializing the LLMPipeline , make sure both the target and draft models are converted to OpenVINO.\nThis inspired us to shrink the draft model while still preserving its quality, i.e.\nThe resulting pruned draft model delivered ~1.4x speedup compared to the baseline, an improvement over the ~1.3\u00d7 gain achieved with the original draft.\nThis demonstrates how pruning + speculative decoding can unlock faster and more efficient inference\u2014making local AI agents even more practical.",
    "Keywords": [
      "ultra",
      "speculative",
      "models",
      "depthpruned",
      "draft",
      "core",
      "agentic",
      "target",
      "decoding",
      "agent",
      "accelerating",
      "speedup",
      "qwen38b",
      "using",
      "model",
      "intel"
    ]
  },
  {
    "Title": "VibeGame: Exploring Vibe Coding Games",
    "Authors": [],
    "Publish Date": null,
    "Text": "VibeGame: Exploring Vibe Coding Games\n\nPublished September 29, 2025 Update on GitHub\n\nPeople are trying to vibe code games. And it kind of works, at first. However, as the project grows, things begin to fall apart. Why? And what can we do about it?\n\nI'll talk about the problem, how I fixed it, and where to go from here.\n\nWhat Is \"Vibe Coding\"?\n\nFirst, what is vibe coding? It's originally coined by Andrej Karpathy in a viral tweet where it's defined as where you \"fully give in to the vibes, embrace exponentials and forget the code even exists\".\n\nHowever, since then, it's used descriptively to mean a lot of different things, anywhere from just \"using AI when coding\" to \"not thinking about the code at all\". In this blog post, I'll define it as: using AI as a high-level programming language to build something. Like other programming languages, this benefits from understanding what's going on under the hood, but doesn't necessarily require it.\n\nWith this interpretation, you could make a game without understanding code, though knowing the fundamentals still helps.\n\nContext Management\n\nEarlier I mentioned that \"as the project grows, things begin to fall apart\". This is because there is evidence that as the context window fills up, model performance begins to degrade. This is especially true for game development, where the context can grow very large, very quickly.\n\nTo address this issue, there are many personal ad-hoc solutions, such as writing LLM-specific context directly in the project files, or more comprehensive solutions like Claude Code Development Kit for large-scale context management.\n\nI couldn't find a lightweight, accessible solution, which doesn't rely on significant domain knowledge. So I made one: \ud83e\uddc5 Shallot, a simple, lightweight, unopinionated context management system for Claude Code. It relies on two basic commands:\n\n/peel [prompt] to load context at the beginning of a conversation /nourish to update context at the end of a conversation\n\nAnecdotally, this works well. However, it works best when the project stays lean and well-organized, so all relevant context can easily fit in the model's context window. While Claude Code is used here, the same principles generalize to other models.\n\nBeyond context management tools, platform choice is critical. The platform should ideally naturally keep projects lean through high-level abstractions, while also being something AI models understand well. So, what existing platforms are best suited for vibe coding?\n\nInitial Exploration\n\nI initially tried 3 different approaches to vibe coding games: Roblox MCP, Unity MCP, and web. For each, I tried to build a simple incremental game inspired by Grass Cutting Incremental, using Claude Code for each.\n\nHere's how it went:\n\nAttempt 1: Roblox MCP\n\nThe official MCP server from Roblox. This allows AI to interact with Roblox Studio by sending commands to run code.\n\nPros:\n\nExcellent level of abstraction with built-in game mechanics\n\nAI could very easily understand the syntax and convert instructions to code\n\nCons:\n\nNo files, only using code to read data, which severely limits context management\n\nVery limited runtime information for AI to work with\n\nProprietary walled garden\n\nRoblox provides an excellent layer of abstraction for keeping the codebase lean and manageable, which is perfect for vibe coding. However, the walled garden and lack of context makes it infeasible for vibe coding, unless it's in-house at Roblox.\n\nAttempt 2: Unity MCP\n\nThe unofficial MCP server for Unity. This allows AI to interact with the Unity Editor: reading the console, managing assets, and validating scripts.\n\nPros:\n\nFull file system access\n\nCons:\n\nThere are many ways to do everything in Unity, changing frequently across versions, causing AI to get confused\n\nRequires significant domain knowledge to tell the AI how to do things, rather than what to do\n\nAI performance was inconsistent and unreliable\n\nProprietary engine (though much more transparent than Roblox)\n\nUnity is a powerful engine with a lot of capabilities. However, the complexity and variability of the engine makes it difficult for AI to consistently produce good results without significant user domain knowledge.\n\nAttempt 3: Web Stack\n\nThe open web platform, using three.js for 3D rendering, rapier for physics, and bitecs for game logic.\n\nPros:\n\nFar superior AI proficiency compared to game engines, likely due to massive training data\n\nFull file system access\n\nFully open source stack with complete control/transparency\n\nCons:\n\nRelatively low level libraries, requiring essentially building the engine before building the game\n\nLack of ecosystem for high-quality 3D games; web tends toward 2D games and simple 3D experiences\n\nThis approach had the best AI performance by far, likely due to the vast amount of web development data available during training. However, the low-level nature of the libraries meant that I had to essentially build a game engine before I could build the game itself. This allows us to work at a much higher level of abstraction, like we did with Roblox.\n\nAlthough it required building an engine first, this approach was the only one that produced a fun result without heavy domain knowledge.\n\nComparison Summary\n\nPlatform AI Performance Abstraction Level Context Management Open Source Roblox \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50 \u274c Unity \u2b50 \u2b50\u2b50 \u2b50\u2b50\u2b50 \u274c Web \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50 \u2b50\u2b50\u2b50\u2b50\u2b50 \u2705\n\nThe Solution: VibeGame\n\nAfter these experiments, I had a clear picture: the web stack had excellent AI performance but was too low-level, while Roblox had perfect abstraction but lacked openness and context management.\n\nSo, what about combining the best of both?\n\nIntroducing VibeGame, a high-level declarative game engine built on top of three.js, rapier, and bitecs, designed specifically for AI-assisted game development.\n\nDesign Philosophy\n\nThere were three key decisions that went into the design of VibeGame:\n\nAbstraction: A high-level abstraction with built-in features like physics, rendering, and common game mechanics, keeping the codebase lean and manageable. This takes inspiration from popular high-level sandbox games/game \"engines\" like Roblox, Fortnite UEFN, and Minecraft. Syntax: A declarative XML-like syntax for defining game objects and their properties, making it easy for AI to understand and generate code. This is similar to HTML/CSS, which AI models are already proficient in. Architecture: An Entity-Component-System (ECS) architecture for scalability and flexibility. ECS separates data (components) from behavior (systems), encouraging the project to stay modular and organized as it grows, conducive to vibe coding and context management.\n\nA basic game looks like this:\n\n< world canvas = \"#game-canvas\" sky = \"#87ceeb\" > < static-part pos = \"0 -0.5 0\" shape = \"box\" size = \"20 1 20\" color = \"#90ee90\" > </ static-part > < dynamic-part pos = \"-2 4 -3\" shape = \"sphere\" size = \"1\" color = \"#ff4500\" > </ dynamic-part > </ world > < canvas id = \"game-canvas\" > </ canvas > < script type = \"module\" > import * as GAME from 'vibegame' ; GAME . run (); </ script >\n\nSee it in action in this JSFiddle or the Live Demo.\n\nThis will create a simple scene with a ground plane and a falling ball. The player, camera, and lighting are created automatically. All of this is modular and can be replaced. Arbitrary custom components and systems can be added as needed.\n\nThis comes bundled with an llms.txt file containing documentation about the engine, designed specifically for AI, to be included in its system prompt or initial context.\n\nSo Does It Actually Work?\n\nYes.\n\nWell, kind of.\n\nHere's the game I built to test building a simple incremental grass collection game using VibeGame and Claude Code. It worked very well, requiring minimal domain knowledge for implementing the core game mechanics.\n\nHowever, there are still some major caveats:\n\nIt works well for building what the game engine supports, i.e. a simple platformer or game that only relies on basic physics and rendering. However, it struggles with anything more complex that isn't yet implemented in the engine, like interaction, inventory, multiplayer, combat, etc.\n\nSo, with a definition of vibe coding that is the one-shot \"make me a game\" approach, it doesn't work. However, with the definition of treating vibe coding like a high-level programming language, it works very well, but requires users to understand the engine's capabilities and limitations.\n\nTry It Yourself\n\nTo try it immediately, I built a demo where you can develop a game directly in the browser using VibeGame with Qwen3-Next-80B-A3B-Instruct: Live Demo on Hugging Face.\n\nYou can also test it locally with a frontier model like Claude Code:\n\nnpm create vibegame@latest my-game cd my-game npm run dev\n\nThen, paste all the contents of the included llms.txt to CLAUDE.md , providing full documentation about the engine for the AI to reference (or point your own context management system to it). This works with other models as well.\n\nThe engine is currently very barebones and only supports very basic mechanics (unless writing it from scratch). However, initial results are promising.\n\nNext steps would be:\n\nFlesh out the engine with more built-in mechanics, getting closer to par with early versions of Roblox or UEFN. This includes:\n\nInteraction\n\nInventory/items\n\nMultiplayer\n\nSkinned meshes/animations with curated database\n\nAudio with curated database\n\nImprove the AI guidance systems, providing beginners with a better experience. This includes:\n\nClear messaging about engine capabilities/limitations\n\nGuided prompts for common tasks\n\nMany more examples and templates\n\nEducational resources\n\nIt's also worth exploring how vibe coding games could harness more proven engines. For example, building a high-level sandbox game editor on top of Unity or Unreal Engine (similar to how Unreal Editor for Fortnite is built on Unreal Engine) could provide a more controlled environment for AI to work with, while leveraging the power of established engines.\n\nWe're also likely to see more in-house solutions from major players.\n\nFollow me to keep up with what's going on in the space!\n\nLinks:",
    "link": "https://huggingface.co/blog/vibegame",
    "Summary": "VibeGame: Exploring Vibe Coding GamesPublished September 29, 2025 Update on GitHubPeople are trying to vibe code games.\nWhat Is \"Vibe Coding\"?\nInitial ExplorationI initially tried 3 different approaches to vibe coding games: Roblox MCP, Unity MCP, and web.\nIntroducing VibeGame, a high-level declarative game engine built on top of three.js, rapier, and bitecs, designed specifically for AI-assisted game development.\nThis includes:Clear messaging about engine capabilities/limitationsGuided prompts for common tasksMany more examples and templatesEducational resourcesIt's also worth exploring how vibe coding games could harness more proven engines.",
    "Keywords": [
      "engine",
      "coding",
      "vibegame",
      "roblox",
      "games",
      "exploring",
      "ai",
      "context",
      "vibe",
      "code",
      "game",
      "using",
      "web"
    ]
  },
  {
    "Title": "Swift Transformers Reaches 1.0 \u2013 and Looks to the Future",
    "Authors": [],
    "Publish Date": null,
    "Text": "Swift Transformers Reaches 1.0 \u2013 and Looks to the Future\n\nPublished September 26, 2025 Update on GitHub\n\nWe released swift-transformers two years ago (!) with the goal to support Apple developers and help them integrate local LLMs in their apps. A lot has changed since then (MLX and chat templates did not exist!), and we\u2019ve learned how the community is actually using the library.\n\nWe want to double down on the use cases that provide most benefits to the community, and lay out the foundations for the future. Spoiler alert: after this release, we\u2019ll focus a lot on MLX and agentic use cases \ud83d\ude80\n\nWhat is swift-transformers\n\nswift-transformers is a Swift library that aims to reduce the friction for developers that want to work with local models on Apple Silicon platforms, including iPhones. It includes the missing pieces that are not provided by Core ML or MLX alone, but that are required to work with local inference. Namely, it provides the following components:\n\nTokenizers . Preparing inputs for a language model is surprisingly complex. We've built a lot of experience with our tokenizers Python and Rust libraries, which are foundational to the AI ecosystem. We wanted to bring the same performant, ergonomic experience to Swift. The Swift version of Tokenizers should handle everything for you, including chat templates and agentic use!\n\n. Preparing inputs for a language model is surprisingly complex. We've built a lot of experience with our Python and Rust libraries, which are foundational to the AI ecosystem. We wanted to bring the same performant, ergonomic experience to Swift. The Swift version of should handle everything for you, including chat templates and agentic use! Hub . This is an interface to the Hugging Face Hub, where all open models are available. It allows you to download models from the Hub and cache them locally, and supports background resumable downloads, model updates, offline mode. It contains a subset of the functionality provided by the Python and JavaScript libraries, focused on the tasks that Apple developers need the most (i.e., uploads are not supported).\n\n. This is an interface to the Hugging Face Hub, where all open models are available. It allows you to download models from the Hub and cache them locally, and supports background resumable downloads, model updates, offline mode. It contains a subset of the functionality provided by the Python and JavaScript libraries, focused on the tasks that Apple developers need the most (i.e., uploads are not supported). Models and Generation . These are wrappers for LLMs converted to the Core ML format. Converting them is out of the scope of the library (but we have some guides). Once they are converted, these modules make it easy to run inference with them.\n\nTest app from mlx-swift-examples, showing SmolVLM2 explaining actions in a video.\n\nHow is the community using it\n\nMost of the time people use the Tokenizers or Hub modules, and frequently both. Some notable projects that rely on swift-transformers include:\n\nmlx-swift-examples , by Apple. It\u2019s, in fact, not just a collection of examples, but a list of libraries you can use to run various types of models using MLX, including LLMs and VLMs (vision-language models). It\u2019s kind of our Models and Generation libraries but for MLX instead of Core ML \u2013 and it supports many more model types like embedders or Stable Diffusion.\n\n, by Apple. It\u2019s, in fact, not just a collection of examples, but a list of libraries you can use to run various types of models using MLX, including LLMs and VLMs (vision-language models). It\u2019s kind of our and libraries but for MLX instead of Core ML \u2013 and it supports many more model types like embedders or Stable Diffusion. WhisperKit, by argmax. Open Source ASR (speech recognition) framework, super heavily optimized for Apple Silicon. It relies on our Hub and Tokenizers modules.\n\nand modules. FastVLM, by Apple, and many other app demos, such as our own SmolVLM2 native app.\n\nWhat changes with v1.0\n\nVersion 1.0 signals stability in the package. Developers are building apps on swift-transformers, and this first major release recognizes those use cases and brings the version number in line with that reality. It also provides the foundation on which to iterate with the community to build the next set of features. These are some of our preferred updates:\n\nTokenizers and Hub are now first-class, top-level modules. Before 1.0, you had to depend on and import the full package, whereas now you can just pick Tokenizers , for instance.\n\nare now first-class, top-level modules. Before 1.0, you had to depend on and import the full package, whereas now you can just pick , for instance. Speaking of Jinja, we are super proud to announce that we have collaborated with John Mai (X) to create the next version of his excellent Swift Jinja library . John\u2019s work has been crucial for the community: he single-handedly took on the task to provide a solid chat template library that could grow as templates became more and more complex. The new version is a couple orders of magnitude faster (no kidding), and lives here as swift-jinja .\n\n. John\u2019s work has been crucial for the community: he single-handedly took on the task to provide a solid chat template library that could grow as templates became more and more complex. The new version is a couple orders of magnitude faster (no kidding), and lives here as . To further reduce the load imposed on downstream users, we have removed our example CLI targets and the swift-argument-parser dependency , which in turn prevents version conflicts for projects that already use it.\n\n, which in turn prevents version conflicts for projects that already use it. Thanks to contributions by Apple, we have adopted Modern Core ML APIs with support for stateful models (for easier KV-caching) and expressive MLTensor APIs \u2013 this removes thousands of lines of custom tensor operations and math code.\n\nwith support for stateful models (for easier KV-caching) and expressive APIs \u2013 this removes thousands of lines of custom tensor operations and math code. Lots of additional cruft removed and API surface reduced to reduce cognitive load and iterate faster.\n\nto reduce cognitive load and iterate faster. Tests are better, faster, stronger.\n\nare better, faster, stronger. Documentation comments have been added to public APIs.\n\nhave been added to public APIs. Swift 6 is fully supported.\n\nVersion 1.0 comes with breaking API changes. However, we don\u2019t expect major problems if you are a user of Tokenizers or Hub . If you use the Core ML components of the library, please get in touch so we can support you during transition. We\u2019ll prepare a migration guide and add it to the documentation.\n\nUsage Examples\n\nHere's how to use Tokenizers to format tool calling input for an LLM:\n\nimport Tokenizers let tokenizer = try await AutoTokenizer .from(pretrained: \"mlx-community/Qwen2.5-7B-Instruct-4bit\" ) let weatherTool = [ \"type\" : \"function\" , \"function\" : [ \"name\" : \"get_current_weather\" , \"description\" : \"Get the current weather in a given location\" , \"parameters\" : [ \"type\" : \"object\" , \"properties\" : [ \"location\" : [ \"type\" : \"string\" , \"description\" : \"City and state\" ]], \"required\" : [ \"location\" ] ] ] ] let tokens = try tokenizer.applyChatTemplate( messages: [[ \"role\" : \"user\" , \"content\" : \"What's the weather in Paris?\" ]], tools: [weatherTool] )\n\nFor additional examples, please check this section in the README and the Examples folder.\n\nWhat comes next\n\nHonestly, we don\u2019t know. We do know that we are super interested in exploring MLX, because that\u2019s usually the current go-to approach for developers getting started with ML in native apps, and we want to help make the experience as seamless as possible. We are thinking along the lines of better integration with mlx-swift-examples for LLMs and VLMs, potentially through pre-processing and post-processing operations that developers encounter frequently.\n\nWe are also extremely excited about agentic use in general and MCP in particular. We think that exposure of system resources to local workflows would be \ud83d\ude80\n\nIf you want to follow along in this journey or want to share your ideas, please contact us through our social networks or the repo.\n\nWe couldn\u2019t have done this without you \ud83e\udef5\n\nWe are immensely grateful to all the contributors and users of the library for your help and feedback. We love you all, and can't wait to continue working with you to shape the future of on-device generation! \u2764\ufe0f",
    "link": "https://huggingface.co/blog/swift-transformers",
    "Summary": "Swift Transformers Reaches 1.0 \u2013 and Looks to the FuturePublished September 26, 2025 Update on GitHubWe released swift-transformers two years ago (!)\nWe've built a lot of experience with our tokenizers Python and Rust libraries, which are foundational to the AI ecosystem.\nThe Swift version of Tokenizers should handle everything for you, including chat templates and agentic use!\nThe Swift version of should handle everything for you, including chat templates and agentic use!\nWe love you all, and can't wait to continue working with you to shape the future of on-device generation!",
    "Keywords": [
      "models",
      "apple",
      "version",
      "future",
      "ml",
      "mlx",
      "reaches",
      "tokenizers",
      "hub",
      "library",
      "libraries",
      "looks",
      "transformers",
      "swift"
    ]
  },
  {
    "Title": "Smol2Operator: Post-Training GUI Agents for Computer Use",
    "Authors": [],
    "Publish Date": null,
    "Text": "Smol2Operator: Post-Training GUI Agents for Computer Use\n\nPublished September 23, 2025 Update on GitHub\n\nThis video demonstrates the model obtained through the recipe described below, executing a task end-to-end.\n\nTable of Contents\n\nIntroduction\n\nThis work shows how a lightweight vision\u2013language model can acquire GUI-grounded skills and evolve into an agentic GUI coder. We release all training recipes, data-processing tools, resulting model, demo and datasets to enable full reproducibility and foster further research \ud83e\udee1. Find the collection here\n\nGraphical User Interface (GUI) automation is one of the most challenging frontiers in computer vision. Developing models that see and interact with user interfaces enables AI agents to navigate mobile, desktop, and web platforms. This will reshape the future of digital interaction.\n\nIn this blog post, we present a comprehensive approach to training vision-language models for GUI automation through a multi-phase training strategy. We demonstrate how to transform a model with zero grounding capabilities into an agentic coder capable of understanding and interacting with graphical interfaces.\n\nRather than aiming for a SOTA model, our goal is to demonstrate the entire process, from data processing to model training, and, in doing so, show how to unlock GUI-grounding capabilities in VLMs.\n\nGUI capabilities combine understanding of the interface and precise element localization. These abilities enable the model to translate high-level tasks into low-level GUI actions such as clicking, typing, \u2026\n\nOur approach leverages SmolVLM2-2.2B-Instruct as the baseline model, a small powerful vision-language model that initially has no grounding capabilities for GUI tasks. This makes it an ideal candidate to demonstrate the effectiveness of our training methodology. Through our two-phase training process, we first instill grounding capabilities in the model, then enhance it with agentic reasoning abilities using Supervised Fine-Tuning (SFT).\n\nWe evaluate our approach on an established perception benchmark: ScreenSpot-v2, which tests the model\u2019s ability to understand and locate elements within screenshots. Our process is inspired by the AGUVIS paper, and we leverage their carefully curated datasets to build upon their foundational work.\n\nEvolution of ScreenSpot-v2 performance during the training phase of the base model SmolVLM2-2.2B-Instruct.\n\n1. Data Transformation and Unified Action Space\n\nThis section explains how we convert heterogeneous GUI actions format from multiple datasets into a single unified format. By standardizing function names, signatures, and parameters, we create consistent, high-quality data that forms the foundation for effective model training.\n\nThe Challenge of Inconsistent Action Spaces\n\nOne of the primary challenges when working with multiple GUI automation datasets is the lack of standardization in action representations. Different datasets use varying function signatures, parameter naming conventions, and action taxonomies, making it difficult to train a unified model across diverse data sources.\n\nOur Unified Approach\n\nWe took the open-source datasets (xlangai/aguvis-stage1, xlangai/aguvis-stage2), originally used by AGUVIS, and implemented a comprehensive data transformation pipeline to create a unified action space. Our approach involved:\n\nFunction Parsing and Normalization: We developed a function parser (see utils/function_parser.py ) that can extract and parse function calls from various formats across all datasets. This parser supports any function signature format, handles complex parameter structures, and can reconstruct function calls with proper parameter ordering. Action Space Unification: We implemented a comprehensive action conversion system (see preprocessing/action_conversion.py ) that transforms all original action representations into a standardized function naming and argument structure. This process highlighted the significant inconsistencies in function signatures across different datasets and allowed us to: Remove undesired or redundant actions\n\nStandardize parameter naming conventions\n\nCreate a cohesive action vocabulary (Bonus) Flexible Adaptation Framework: Our transformation pipeline includes utilities that allow users to: Adapt the entire dataset to their own action space naming conventions using the utils/action_space_converter.py tool\n\ntool Extract and analyze the current action space structure\n\nExample Data Transformation\n\nHere are real examples from our action conversion system ( preprocessing/action_conversion.py ) showing how we transform heterogeneous action representations into our unified format (grounding coordinates normalized to [0,1]):\n\nBefore (Original Action Dataset Formats):\n\nmobile.home() mobile.open_app(app_name= 'drupe' ) mobile.swipe(from_coord=[ 0.581 , 0.898 ], to_coord=[ 0.601 , 0.518 ]) mobile.long_press(x= 0.799 , y= 0.911 ) mobile.terminate(status= 'success' ) pyautogui.click(x= 0.8102 , y= 0.9463 ) pyautogui.doubleClick(x= 0.8102 , y= 0.9463 ) pyautogui.hotkey(keys=[ 'ctrl' , 'c' ]) pyautogui.scroll(page=- 0.1 ) pyautogui.write(message= 'bread buns' ) pyautogui.dragTo(from_coord=[ 0.87 , 0.423 ], to_coord=[ 0.8102 , 0.9463 ])\n\nAfter (Unified Action Dataset Formats):\n\nnavigate_home() open_app(app_name= 'drupe' ) swipe(from_coord=[ 0.581 , 0.898 ], to_coord=[ 0.601 , 0.518 ]) long_press(x= 0.799 , y= 0.911 ) final_answer( 'success' ) click(x= 0.8102 , y= 0.9463 ) double_click(x= 0.8102 , y= 0.9463 ) press(keys=[ 'ctrl' , 'c' ]) scroll(direction= 'up' , amount= 10 ) type (text= 'bread buns' ) drag(from_coord=[ 0.87 , 0.423 ], to_coord=[ 0.8102 , 0.9463 ])\n\nThis unification process was essential for creating coherent training data that allows the model to learn consistent action patterns across diverse GUI environments.\n\n\ud83d\udca1 Why Normalized Coordinates?\n\nUsing raw pixel coordinates in text-action datapoint (e.g. click(x=302, y=63) ) ties them to a single image size. Vision Language Models (VLMs) often resize images, causing pixel coordinates to break and require adjustment. Normalized coordinates (relative to image size) remain valid at any resolution and keep the dataset consistent.\n\n(Bonus) Custom Action Space Adaptation with Action Space Converter\n\nTo maximize flexibility for different use cases, we developed the Action Space Converter ( utils/action_space_converter.py ), a tool that allows users to easily adapt from an action space to their own custom action vocabularies and naming conventions.\n\nYou can use this tool to transform one action signature (function names, parameter names, and parameter value changes, ...) into another:\n\nBefore\n\nassistant_message: \"Action: click(x=0.5, y=0.3)\"\n\nAfter\n\nassistant_message: \"Action: touch(x_coord=200, y_coord=300)\"\n\nKey Features\n\nThe Action Space Converter provides:\n\nConfigurable Mappings: Define custom mappings between unified actions and your preferred action names Parameter Transformation: Rename parameters, apply value transformations, and set default values Flexible Architecture: Support for both simple parameter mappings and complex custom transformation functions Validation: Built-in validation to ensure mapping configurations are valid\n\nUsage Example\n\nfrom utils.action_space_converter import ActionSpaceConverter, ActionMapping, ParameterMapping from utils.function_parser import parse_function_call mappings = [ ActionMapping( source_function= \"click\" , target_function= \"touch\" , parameter_mappings=[ ParameterMapping(source_name= \"x\" , target_name= \"x_coord\" ), ParameterMapping(source_name= \"y\" , target_name= \"y_coord\" ) ], description= \"Touch screen at coordinates\" ), ActionMapping( source_function= \"type\" , target_function= \"write\" , parameter_mappings=[ ParameterMapping(source_name= \"text\" , target_name= \"content\" ) ], description= \"Input text\" ) ] assistant_message = \"I'll interact at those coordinates for you. click(x=0.5, y=0.3) Now I'll input the text. type(text='hello world')\" parsed_function_calls = parse_function_call(text) converter = ActionSpaceConverter(mappings) converted_actions = converter.convert_actions(parsed_function_calls) for new_function_call, old_function_call in zip (converted_actions, parsed_function_calls): text = text.replace(old_function_call.to_string(), new_function_call.to_string()) print (text)\n\nThis tool enables researchers and practitioners to:\n\nCustomize Training Data : Adapt the dataset to match their specific action vocabulary requirements\n\n: Adapt the dataset to match their specific action vocabulary requirements Domain Adaptation : Transform actions for different platforms (mobile vs. desktop vs. web)\n\n: Transform actions for different platforms (mobile vs. desktop vs. web) Framework Integration : Easily align training data with existing automation frameworks\n\n: Easily align training data with existing automation frameworks Rapid Experimentation : Quickly test different action space configurations\n\n: Quickly test different action space configurations Release Preparation: Standardize action spaces for production deployment with consistent naming conventions\n\nThe Action Space Converter is particularly valuable for preparing datasets for training, as it ensures consistent action vocabularies across different deployment environments while maintaining compatibility with existing automation frameworks.\n\nTransformed and Released Datasets\n\nThrough this pipeline, we transform the open-source datasets xlangai/aguvis-stage1, xlangai/aguvis-stage2 into our unified action space (see here). The output of this process is released as two new fully formatted datasets: smolagents/aguvis-stage-1 and smolagents/aguvis-stage-2.\n\n2. Phase 1: From Zero to Perception\n\nTraining Data\n\nPhase 1 leverages the smolagents/aguvis-stage-1 dataset, which introduces GUI grounding by pairing low-level instructions with diverse executable actions (expressed in code form). For example, a user/assistant turn in smolagents/aguvis-stage-1 follows the structure:\n\n{ \"user\" : \"click on more button\" , \"assistant\" : \"click(x=0.8875, y=0.2281)\" , }\n\nEach sample links a screenshot with multi-turn user/assistant interactions, enabling the model to learn fine-grained action grounding across dialogue turns. During fine-tuning, the data collator masks everything except the assistant\u2019s answers when computing the loss.\n\nOptimization Experiments\n\nBefore proceeding with full-scale Phase 1 training, we conducted comprehensive ablation studies to determine optimal training configurations\n\nImage Resolution and Coordinate System Analysis\n\nWe experimented with different image sizes and coordinate representation systems to identify the optimal configuration for SmolVLM2:\n\nImage Sizes Tested : 384px, 768px, 1152px\n\n: 384px, 768px, 1152px Coordinate Systems : Pixel coordinates vs. normalized coordinates (0-1 range)\n\n: Pixel coordinates vs. normalized coordinates (0-1 range) Training Data: 400K samples from Aguvis datasets\n\nSome SOTA GUI VLMs (e.g., Qwen-VL) appear also to use a different normalized range (0\u20131000), which was not tested in this experiment.\n\nConfiguration (coords / image size) Screenspot-v2 (%) Normalized coordinates Base / \u2013 0.47 384 31.28 764 32.32 1152 33.72 Pixel coordinates Base / \u2013 0.55 384 1.17 764 2.67 1152 4.32 Table 1: Baseline on HuggingFaceTB/SmolVLM2-2.2B-Instruct (400k samples, aguvis-stage-1). Higher is better.\n\nAs demonstrated in our benchmark results, SmolVLM2-2.2B-Instruct base initially achieved 0% performance on perception benchmarks like ScreenSpot-v2. This complete lack of grounding capability provided us with a clean slate to evaluate the effectiveness of our training methodology.\n\nKey Findings\n\nFrom our experiments, we determined that:\n\nImage Size : 1152px\n\n: 1152px Coordinate System : Normalized coordinates (0-1 range) proved most effective for SmolVLM2\n\n: Normalized coordinates (0-1 range) proved most effective for SmolVLM2 Note: The optimal choice between pixel and normalized coordinates may vary depending on the base model\u2019s pre-training approach\n\nPhase 1 Results\n\nUsing the optimal configuration (1152px resolution with normalized coordinates), we trained for 2 epochs on the smolagents/aguvis-stage-1 dataset. The results were remarkable, +41% improvement over baseline on ScreenSpot-v2\n\nThis dramatic improvement demonstrates that our Phase 1 training successfully instilled fundamental grounding capabilities in the model, enabling it to understand and locate visual elements within screenshots.\n\nConfiguration (coords / image size) Screenspot-v2 (%) Normalized coordinates / 1152 41.27 Table 2: Baseline on HuggingFaceTB/SmolVLM2-2.2B-Instruct (2 epochs, aguvis-stage-1).\n\n3. Phase 2: From Perception to Cognition\n\nWhereas Phase 1 provided grounding capabilities, Phase 2 targets agentic reasoning, the ability to deliberate and plan before acting. This stage transforms the model from a reactive system identifying GUI elements into a proactive agent capable of executing complex, multi-step interactions.\n\nTraining Data\n\nPhase 2 uses the smolagents/aguvis-stage-2 dataset, which introduces agentic scenarios:\n\nExplicit reasoning about upcoming actions\n\nContext consistency across multiple interaction steps\n\nHigh-level instructions require multi-step, low-level actions.\n\nFor example, the smolagents/aguvis-stage-2 chat message is like this:\n\n{ \"system\" : \"You are a helpful GUI agent. ...\" , \"user\" : \"Please generate the next move according to the UI screenshot, instruction and previous actions.\n\n\n\nInstruction: What information does the site provide about Judith Lauand's career, works and exhibitions?\n\n\n\nPrevious actions:\n\nNone\" , \"assistant\" : \"<think>\n\nClick on the link labeled 'Judith Lauand: Brazilian 1922-2022' to explore more about her career and exhibitions.\n\n</think>\n\n<code>\n\nclick(x=0.41, y=0.178)\n\n</code>\" , }\n\nEach sample links a screenshot with a system/user/assistant turn. During fine-tuning, the data collator masks everything except the assistant\u2019s answers when computing the loss.\n\nPhase 2 Results\n\nStarting from the Phase 1 checkpoint (1152 px resolution, normalized coordinates), we fine-tuned the model for two epochs on smolagents/aguvis-stage-2. The accuracy on ScreenSpot-v2 increased from 41% to 61%, indicating that explicit reasoning improves GUI grounding performance.\n\nConfiguration (coords / image size) Screenspot-v2 (%) Normalized coordinates / 1152 61.71 Table 2: Baseline on HuggingFaceTB/SmolVLM2-2.2B-Instruct after Phase 1 finetuning (2 epochs, aguvis-stage-1).\n\n~58% on ScreenSpot-v2, demonstrating that the training strategy scales down effectively, making it SOTA on ScreenSpot-v2 for this model size (460M parameters). In addition, aguvis-stage-1 is already included in \ud83d\udca1 We also reproduced the two-phase training on a much smaller VLM (nanoVLM-460M). Despite its reduced capacity, the model achieved, demonstrating that the training strategy scales down effectively,. In addition, aguvis-stage-1 is already included in FineVision Dataset\n\n4. All you need is Open Source\n\nAll training code, data processing pipelines, datasets and model are open-source!\n\nTraining Recipe ( recipe.ipynb ): Complete training pipeline for both Phase 1 and Phase 2, including dataset mixture configurations and training orchestration. We leverage the TRL library to train our models. Datasets ( smolagents/aguvis-stage-1 , smolagents/aguvis-stage-2 ): all datasets used are open-source. Model ( smolagents/SmolVLM2-2.2B-Instruct-Agentic-GUI ): the model produced by applying the training recipe described above. Preprocessing Tools: Function Parser ( utils/function_parser.py ): Utilities for parsing, normalizing, and reconstructing function calls from diverse dataset formats. Supports complex parameter structures, positional arguments, and multiple function call extraction.\n\n( ): Utilities for parsing, normalizing, and reconstructing function calls from diverse dataset formats. Supports complex parameter structures, positional arguments, and multiple function call extraction. Action Conversion System ( preprocessing/action_conversion.py ): Core unification engine transforming mobile and PyAutoGUI desktop actions into a standardized API format. Features smart coordinate handling, direction detection for scroll actions, and comprehensive parameter normalization.\n\n( ): Core unification engine transforming mobile and PyAutoGUI desktop actions into a standardized API format. Features smart coordinate handling, direction detection for scroll actions, and comprehensive parameter normalization. Action Space Converter ( utils/action_space_converter.py ): Flexible tool for adapting the unified action space to custom vocabularies and naming conventions. Enables domain-specific customization through configurable parameter mappings.\n\n\ud83d\udca1 We\u2019ve also released a Space to try the model\u2019s agentic grounding capabilities: A-Mahla/Smol2Operator\n\n5. Conclusion\n\nOur experiments demonstrate that high-quality, reasoning-oriented data can substantially improve GUI grounding, even for small VLMs, using only supervised fine-tuning (SFT). Beyond raw performance gains, these results show that the GUI grounding capabilities are largely determined by the quality of the data. Carefully curated datasets teach models the structure and semantics of user interfaces, providing the grounding needed for accurate action prediction.\n\nTo support the development of GUI agents, we\u2019re open-sourcing everything: our complete pipeline, datasets, and trained model. You can reproduce our results, experiment with different models and architectures, or adapt our approach to new domains. The future of agentic AI depends on researchers like you pushing these boundaries further!\n\nWhile SFT excels at supervised tasks, emerging methods such as Reinforcement Learning (RL) or Direct Preference Optimization (DPO) help develop stronger reasoning capabilities and enable real-time adaptation. These advances point toward a new generation of GUI agents that learn and improve through interaction rather than relying solely on static datasets.\n\nLet\u2019s build the future of GUI agents together \ud83e\udd17",
    "link": "https://huggingface.co/blog/smol2operator",
    "Summary": "Smol2Operator: Post-Training GUI Agents for Computer UsePublished September 23, 2025 Update on GitHubThis video demonstrates the model obtained through the recipe described below, executing a task end-to-end.\n(Bonus) Custom Action Space Adaptation with Action Space ConverterTo maximize flexibility for different use cases, we developed the Action Space Converter ( utils/action_space_converter.py ), a tool that allows users to easily adapt from an action space to their own custom action vocabularies and naming conventions.\nKey FindingsFrom our experiments, we determined that:Image Size : 1152px: 1152px Coordinate System : Normalized coordinates (0-1 range) proved most effective for SmolVLM2: Normalized coordinates (0-1 range) proved most effective for SmolVLM2 Note: The optimal choice between pixel and normalized coordinates may vary depending on the base model\u2019s pre-training approachPhase 1 ResultsUsing the optimal configuration (1152px resolution with normalized coordinates), we trained for 2 epochs on the smolagents/aguvis-stage-1 dataset.\nThe accuracy on ScreenSpot-v2 increased from 41% to 61%, indicating that explicit reasoning improves GUI grounding performance.\nAction Space Converter ( utils/action_space_converter.py ): Flexible tool for adapting the unified action space to custom vocabularies and naming conventions.",
    "Keywords": [
      "data",
      "smol2operator",
      "agents",
      "coordinates",
      "training",
      "normalized",
      "space",
      "grounding",
      "datasets",
      "posttraining",
      "gui",
      "computer",
      "model",
      "action"
    ]
  },
  {
    "Title": "Scaleway on Hugging Face Inference Providers \ud83d\udd25",
    "Authors": [],
    "Publish Date": null,
    "Text": "Scaleway on Hugging Face Inference Providers \ud83d\udd25\n\nPublished September 19, 2025 Update on GitHub\n\nWe're thrilled to share that Scaleway is now a supported Inference Provider on the Hugging Face Hub! Scaleway joins our growing ecosystem, enhancing the breadth and capabilities of serverless inference directly on the Hub\u2019s model pages. Inference Providers are also seamlessly integrated into our client SDKs (for both JS and Python), making it super easy to use a wide variety of models with your preferred providers.\n\nThis launch makes it easier than ever to access popular open-weight models like gpt-oss, Qwen3, DeepSeek R1, and Gemma 3 \u2014 right from Hugging Face. You can browse Scaleway's org on the Hub at https://huggingface.co/scaleway and try trending supported models at https://huggingface.co/models?inference_provider=scaleway&sort=trending.\n\nScaleway Generative APIs is a fully managed, serverless service that provides access to frontier AI models from leading research labs via simple API calls. The service offers competitive pay-per-token pricing starting at \u20ac0.20 per million tokens.\n\nThe service runs on secure infrastructure located in European data centers (Paris, France), ensuring data sovereignty and low latency for European users. The platform supports advanced features including structured outputs, function calling, and multimodal capabilities for both text and image processing.\n\nBuilt for production use, Scaleway's inference infrastructure delivers sub-200ms response times for first tokens, making it ideal for interactive applications and agentic workflows. The service supports both text generation and embedding models. You can learn more about Scaleway's platform and infrastructure at https://www.scaleway.com/en/generative-apis/.\n\nRead more about how to use Scaleway as an Inference Provider in its dedicated documentation page.\n\nSee the list of supported models here.\n\nHow it works\n\nIn the website UI\n\nIn your user account settings, you are able to:\n\nSet your own API keys for the providers you\u2019ve signed up with. If no custom key is set, your requests will be routed through HF.\n\nOrder providers by preference. This applies to the widget and code snippets in the model pages.\n\nAs mentioned, there are two modes when calling Inference Providers:\n\nCustom key (calls go directly to the inference provider, using your own API key of the corresponding inference provider)\n\nRouted by HF (in that case, you don't need a token from the provider, and the charges are applied directly to your HF account rather than the provider's account)\n\nModel pages showcase third-party inference providers (the ones that are compatible with the current model, sorted by user preference)\n\nFrom the client SDKs\n\nfrom Python, using huggingface_hub\n\nThe following example shows how to use OpenAI's gpt-oss-120b using Scaleway as the inference provider. You can use a Hugging Face token for automatic routing through Hugging Face, or your own Scaleway API key if you have one.\n\nNote: this requires using a recent version of huggingface_hub (>= 0.34.6).\n\nimport os from huggingface_hub import InferenceClient client = InferenceClient( provider= \"scaleway\" , api_key=os.environ[ \"HF_TOKEN\" ], ) messages = [ { \"role\" : \"user\" , \"content\" : \"Write a poem in the style of Shakespeare\" } ] completion = client.chat.completions.create( model= \"openai/gpt-oss-120b\" , messages=messages, ) print (completion.choices[ 0 ].message)\n\nfrom JS using @huggingface/inference\n\nimport { InferenceClient } from \"@huggingface/inference\" ; const client = new InferenceClient (process. env . HF_TOKEN ); const chatCompletion = await client. chatCompletion ({ model : \"openai/gpt-oss-120b\" , messages : [ { role : \"user\" , content : \"Write a poem in the style of Shakespeare\" , }, ], provider : \"scaleway\" , }); console . log (chatCompletion. choices [ 0 ]. message );\n\nBilling\n\nHere is how billing works:\n\nFor direct requests, i.e. when you use the key from an inference provider, you are billed by the corresponding provider. For instance, if you use a Scaleway API key you're billed on your Scaleway account.\n\nFor routed requests, i.e. when you authenticate via the Hugging Face Hub, you'll only pay the standard provider API rates. There's no additional markup from us; we just pass through the provider costs directly. (In the future, we may establish revenue-sharing agreements with our provider partners.)\n\nImportant Note \u203c\ufe0f PRO users get $2 worth of Inference credits every month. You can use them across providers. \ud83d\udd25\n\nSubscribe to the Hugging Face PRO plan to get access to Inference credits, ZeroGPU, Spaces Dev Mode, 20x higher limits, and more.\n\nWe also provide free inference with a small quota for our signed-in free users, but please upgrade to PRO if you can!\n\nFeedback and next steps\n\nWe would love to get your feedback! Share your thoughts and/or comments here: https://huggingface.co/spaces/huggingface/HuggingDiscussions/discussions/49",
    "link": "https://huggingface.co/blog/inference-providers-scaleway",
    "Summary": "Scaleway on Hugging Face Inference Providers \ud83d\udd25Published September 19, 2025 Update on GitHubWe're thrilled to share that Scaleway is now a supported Inference Provider on the Hugging Face Hub!\nYou can use a Hugging Face token for automatic routing through Hugging Face, or your own Scaleway API key if you have one.\nFor instance, if you use a Scaleway API key you're billed on your Scaleway account.\nwhen you authenticate via the Hugging Face Hub, you'll only pay the standard provider API rates.\n\ud83d\udd25Subscribe to the Hugging Face PRO plan to get access to Inference credits, ZeroGPU, Spaces Dev Mode, 20x higher limits, and more.",
    "Keywords": [
      "inference",
      "models",
      "face",
      "hugging",
      "api",
      "providers",
      "key",
      "provider",
      "using",
      "scaleway"
    ]
  },
  {
    "Title": "Gaia2 and ARE: Empowering the community to study agents",
    "Authors": [],
    "Publish Date": null,
    "Text": "Gaia2 and ARE: Empowering the Community to Evaluate Agents\n\nPublished September 22, 2025 Update on GitHub\n\nIn an ideal world, AI agents would be reliable assistants. When given a query, they would easily manage ambiguity in instructions, construct step-by-step plans, correctly identify necessary resources, execute those plans without getting sidetracked, and adapt to unexpected events, all while maintaining accuracy and avoiding hallucinations. However, developing agents and testing these behaviors is no small feat: if you have ever tried to debug your own agent, you\u2019ve probably observed how tedious and frustrating this can be. Existing evaluation environments are tightly coupled with the tasks they evaluate, lack real-world flexibility, and do not reflect the messy reality of open-world agents: simulated pages never fail to load, events don\u2019t spontaneously emerge, and asynchronous chaos is absent.\n\nThat\u2019s why we\u2019re very happy to introduce Gaia2, the follow-up to the agentic benchmark GAIA, allowing analysis of considerably more complex behaviors. Gaia2 is released with the open Meta Agents Research Environments (ARE) framework to run, debug and evaluate agents. ARE simulates complex real world-like conditions and can be customized to further study agents behaviors. Gaia2 dataset is released under CC by 4.0 license, and ARE under MIT license.\n\nGaia2: Agentic Evaluation on Real Life Assistant Tasks\n\nGAIA is an agentic benchmark published in 2023, with 3 levels of information retrieval questions requiring tools, web browsing, and reasoning to solve. In 2 years, the easiest levels have become too easy for models, and the community is coming close to solving the hardest questions, so it was time for an entirely new and harder agent benchmark!\n\nHere comes Gaia2, a follow up to GAIA, going way beyond it in terms of capabilities studied!\n\nWhere GAIA was read-only, Gaia2 is now a read-and-write benchmark, focusing on interactive behavior and complexity management. Agents are now evaluated not only on search and retrieval, but also on instruction following over ambiguous or time-sensitive queries, in a noisy environment with controlled failures - reflecting real-world conditions more than any other simulated environment. We want to test how agents manage tools or APIs that sometimes do not work, plan successions of actions with very specific time frames, and adapt to new events - a whole new range of complexity!\n\nTo do this, we use the following task groups (thanks to 1000 brand new human-created scenarios):\n\nExecution : Multi-step instruction following and tool-use (e.g., contact updates)\n\n: Multi-step instruction following and tool-use (e.g., contact updates) Search : Cross-source information gathering (e.g., friend cities from WhatsApp)\n\n: Cross-source information gathering (e.g., friend cities from WhatsApp) Ambiguity Handling : Clarification of conflicting requests (e.g., scheduling conflicts)\n\n: Clarification of conflicting requests (e.g., scheduling conflicts) Adaptability : Response to changes in the simulation (e.g., updating an email using follow up information)\n\n: Response to changes in the simulation (e.g., updating an email using follow up information) Time/temporal Reasoning : Time-sensitive actions (e.g., cab orders after 3-minute delays)\n\n: Time-sensitive actions (e.g., cab orders after 3-minute delays) Agent-to-Agent Collaboration : Communication between agents without direct API access\n\n: Communication between agents without direct API access Noise Tolerance: Robustness to API failures and environmental instability\n\nIn the spirit of GAIA, scenarios do not require specialized knowledge: humans should in principle be able to get 100%, which allows easy debugging for model developers.\n\nWant to explore the benchmark? Check out our dataset, which you can better display in our demo here.\n\nHow does Gaia2 run?\n\nGaia2 runs with ARE, an execution environment, where an agent of your choice has access to a combination of applications and associated pre-populated data.\n\nFor Gaia2, we created a smartphone mock-up environment, simulating what a human would use in their daily life. It contains real-world applications such as messaging (Email), utilities (Calendar, Contacts, Shopping, a FileSystem, \u2026), and a chat interface to talk to the agent. All applications are also accessible to the agents through tool calling. Last but not least, the demo also contains a simulated persona\u2019s history of conversations and app interactions.\n\nAll agent interactions are automatically recorded as structured traces during execution for deep dives and analysis: they include tool calls, API responses, model thoughts, timing metrics (e.g., response latency), user interactions, and so forth - and can all be exported as JSON.\n\nResults\n\nFor reference, we compare a range of large open and closed source models: Llama 3.3-70B Instruct, Llama-4-Maverick, GPT-4o, Qwen3-235B-MoE, Grok-4, Kimi K2, Gemini 2.5 Pro, Claude 4 Sonnet, and GPT-5 in all reasoning modes.\n\nAll models are evaluated using the same setup (a uniform ReAct loop for consistency, temperature of 0.5, generation limit of 16K tokens), with a combination of model-as-a-judge (Llama 3.3 Instruct 70B) and exact-match evaluation depending on the particular task. All 101 tools (and the general environment description) are provided in the system prompt.\n\nAmong the evaluated models, the highest-scoring model overall as of September 2025 is GPT-5 with high reasoning, and the best open source model is Kimi K2.\n\nSome capabilities appear to be already close to solved by the best models: execution of simple tool calls and instruction following ( execution ), and overall search (as we could have guessed from current results on GAIA). The ambiguity, adaptability, and noise splits remain challenging for now for all models, and it\u2019s interesting to see that performance on what were considered complex agentic tasks (instruction following and search) is not a good proxy for performance on closer-to-real-world tasks. Last but not least, the hardest split for all models at the moment is the time one: it\u2019s very hard at this moment for models to correctly handle time-sensitive actions (though this could likely be mitigated by the use of specialised tools and better temporal reasoning). Detailed analysis of these results can be found in the paper.\n\nHowever, we believe it\u2019s important to push reporting beyond raw scores: if the model is correct but took several thousand tokens to reach the correct solution, or ran for several hours, it is \u201cnot as good\u201d as a model which succeeded orders of magnitude faster. We therefore also normalize scores for cost, quantified as the average number of LLM calls and output tokens (which both define a cost-performance Pareto frontier). In the paper you\u2019ll find score vs monetary cost and time.\n\nCompare with your favorite models! Evaluating on Gaia2\n\nIf you want to evaluate your model on Gaia2, you can follow these steps:\n\nFirst, install Meta's Agent Research Environment in your Python environment of choice (uv, conda, virtualenv, ...)\n\npip install meta-agents-research-environments\n\nThen, run the benchmark for all configurations: execution, search, adaptability, time and ambiguity. Don't forget to upload all results to the hub with the hf_upload kwarg!\n\nare-benchmark run --hf meta-agents-research-environments/Gaia2 -- split validation --config CONFIGURATION --model YOUR_MODEL --model_provider YOUR_PROVIDER --agent default --max_concurrent_scenarios 2 --scenario_timeout 300 --output_dir ./monitored_test_results --hf_upload YOUR_HUB_DATASET_TO_SAVE_RESULTS\n\nRun the oracle to get your aggregated score file\n\nare-benchmark judge --hf meta-agents-research-environments/Gaia2 -- split validation --config CONFIGURATION --agent default --max_concurrent_scenarios 2 --scenario_timeout 300 --output_dir ./monitored_test_results --hf_upload YOUR_HUB_DATASET_TO_SAVE_RESULTS\n\nFinally, add all the relevant information about your model in the README, and share it on the leaderboard to centralize Gaia2 traces here!\n\nBeyond Gaia2: study your agents with ARE\n\nBeyond benchmark scenarios, you can use Gaia2 apps and content in ARE to see if the model is able to correctly solve less verifiable tasks such as loading emails, writing follow-ups, adding events to the calendar or booking meetings - in sum, providing the perfect setup to evaluate your AI assistants through interaction!\n\nYou can also easily customise the environment, by 1) connecting your tools (via MCP or directly ) to test your agents on it; 2) implementing your own scenarios, including defining trigger or timed events (eg: after 2 minutes, the Mail app will receive a new email from Contact), to see how the agent is able to adapt to an evolving environment\n\n(As the agents are by default json agents , they can\u2019t mess up your machine, unless of course you connect them to external apps with unsafe rights. So, operate with caution when adding your own apps or using untrusted MCPs)\n\nHere are several use cases that we\u2019ve used ARE for:\n\nVibe-check any agent on real or simulated data, to study a variety of setups, with their own rules, tools, content, and verifications\n\non real or simulated data, to study a variety of setups, with their own rules, tools, content, and verifications Test agent tool calling and orchestration capabilities , either with local apps or MCP tools\n\n, either with local apps or MCP tools Generate your own tool-calling trace to fine-tune tool calling models\n\nEasily gather and reproduce existing agentic benchmarks in a unified framework\n\nin a unified framework Debug and study agent to agent interactions on the fly within the user interface\n\nwithin the user interface Study model limitations in noisy environments (with API timeouts and ambiguity)\n\nWe recorded 3 videos so you can check some of these use cases (but of course, we hope the community gets creative with ARE :hugging_face:). For these videos, we use the default demo described above, which contains the simulated life of Linda Renne, PhD student in machine learning.\n\n1) Testing an agent on a simple task: event organisation\n\nTo test how good the default model is at event organisation, let\u2019s plan a birthday party!\n\nWe first ask the agent to text everyone in the Renne family about the user\u2019s 30th birthday party on November 7. The default universe has 21 contacts in the list, including 5 Renne family members - Linda, the simulation \u201cowner\u201d, George and Stephie, her parents, Anna her sister, and Morgan her grandfather. The agent successfully goes through the contact list, finds the four family members, and texts them.\n\nNext, we ask the agent to create a calendar invite and add them as invitees. The agent remembers the above context! It creates a calendar invite on the correct date and correctly adds the family members to it.\n\nYour browser does not support the video tag.\n\n2) Understanding agents: deep diving the traces\n\nARE also allows us to check the traces behind the actions taken by the agent. Upon opening the Agent logs tool on the left, we can see the system prompt, the chain of thought, multi-step actions taken with the tools called, and the outcomes as neatly organised logs. Everything can be exported as json if you want to consult things offline!\n\nYour browser does not support the video tag.\n\n3) Playing around and extending the demo: Connecting the agent to your own MCPs\n\nIn this last example, we connect ARE to a remote robot arm via MCP, so it can gesture things to us, then ask the agent to answer our yes or no questions by waving the robot arm! Here\u2019s what it looks like.\n\nYour browser does not support the video tag.\n\nBut these examples are only very simple starting points, and we\u2019re really looking towards what you\u2019ll build! (For more advanced users, you can even directly install and edit the Meta-ARE code here.)\n\nConclusion\n\nGaia2 and ARE are new research tools that we hope will empower anyone to easily build more reliable and adaptable AI agents - by allowing easy experiments, making real-world evaluation accessible to anyone, as well as improving trust through transparent, reproducible benchmarks and debuggable traces.\n\nWe\u2019d love to see what you will do with this project!",
    "link": "https://huggingface.co/blog/gaia2",
    "Summary": "Gaia2 and ARE: Empowering the Community to Evaluate AgentsPublished September 22, 2025 Update on GitHubIn an ideal world, AI agents would be reliable assistants.\nGaia2 is released with the open Meta Agents Research Environments (ARE) framework to run, debug and evaluate agents.\nARE simulates complex real world-like conditions and can be customized to further study agents behaviors.\nHere comes Gaia2, a follow up to GAIA, going way beyond it in terms of capabilities studied!\nWhere GAIA was read-only, Gaia2 is now a read-and-write benchmark, focusing on interactive behavior and complexity management.",
    "Keywords": [
      "agents",
      "models",
      "environment",
      "simulated",
      "empowering",
      "tool",
      "gaia2",
      "study",
      "tools",
      "community",
      "model",
      "agent"
    ]
  },
  {
    "Title": "Democratizing AI Safety with RiskRubric.ai",
    "Authors": [],
    "Publish Date": null,
    "Text": "Democratizing AI Safety with RiskRubric.ai\n\nPublished September 18, 2025 Update on GitHub\n\nBuilding trust in the open model ecosystem through standardized risk assessment\n\nMore than 500,000 models can be found on the Hugging Face hub, but it\u2019s not always clear to users how to choose the best model for them, notably on the security aspects. Developers might find a model that perfectly fits their use case, but have no systematic way to evaluate its security posture, privacy implications, or potential failure modes.\n\nAs models become more powerful and adoption accelerates, we need equally rapid progress in AI safety and security reporting. We're therefore excited to announce RiskRubric.ai, a novel initiative led by Cloud Security Alliance and Noma Security, with contributions by Haize Labs and Harmonic Security, for standardized and transparent risk assessment in the AI model ecosystem.\n\nRisk Rubric, a new Standardized Assessment of Risk for models\n\nRiskRubric.ai provides consistent, comparable risk scores across the entire model landscape, by evaluating models across six pillars: transparency, reliability, security, privacy, safety, and reputation.\n\nThe platform's approach aligns perfectly with open-source values: rigorous, transparent, and reproducible. Using Noma Security capabilities to automate the effort, each model undergoes:\n\n1,000+ reliability tests checking consistency and edge case handling\n\nchecking consistency and edge case handling 200+ adversarial security probes for jailbreaks and prompt injections\n\nfor jailbreaks and prompt injections Automated code scanning of model components\n\nof model components Comprehensive documentation review of training data and methods\n\nof training data and methods Privacy assessment including data retention and leakage testing\n\nincluding data retention and leakage testing Safety evaluation through structured harmful content tests\n\nThese assessments produce 0-100 scores for each risk pillar, rolling up to clear A-F letter grades. Each evaluation also includes specific vulnerabilities found, recommended mitigations, and suggestions for improvements.\n\nRiskRubric also comes with filters to help developers and organizations make deployment decisions based on what\u2019s important for them. Need a model with strong privacy guarantees for healthcare applications? Filter by privacy scores. Building a customer-facing application requiring consistent outputs? Prioritize reliability ratings.\n\nWhat we found (as of September 2025)\n\nEvaluating both open and closed models with the exact same standards highlighted some interesting results: many open models actually outperform their closed counterparts in specific risk dimensions (particularly transparency, where open development practices shine).\n\nLet\u2019s look at general trends:\n\nRisk distribution is polarized \u2013 most models are strong, but mid-tier scores show elevated exposure\n\nThe total risk scores range from 47 to 94, with a median of 81 (on a 100 points). Most models cluster in the \u201csafer\u201d range (54% are A or B level), but a long tail of underperformers drags the average down. That split shows a polarization: models tend to be either well-protected or in the middle-score range, with fewer in between.\n\nThe models concentrated in the 50\u201367 band (C/D range) are not outright broken, but they do provide only medium to low overall protection. This band represents the most practical area of concern, where security gaps are material enough to warrant prioritization.\n\nWhat this means: Don\u2019t assume the \u201caverage\u201d model is safe. The tail of weak performers is real \u2013 and that\u2019s where attackers will focus. Teams can use composite scores to set a minimum threshold (e.g. 75) for procurement or deployment, ensuring outliers don\u2019t slip into production.\n\nSafety risk is the \u201cswing factor\u201d \u2013 but it tracks closely with security posture\n\nThe Safety & Societal pillar (e.g. harmful output prevention) shows the widest variation across models. Importantly, models that invest in security hardening (prompt injection defenses, policy enforcement) almost always score better on safety as well.\n\nWhat this means: Strengthening core security controls goes beyond preventing jailbreaks, but also directly reduces downstream harms! Safety seems like it is a byproduct of robust security posture.\n\nGuardrails can erode transparency \u2013 unless you design for it\n\nStricter protections often make models less transparent to end users (e.g. refusals without explanations, hidden boundaries). This can create a trust gap: users may perceive the system as \u201copaque\u201d even while it\u2019s secure.\n\nWhat this means: Security shouldn\u2019t come at the cost of trust. To balance both, pair strong safeguards with explanatory refusals, provenance signals, and auditability. This preserves transparency without loosening defenses.\n\nAn updating results sheet can be accessed here\n\nConclusion\n\nWhen risk assessments are public and standardized, the entire community can work together to improve model safety. Developers can see exactly where their models need strengthening, and the community can contribute fixes, patches, and safer fine-tuned variants. This creates a virtuous cycle of transparent improvement that's impossible with closed systems. It also helps the community at large understand what works and does not, safety wise, by studying best models.\n\nIf you want to take part in this initiative, you can submit your model for evaluation (or suggest existing models!) to understand their risk profile!\n\nWe also welcome all feedback on the assessment methodology and scoring framework",
    "link": "https://huggingface.co/blog/riskrubric",
    "Summary": "As models become more powerful and adoption accelerates, we need equally rapid progress in AI safety and security reporting.\nWe're therefore excited to announce RiskRubric.ai, a novel initiative led by Cloud Security Alliance and Noma Security, with contributions by Haize Labs and Harmonic Security, for standardized and transparent risk assessment in the AI model ecosystem.\nFilter by privacy scores.\nSafety risk is the \u201cswing factor\u201d \u2013 but it tracks closely with security postureThe Safety & Societal pillar (e.g.\nAn updating results sheet can be accessed hereConclusionWhen risk assessments are public and standardized, the entire community can work together to improve model safety.",
    "Keywords": [
      "models",
      "risk",
      "scores",
      "riskrubricai",
      "transparency",
      "standardized",
      "democratizing",
      "ai",
      "security",
      "privacy",
      "transparent",
      "model",
      "safety"
    ]
  },
  {
    "Title": "Visible Watermarking with Gradio",
    "Authors": [],
    "Publish Date": null,
    "Text": "Visible Watermarking with Gradio\n\nPublished September 15, 2025 Update on GitHub\n\nLast year, we shared a blogpost on watermarking, explaining what it means to watermark generative AI content, and why it's important. The need for watermarking has become even more critical as people all over the world have begun to generate and share AI-generated images, video, audio, and text. Images and video have become so realistic that they\u2019re nearly impossible to distinguish from what you\u2019d see captured by a real camera. Addressing this issue is multi-faceted, but there is one, clear, low-hanging fruit \ud83c\udf47:\n\nIn order for people to know what's real and what's synthetic, use visible watermarks.\n\nTo help out, we at Hugging Face have made visible watermarking trivially easy: Whenever you create a Space like an app or a demo, you can use our in-house app-building library Gradio to display watermarks with a single command.\n\nFor images and video, simply add the watermark parameter, like so:\n\ngr.Image(my_generated_image, watermark=my_watermark_image)\n\ngr.Video(my_generated_video, watermark=my_watermark_image)\n\nWatermarks can be specified as filenames, and for images we additionally support open images or even numpy arrays, to work best with how you want to set up your interface. One option I particularly like is QR watermarks, which can be used to get much more information about the content, and can even be matched to the style of your image or video.\n\nYou can also add custom visible watermarks for AI-generated text, so that whenever it is copied, the watermark will appear. Like so:\n\ngr.Chatbot(label=my_model_name, watermark=my_watermark_text, type=\"messages\", show_copy_button=True, show_copy_all_button=True)\n\nThis automatically adds attribution when users copy text from AI responses, further aiding in AI transparency and disclosure for text generation.\n\nTry it all out today, build your own watermark, have fun!\n\nHappy Coding!\n\nAcknowledgements: Abubakar Abid and Yuvraj Sharma collaborated on this work and blog post.",
    "link": "https://huggingface.co/blog/watermarking-with-gradio",
    "Summary": "Visible Watermarking with GradioPublished September 15, 2025 Update on GitHubLast year, we shared a blogpost on watermarking, explaining what it means to watermark generative AI content, and why it's important.\nThe need for watermarking has become even more critical as people all over the world have begun to generate and share AI-generated images, video, audio, and text.\nAddressing this issue is multi-faceted, but there is one, clear, low-hanging fruit \ud83c\udf47:In order for people to know what's real and what's synthetic, use visible watermarks.\nTo help out, we at Hugging Face have made visible watermarking trivially easy: Whenever you create a Space like an app or a demo, you can use our in-house app-building library Gradio to display watermarks with a single command.\nYou can also add custom visible watermarks for AI-generated text, so that whenever it is copied, the watermark will appear.",
    "Keywords": [
      "gradio",
      "images",
      "work",
      "watermarking",
      "visible",
      "video",
      "ai",
      "watermarks",
      "whats",
      "watermark",
      "text"
    ]
  },
  {
    "Title": "LeRobotDataset:v3.0: Bringing large-scale datasets to lerobot",
    "Authors": [],
    "Publish Date": null,
    "Text": "LeRobotDataset:v3.0: Bringing large-scale datasets to lerobot\n\nPublished September 16, 2025 Update on GitHub\n\nLeRobotDataset:v3\n\nLeRobotDataset:v2\n\nLeRobotDataset:v3\n\nTable of Contents\n\nToday we release! In our previousrelease, we stored one episode per file, hitting file-system limitations when scaling datasets to millions of episodes.packs multiple episodes in a single file, using relational metadata to retrieve information at the individual episode level from multi-episode files. The new format also natively supports accessing datasets in streaming mode, allowing to process large datasets on the fly.We provide a one-liner util to convert all datasets in the LeRobotDataset format to the new format, and are very excited to share this milestone with the community ahead of our next stable release!\n\nLeRobotDataset is a standardized dataset format designed to address the specific needs of robot learning, and it provides unified and convenient access to robotics data across modalities, including sensorimotor readings, multiple camera feeds and teleoperation status. Our dataset format also stores general information regarding the way the data is being collected (metadata), including a textual description of the task being performed, the kind of robot used and measurement details like the frames per second at which both image and robot state streams are sampled. Metadata are useful to index and search across robotics datasets on the Hugging Face Hub!\n\nWithin lerobot , the robotics library we are developing at Hugging Face, LeRobotDataset provides a unified interface for working with multi-modal, time-series data, and it seamlessly integrates both with the Hugging Face and Pytorch ecosystems. The dataset format is designed to be easily extensible and customizable, and already supports openly available datasets from a wide range of embodiments\u2014including manipulator platforms such as the SO-100 arms and ALOHA-2 setup, real-world humanoid data, simulation datasets, and even self-driving car data! You can explore the current datasets contributed by the community using the dataset visualizer! \ud83d\udd17\n\nBesides scale, this new release of LeRobotDataset also enables support for a streaming functionality, allowing to process batches of data from large datasets on the fly, without having to download prohibitively large collections of data onto disk. You can access and use any dataset in v3.0 in streaming mode by using the dedicated StreamingLeRobotDataset interface! Streaming datasets is a key milestone towards more accessible robot learning, and we are excited about sharing it with the community \ud83e\udd17\n\nFrom episode-based to file-based datasets We directly enable dataset streaming from the Hugging Face Hub for on-the-fly processing.\n\nInstall lerobot , and record a dataset\n\nlerobot is the end-to-end robotics library developed at Hugging Face, supporting real-world robotics as well as state of the art robot learning algorithms. The library allows to record datasets locally directly on real-world robots, and to store datasets on the Hugging Face Hub. You can read more about the robots we currently support here!\n\nLeRobotDataset:v3 is going to be a part of the lerobot library starting from lerobot-v0.4.0 , and we are very excited about sharing it early with the community. You can install the latest lerobot-v0.3.x supporting this new dataset format directly from PyPI using:\n\npip install \"https://github.com/huggingface/lerobot/archive/33cad37054c2b594ceba57463e8f11ee374fa93c.zip\"\n\nFollow the community's progress towards a stable release of the library here \ud83e\udd17\n\nOnce you have installed a version of lerobot which supports the new dataset format, you can record a dataset with our signature robot arm, the SO-101, by using teleoperation alongside the following instructions:\n\nlerobot-record \\ --robot.type=so101_follower \\ --robot.port=/dev/tty.usbmodem585A0076841 \\ --robot.id=my_awesome_follower_arm \\ --robot.cameras= \"{ front: {type: opencv, index_or_path: 0, width: 1920, height: 1080, fps: 30}}\" \\ --teleop.type=so101_leader \\ --teleop.port=/dev/tty.usbmodem58760431551 \\ --teleop.id=my_awesome_leader_arm \\ --display_data= true \\ --dataset.repo_id= ${HF_USER} /record-test \\ --dataset.num_episodes=5 \\ --dataset.single_task= \"Grab the black cube\"\n\nHead to the official documentation to see how to record a dataset for your use case.\n\nThe (New) Format Design\n\nA core design choice behind LeRobotDataset is separating the underlying data storage from the user-facing API. This allows for efficient serialization and storage while presenting the data in an intuitive, ready-to-use format. Datasets are organized into three main components:\n\nTabular Data: Low-dimensional, high-frequency data such as joint states, and actions are stored in efficient Apache Parquet files, and typically offloaded to the more mature datasets library, providing fast, memory-mapped access or streaming-based access. Visual Data: To handle large volumes of camera data, frames are concatenated and encoded into MP4 files. Frames from the same episode are always grouped together into the same video, and multiple videos are grouped together by camera. To reduce stress on the file system, groups of videos for the same camera view are also broken into multiple subdirectories. Metadata: A collection of JSON files which describes the dataset's structure in terms of its metadata, serving as the relational counterpart to both the tabular and visual dimensions of data. Metadata includes the different feature schemas, frame rates, normalization statistics, and episode boundaries.\n\nTo support datasets with potentially millions of episodes (resulting in hundreds of millions/billions of individual frames), we merge data from different episodes into the same high-level structure. Concretely, this means that any given tabular collection and video will not contain information about one episode only, but a concatenation of the information available in multiple episodes. This keeps the pressure on the file system manageable, both locally and on remote storage providers like Hugging Face. We can then leverage metadata to gather episode-specific information, e.g. the timestamp a given episode starts or ends in a certain video.\n\nDatasets are organized as repositories containing:\n\nmeta/info.json : This is the central metadata file. It contains the complete dataset schema, defining all features (e.g., observation.state , action ), their shapes, and data types. It also stores crucial information like the dataset's frames-per-second ( fps ), codebase version, and the path templates used to locate data and video files.\n\n: This is the central metadata file. It contains the complete dataset schema, defining all features (e.g., , ), their shapes, and data types. It also stores crucial information like the dataset's frames-per-second ( ), codebase version, and the path templates used to locate data and video files. meta/stats.json : This file stores aggregated statistics (mean, std, min, max) for each feature across the entire dataset. These are used for data normalization and are accessible via dataset.meta.stats .\n\n: This file stores aggregated statistics (mean, std, min, max) for each feature across the entire dataset. These are used for data normalization and are accessible via . meta/tasks.jsonl : Contains the mapping from natural language task descriptions to integer task indices, which are used for task-conditioned policy training.\n\n: Contains the mapping from natural language task descriptions to integer task indices, which are used for task-conditioned policy training. meta/episodes/ : This directory contains metadata about each individual episode, such as its length, corresponding task, and pointers to where its data is stored. For scalability, this information is stored in chunked Parquet files rather than a single large JSON file.\n\n: This directory contains metadata about each individual episode, such as its length, corresponding task, and pointers to where its data is stored. For scalability, this information is stored in chunked Parquet files rather than a single large JSON file. data/ : Contains the core frame-by-frame tabular data in Parquet files. To improve performance and handle large datasets, data from multiple episodes are concatenated into larger files . These files are organized into chunked subdirectories to keep file sizes manageable. Therefore, a single file typically contains data for more than one episode.\n\n: Contains the core frame-by-frame tabular data in Parquet files. To improve performance and handle large datasets, data from . These files are organized into chunked subdirectories to keep file sizes manageable. Therefore, a single file typically contains data for more than one episode. videos/ : Contains the MP4 video files for all visual observation streams. Similar to the data/ directory, video footage from multiple episodes is concatenated into single MP4 files. This strategy significantly reduces the number of files in the dataset, which is more efficient for modern file systems. The path structure ( /videos/<camera_key>/<chunk>/file_...mp4 ) allows the data loader to locate the correct video file and then seek to the precise timestamp for a given frame.\n\nMigrate your v2.1 dataset to v3.0\n\nLeRobotDataset:v3.0 will be released with lerobot-v0.4.0 , together with the possibility to easily convert any dataset currently hosted on the Hugging Face Hub to the new v3.0 using:\n\npython -m lerobot.datasets.v30.convert_dataset_v21_to_v30--repo-id=<HFUSER/DATASET_ID>\n\nWe are very excited about sharing this new format early with the community! While we develop lerobot-v0.4.0 , you can still convert your dataset to the newly updated version by using the latest lerobot-v0.3.x supporting this new dataset format directly from PyPI using:\n\npip install \"https://github.com/huggingface/lerobot/archive/33cad37054c2b594ceba57463e8f11ee374fa93c.zip\" python -m lerobot.datasets.v30.convert_dataset_v21_to_v30 --repo-id=<HFUSER/DATASET_ID>\n\nNote that this is a pre-release, and generally unstable version. You can follow the status of the development of our next stable release here!\n\nThe conversion script convert_dataset_v21_to_v30.py aggregates the multiple episodes episode-0000.mp4, episode-0001.mp4, episode-0002.mp4, ... / episode-0000.parquet, episode-0001.parquet, episode-0002.parquet, episode-0003.parquet, ... into single files file-0000.mp4 / file-0000.parquet , and updates the metadata accordingly, to be able to retrieve episode-specific information from higher-level files.\n\nCode Example: Using LeRobotDataset with torch.utils.data.DataLoader\n\nEvery dataset on the Hugging Face Hub containing the three main pillars presented above (Tabular and Visual Data, as well as relational Metadata), and can be accessed with a single line.\n\nMost robot learning algorithms, based on reinforcement learning (RL) or behavioral cloning (BC), tend to operate on a stack of observations and actions. For instance, RL algorithms typically use a history of previous observations o_{t-H_o:t} , and BC algorithms are instead typically trained to regress chunks of multiple actions. To accommodate for the specifics of robot learning training, LeRobotDataset provides a native windowing operation, whereby we can use the seconds before and after any given observation using a delta_timestamps argument.\n\nConveniently, by using LeRobotDataset with a PyTorch DataLoader one can automatically collate the individual sample dictionaries from the dataset into a single dictionary of batched tensors.\n\nfrom lerobot.datasets.lerobot_dataset import LeRobotDataset repo_id = \"yaak-ai/L2D-v3\" dataset = LeRobotDataset(repo_id) sample = dataset[ 100 ] print (sample) delta_timestamps = { \"observation.images.front_left\" : [- 0.2 , - 0.1 , 0.0 ] } dataset = LeRobotDataset( repo_id delta_timestamps=delta_timestamps ) sample = dataset[ 100 ] print (sample[ 'observation.images.front_left' ].shape) batch_size= 16 data_loader = torch.utils.data.DataLoader( dataset, batch_size=batch_size ) num_epochs = 1 device = \"cuda\" if torch.cuda.is_available() else \"cpu\" for epoch in range (num_epochs): for batch in data_loader: observations = batch[ 'observation.state.vehicle' ].to(device) actions = batch[ 'action.continuous' ].to(device) images = batch[ 'observation.images.front_left' ].to(device) ...\n\nStreaming\n\nYou can also use any dataset in v3.0 format in streaming mode, without the need to download it locally, by using the StreamingLeRobotDataset class.\n\nfrom lerobot.datasets.streaming_dataset import StreamingLeRobotDataset repo_id = \"yaak-ai/L2D-v3\" dataset = StreamingLeRobotDataset(repo_id)\n\nConclusion\n\nLeRobotDataset v3.0 is a stepping stone towards scaling up robotics datasets supported in LeRobot. By providing a format to store and access large collections of robot data we are making progress towards democratizing robotics, allowing the community to train on possibly millions of episodes without even downloading the data itself!\n\nYou can try the new dataset format by installing the latest lerobot-v0.3.x , and share any feedback on GitHub or on our Discord server! \ud83e\udd17\n\nAcknowledgements\n\nWe thank the fantastic yaak.ai team for their precious support and feedback while developing LeRobotDataset:v3. Go ahead and follow their organization on the Hugging Face Hub! We are always looking to collaborate with the community and share early features. Reach out if you would like to collaborate \ud83d\ude0a",
    "link": "https://huggingface.co/blog/lerobot-datasets-v3",
    "Summary": "LeRobotDataset:v3.0: Bringing large-scale datasets to lerobotPublished September 16, 2025 Update on GitHubLeRobotDataset:v3LeRobotDataset:v2LeRobotDataset:v3Table of ContentsToday we release!\nThe library allows to record datasets locally directly on real-world robots, and to store datasets on the Hugging Face Hub.\nTo improve performance and handle large datasets, data from multiple episodes are concatenated into larger files .\nTo improve performance and handle large datasets, data from .\nYou can try the new dataset format by installing the latest lerobot-v0.3.x , and share any feedback on GitHub or on our Discord server!",
    "Keywords": [
      "data",
      "information",
      "files",
      "bringing",
      "lerobot",
      "lerobotdatasetv30",
      "file",
      "dataset",
      "format",
      "contains",
      "hugging",
      "datasets",
      "metadata",
      "largescale"
    ]
  },
  {
    "Title": "Public AI on Hugging Face Inference Providers \ud83d\udd25",
    "Authors": [],
    "Publish Date": null,
    "Text": "Public AI on Hugging Face Inference Providers \ud83d\udd25\n\nPublished September 17, 2025 Update on GitHub\n\nWe're thrilled to share that Public AI is now a supported Inference Provider on the Hugging Face Hub! Public AI joins our growing ecosystem, enhancing the breadth and capabilities of serverless inference directly on the Hub\u2019s model pages. Inference Providers are also seamlessly integrated into our client SDKs (for both JS and Python), making it super easy to use a wide variety of models with your preferred providers.\n\nThis launch makes it easier than ever to access public and sovereign models from institutions like the Swiss AI Initiative and AI Singapore \u2014 right from Hugging Face. You can browse Public AI\u2019s org on the Hub at https://huggingface.co/publicai and try trending supported models at https://huggingface.co/models?inference_provider=publicai&sort=trending.\n\nThe Public AI Inference Utility is a nonprofit, open-source project. The team builds products and organizes advocacy to support the work of public AI model builders like the Swiss AI Initiative and AI Singapore, among others.\n\nThe Public AI Inference Utility runs on a distributed infrastructure that combines a vLLM-powered backend with a deployment layer designed for resilience across multiple partners. Behind the scenes, inference is handled by servers exposing OpenAI-compatible APIs on vLLM, deployed across clusters donated by national and industry partners. A global load-balancing layer ensures requests are routed efficiently and transparently, regardless of which country\u2019s compute is serving the query.\n\nFree public access is supported by donated GPU time and advertising subsidies, while long-term stability is intended to be anchored by state and institutional contributions. You can learn more about Public AI\u2019s platform and infrastructure at https://platform.publicai.co/.\n\nYou can now use the Public AI Inference Utility as an Inference Provider on Hugging Face. We're excited to see what you'll build with this new provider.\n\nRead more about how to use Public AI as an Inference Provider in its dedicated documentation page.\n\nSee the list of supported models here.\n\nHow it works\n\nIn the website UI\n\nIn your user account settings, you are able to:\n\nSet your own API keys for the providers you\u2019ve signed up with. If no custom key is set, your requests will be routed through HF.\n\nOrder providers by preference. This applies to the widget and code snippets in the model pages.\n\nAs mentioned, there are two modes when calling Inference Providers:\n\nCustom key (calls go directly to the inference provider, using your own API key of the corresponding inference provider)\n\nRouted by HF (in that case, you don't need a token from the provider, and the charges are applied directly to your HF account rather than the provider's account)\n\nModel pages showcase third-party inference providers (the ones that are compatible with the current model, sorted by user preference)\n\nFrom the client SDKs\n\nfrom Python, using huggingface_hub\n\nThe following example shows how to use Swiss AI's Apertus-70B using Public AI as the inference provider. You can use a Hugging Face token for automatic routing through Hugging Face, or your own Public AI API key if you have one.\n\nNote: this requires using a recent version of huggingface_hub (>= 0.34.6).\n\nimport os from huggingface_hub import InferenceClient client = InferenceClient( provider= \"publicai\" , api_key=os.environ[ \"HF_TOKEN\" ], ) messages = [ { \"role\" : \"user\" , \"content\" : \"What is the capital of France?\" } ] completion = client.chat.completions.create( model= \"swiss-ai/Apertus-70B-Instruct-2509\" , messages=messages, ) print (completion.choices[ 0 ].message)\n\nfrom JS using @huggingface/inference\n\nimport { InferenceClient } from \"@huggingface/inference\" ; const client = new InferenceClient (process. env . HF_TOKEN ); const chatCompletion = await client. chatCompletion ({ model : \"swiss-ai/Apertus-70B-Instruct-2509\" , messages : [ { role : \"user\" , content : \"What is the capital of France?\" , }, ], provider : \"publicai\" , }); console . log (chatCompletion. choices [ 0 ]. message );\n\nBilling\n\nAt the time of writing, usage of the Public AI Inference Utility through Hugging Face Inference Providers is free of charge. Pricing and availability may change.\n\nHere is how billing works for other providers on the platform:\n\nFor direct requests, i.e. when you use the key from an inference provider, you are billed by the corresponding provider. For instance, if you use a Public AI API key you're billed on your Public AI account.\n\nFor routed requests, i.e. when you authenticate via the Hugging Face Hub, you'll only pay the standard provider API rates. There's no additional markup from us; we just pass through the provider costs directly. (In the future, we may establish revenue-sharing agreements with our provider partners.)\n\nImportant Note \u203c\ufe0f PRO users get $2 worth of Inference credits every month. You can use them across providers. \ud83d\udd25\n\nSubscribe to the Hugging Face PRO plan to get access to Inference credits, ZeroGPU, Spaces Dev Mode, 20x higher limits, and more.\n\nWe also provide free inference with a small quota for our signed-in free users, but please upgrade to PRO if you can!\n\nFeedback and next steps\n\nWe would love to get your feedback! Share your thoughts and/or comments here: https://huggingface.co/spaces/huggingface/HuggingDiscussions/discussions/49",
    "link": "https://huggingface.co/blog/inference-providers-publicai",
    "Summary": "Public AI on Hugging Face Inference Providers \ud83d\udd25Published September 17, 2025 Update on GitHubWe're thrilled to share that Public AI is now a supported Inference Provider on the Hugging Face Hub!\nYou can now use the Public AI Inference Utility as an Inference Provider on Hugging Face.\nYou can use a Hugging Face token for automatic routing through Hugging Face, or your own Public AI API key if you have one.\nmessage );BillingAt the time of writing, usage of the Public AI Inference Utility through Hugging Face Inference Providers is free of charge.\nFor instance, if you use a Public AI API key you're billed on your Public AI account.",
    "Keywords": [
      "inference",
      "face",
      "hugging",
      "providers",
      "ai",
      "key",
      "provider",
      "using",
      "model",
      "public"
    ]
  },
  {
    "Title": "Jupyter Agents: training LLMs to reason with notebooks",
    "Authors": [],
    "Publish Date": null,
    "Text": "Jupyter Agents: training LLMs to reason with notebooks\n\nPublished September 10, 2025 Update on GitHub\n\nThe past year has been all about giving LLMs more tools and autonomy to solve more complex and open ended tasks. The goal of theis to give the model the ultimate tool: code execution.\n\nA natural way to display multi-step code execution together with reasoning is within a Jupyter Notebook, which consists of code and markdown cells. So we built Jupyter Agent to act as an agent that can execute code directly inside a Jupyter notebook and use this environment to solve data analysis and data science tasks. Think of it like Cursor, but living natively inside your data science workflow.\n\nWe built a demo of this vision with Qwen-3 Coder, currently one of the strongest coding models. This is a follow-up to our earlier work on jupyter-agent (v1).\n\nWhile large models are starting to show useful behavior, the key question is how we can continue improving them. To this end, we focus on strengthening smaller models to perform well on agentic data science tasks as they currently struggle to compete with the large models.\n\nThe goal of this project is to build a pipeline to first generate high-quality training data, then fine-tune an existing small model, and finally evaluate whether the model's performance improves on relevant benchmarks.\n\nLet\u2019s begin with the last step: selecting a strong benchmark for evaluating models on data science tasks.\n\n\ud83c\udfc1 Primer: the DABStep Benchmark\n\nIn order to understand if we are making progress towards better data science agents we need a benchmark to measure such capabilities. Last year, in partnership with Adyen, we introduced the DABStep benchmark: a way to evaluate data science agents on realistic tasks. The setup is simple: provide the LLM with datasets and ask it to answer non-trivial data questions.\n\nExample tasks:\n\nQuestion Answer Which card scheme had the highest average fraud rate in 2023? SwiftCharge For the year 2023, focusing on the merchant Crossfit Hanna, if we incentivize users to switch to a different Authorization Characteristics Indicator, which option would be the most cost-effective? E:346.49\n\nThis benchmark remains challenging for today\u2019s LLMs \u2014 e.g. the best out-of-the-box model is Claude 4 Sonnet which reaches not even 20% accuracy on the hard tasks.\n\nYou can explore the live leaderboard here.\n\n\ud83c\udfaf First Baseline\n\nNow that we identified a good benchmark we can try to climb it! We set out to build a dataset for fine-tuning such that even a small data agent model could perform well on DABStep.\n\nOur first choice was Qwen3-4B-Thinking-2507: extremely small (fast to iterate with, easy to run), yet strong enough to act in agentic scenarios.\n\nBaseline results:\n\nEasy tasks: 44.4%\n\nHard tasks: 2.1%\n\nNot great \u2014 but a promising starting point, since it left a lot of room for improvement. Let's see how we can improve it!\n\n\ud83d\udd27 Primer on Scaffolding\n\nA core aspect of agents that sets it apart from a pure chat model is the scaffolding built around the model to steer its behaviour. The evaluation script in DABStep for example uses smolagents to execute code. Smolagents comes with predefined behaviors, prompting structures, and expected formats.\n\nWe also studied the Qwen-Agent codebase, where the authors tailoring scaffolding to the model. This makes sense: Claude Code, for example, works shockingly well with Claude Sonnet because their scaffolding is aligned.\n\nSo, we restructured our scaffolding:\n\nStripped it down to ~200 lines of code.\n\nNo external dependencies.\n\nInspired by the spirit of tiny-agents.\n\n\ud83d\udc49 Check it out here: utils.py.\n\nResults: accuracy jumped from 44.4% \u2192 59.7% (easy split). \ud83d\ude80\n\nOur loop:\n\nWhile loop with two tools: code execution to run the code and final_answer to return the final answer.\n\nWe differ from Qwen-Agent by explicitly adding a final_answer tool \u2014 which in our testing has improved performance.\n\ntool \u2014 which in our testing has improved performance. Compared to smolagents, we simplified the scaffolding by removing a lot of prompts and tools. Smolagents also hardcodes a lot of assumptions into the model by using the ReACT framework.\n\n\ud83c\udfc3\u200d\u2642\ufe0f Training Pipeline\n\nWith simplified scaffolding in place, we focused on fine-tuning Qwen3-4B for data science agentic tasks.\n\n\u2699\ufe0f Dataset Pipeline\n\nThe recipe to improve a model on a certain task or behaviour is to train it on data that reflects the tasks as closely as possible. A natural starting point is to look at real Jupyter Notebooks and find notebooks that align closely with the task that we plan to tackle, namely data analysis.\n\nKaggle notebooks offer a wealth of high quality data analysis notebooks and are made available by Kaggle:\n\nDatasets:\n\nKaggle Notebooks dataset : ~2TB of notebooks.\n\n: ~2TB of notebooks. Kaggle Datasets : 5TB of kaggle datasets that we manually downloaded and linked to the notebooks.\n\n: 5TB of kaggle datasets that we manually downloaded and linked to the notebooks. Rich metadata for each notebook (authors, datasets used, etc.).\n\nNow that we have good results with a base model it's time to build a dataset that will help us improve it even further. We designed a multi-stage pipeline using Datatrove to clean and prepare Kaggle notebooks at scale.\n\nHere\u2019s how each step worked:\n\n1. Large-scale deduplication\n\nWe started with ~2TB of Kaggle notebooks and reduced it to ~250GB reusing our work from the BigCode project. As part of the StarCoder2 training data processing the notebooks (without output cells) were already deduplicated. Most Kaggle notebooks are small variations or near-identical copies, so this step was essential.\n\nKey insight: ~90% of raw notebooks are duplicates, which would have skewed training if left unfiltered.\n\n2. Downloading linked datasets\n\nMost Kaggle notebooks reference external datasets via Kaggle metadata. To make sure the code inside notebooks could actually run, we built a pipeline that automatically fetched these linked datasets. This step was crucial, since many notebooks would otherwise be incomplete or non-executable.\n\nUsing the kagglehub package, we downloaded thousands of datasets \u2014 about 5TB in total. To keep things manageable and relevant:\n\nWe filtered out datasets containing model checkpoints, large multimodal corpora, or LLM-related files.\n\nWe also excluded very large datasets (10GB+) that couldn\u2019t fit into the virtual E2B sandboxes we used for execution.\n\nBy the end, we had a rich collection of executable notebooks paired with their datasets, providing the foundation for training agents in realistic, runnable environments.\n\n3. Edu scoring\n\nWe scored notebooks based on educational quality using Qwen3-32B. We saw that using the whole notebook was not optimal, as many contained trivial or broken code. Our educational scoring approach is detailed in edu_scoring.py.\n\nTL;DR: We assigned each notebook a score from 1\u20135 based on clarity, completeness, and educational value, and kept only those above a chosen threshold. This filtering removed about 70% of the notebooks.\n\nThis is similar to the insight from the BeyondWeb paper, which showed that using high-quality data is better for synthetic data generation \u2014 a step we relied on for QA (Question-Answer) generation. This helped the model learn from \u201chigh quality\u201d notebooks instead of noisy ones.\n\n4. Filtering irrelevant notebooks\n\nWe excluded notebooks about training LLMs or unrelated to data analysis. We also removed notebooks that didn\u2019t actually use datasets through an automated LLM-based filtering process using Qwen3-32B. The implementation of filtering can be found in extract_packages_and_files.py .\n\nTL;DR: We prompted Qwen3-32B to identify and remove notebooks that either (1) had nothing to do with data analysis, or (2) didn\u2019t actually use datasets. This step removed about 20% of the notebooks.\n\nThis ensured we trained only on relevant data science tasks.\n\n5. QA generation\n\nUsing the cleaned notebooks, we generated question\u2013answer pairs using Qwen3-32B. The questions and answer are grounded in the real notebook traces so the QA pairs are based on real code execution results. Prompt design: we asked the LLM to produce natural questions that could realistically be asked of the dataset, then validated whether the notebook provided a correct answer.\n\nChallenge: We had to try many prompts to get higher-difficulty questions because LLMs tended to generate trivial ones like \"what is the size of the dataset\".\n\nInsight: We broke this into two steps because LLMs tended to hallucinate answers:\n\nGenerate the question and answer. Ask another LLM (with access to the notebook) to check whether the answer was correct.\n\nThe complete prompting strategy and implementation is available in generate_qa.py .\n\n6. Trace generation\n\nFinally we want to generate clean code execution traces since even the original notebooks after processing are often open ended and verbose with lots of irrelevant parts. However, we want our Jupyter Agent to get to the result efficiently. To generate cleaner notebook traces for training we generated traces synthetically based on the original notebooks.\n\nWe have prompted Qwen-3-Coder-480B model to generate a jupyter notebook code to answer the question from the previously generated synthetic QA pair. Traces captured step-by-step code execution, including intermediate outputs, which are crucial for agent training.\n\nWe used E2B for our agent to solve the synthetic QA pairs, which required fetching Kaggle datasets so the code could actually run via E2B.\n\nChallenge 1: Many datasets were unavailable.\n\nTrick: Since LLMs are strong at code and have a decent world model, we prompted them to act as a code interpreter when the dataset was missing.\n\nBeginning of the prompt:\n\nYou are a stateful Python code interpreter that executes code in a persistent environment. Your role is to execute Python code while maintaining state across multiple code cells, similar to a Jupyter notebook environment. [REST OF THE PROMPT]\n\nChallenge 2: Qwen3-Coder-480B-A35B model does not support thinking mode - how can we extract code commentary? By default it often outputs just a brief comment followed by several steps of code execution. However, we'd like some reasoning or comments between every cell. Trick: When switching from Qwen3-32B to Qwen3-Coder-480B-A35B we noticed that often output message content was empty. This turns out to be a previously known quirk of Qwen3-Coder models in which when using tool calling the model would not return an empty assistant response. We enforce some text commentary through tooling by passing 'comment' as a required field in the code execution tool call. This way when non-reasoning model is used for code cell generation it will by default output some description of its actions from 1st POV, emulating the thinking traces structure.\n\nNote: the generated final answer in the notebook may vary from the answer specified in the QA pair. This is caused by the fact that the agent model could use data preprocessing methods and steps different from the original Kaggle notebook and the synthetic question would not usually specify them. This discrepancy is normal and lays the foundation for a new exciting research direction of how language models tend to treat data analysis and whether they do it differently from humans. For full transparency we keep both LLM-generated final answer and original answer from the real Kaggle notebook as a signal of model's performance. We encourage the community to try different dataset mixes to see how they can push performance even further.\n\n7. Final curation\n\nWe truncated overly long outputs and filtered out trivial traces to prevent content length issues and keep only high-quality traces.\n\nWe kept non-trivial, multi-turn traces aligned with DABStep-style tasks.\n\nThe resulting Jupyter Agent Dataset became the foundation for SFT on Qwen3-4B models with 51k synthetic notebooks and almost 0.2B tokens.\n\nWith this dataset in hand, the natural next step is to see whether it actually helps our model become a stronger data science agent. Let\u2019s move on to the training pipeline and evaluate the impact!\n\n\ud83c\udfc3\u200d\u2642\ufe0f Training Pipeline\n\nWith the curated dataset ready, we turned to the key question: does this data actually help the model get better at solving data analysis tasks? To find out, we set up a simple fine-tuning pipeline and ran experiments to measure the impact of training on our synthetic notebooks.\n\nSome training steps turned out to be particularly interesting and gave us useful insights:\n\nFor trace generation, we used LLMs to generate QA pairs, which gave us a verifiable environment .\n\n. Finally, we fine-tuned Qwen3-4B with TRL. Used assistant_loss_only=True \u2192 small performance boost. Added neftune noise for full-parameter multi-epoch training \u2192 avoids overfitting.\n\nwith TRL.\n\nChallenges:\n\nPrompting models for tool calling is tricky: not all prompts deliver the same performance (Qwen docs).\n\nWe had to manually test each one to find what worked best.\n\nThere\u2019s no standardization in response formats for tool calling, making it difficult to switch between models.\n\nNative Qwen's generation prompt is not adapted to assistant_loss_only=True training mode in TRL which requires to have generation tokens by default. Thus, we adapt the original chat templates by wrapping the assistant response part in the generation tags.\n\ntraining mode in TRL which requires to have generation tokens by default. Thus, we adapt the original chat templates by wrapping the assistant response part in the generation tags. Training thinking models on short reasoning texts may disrupt model capabilities \u2192 full-parameter training works better comparing to PEFT in this case.\n\nOur complete training implementation, including hyperparameter configurations and template adaptations, is available in our finetuning directory in our repo.\n\n\ud83d\udcca Results\n\nFirst, we generated our final dataset using Qwen3-Coder-480B-A35B which contains high quality code and short reasoning-like traces. Afterwards, we started our training and we have experimented with various configurations like PEFT/adapters vs. full-parameter tuning, learning rate, number of epochs, adding noise and others. We found out, that full-parameter fine-tuning allows the model to learn and replicate the Qwen3-Coder-480B-A35B behavior response quality better with shorter supporting commentary fitting more to the data analysis task without unnecessary long reasoning.\n\nWe have done a small ablation study on the impact of no. training epochs:\n\nModel No. of epochs DABstep (Easy) Qwen-3-4B-Instruct-2507 (Base) 0 38.67% Qwen-3-4B-Instruct-2507 (Our Scaffolding) 0 52.78% Qwen-3-4B-Instruct-2507 2 63.89% Qwen-3-4B-Instruct-2507 3 73.61% Qwen-3-4B-Instruct-2507 5 75% Qwen-3-4B-Instruct-2507 7 70.83%\n\nWe observe that it is beneficial to have a bit more epochs than usual for SFT with lower learning rate and higher neftune noise (7). Finally, we compare our trained models with implemented scaffolding to define the pure impact of our training dataset. In summary, we can see up to 36%/22% boost on DABStep easy score compared with base/scaffolded model:\n\nWe can also see, that the hard score can increase too even though our dataset is focused on easier questions:\n\nFrom figures above one can notice a noticeable impact of both new scaffolding and tuning on our synthetic notebooks. This makes Qwen-4B (with our pipeline + scaffolding) a state-of-the-art small-model agent on DABStep.\n\nIn practice, the model can now solve a wide range of realistic Kaggle-style data analysis tasks with consistent execution.\n\nIt\u2019s not yet strong enough for the hardest queries, but we\u2019ve shown that even small models can become powerful agents when paired with the right data and scaffolding.\n\nTry Jupyter Agent Yourself\n\nThese results demonstrate that even small models can become powerful data science agents with the right training approach. Ready to try it yourself? We've made everything openly available so you can experiment with our fine-tuned models and dataset.\n\nWe openly release best-performing checkpoints of tuned Qwen3-4B-Instruct-2507 and Qwen3-4B-Thinking-2507 together with the training dataset, which you can try out and experiment with:\n\nYou can load Jupyter Agent Dataset in just a couple of lines using the following code:\n\nfrom datasets import load_dataset ds = load_dataset( \"jupyter-agent/jupyter-agent-dataset\" , split= \"non-thinking\" ) tokenizer.apply_chat_template(ds[ 0 ][ \"text\" ])\n\nYou can also use sourced Kaggle datasets directly with E2B code execution using the following code:\n\nimport kagglehub import e2b_code_interpreter as e2b from datasets import load_dataset ds = load_dataset( \"jupyter-agent/jupyter-agent-dataset\" , split= \"thinking\" ) dataset_name = ds[ 0 ][ \"kaggle_dataset_name\" ] path = kagglehub.dataset_download(dataset_name) print (path) sandbox_init = e2b.Sandbox(timeout= 240 ) file_name = ds[ 0 ][ \"files_used\" ][ 0 ] file_name = file_name.split( '/' )[- 1 ] if '/' in file_name else file_name with open ( f\" {path} / {file_name} \" , \"rb\" ) as file: sandbox_init.files.write( f\"/home/user/input/ {file_name} \" , file) execution = sandbox_init.run_code( \"<some code>\" )\n\nYou use tuned Jupyter Agent Qwen-based models following the Qwen documentation code:\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer model_name = \"jupyter-agent/jupyter-agent-qwen3-4b-instruct\" tokenizer = AutoTokenizer.from_pretrained(model_name) model = AutoModelForCausalLM.from_pretrained( model_name, torch_dtype= \"auto\" , device_map= \"auto\" ) prompt = \"Give me a short introduction to large language model.\" messages = [ { \"role\" : \"user\" , \"content\" : prompt} ] text = tokenizer.apply_chat_template( messages, tokenize= False , add_generation_prompt= True , ) model_inputs = tokenizer([text], return_tensors= \"pt\" ).to(model.device) generated_ids = model.generate( **model_inputs, max_new_tokens= 16384 ) output_ids = generated_ids[ 0 ][ len (model_inputs.input_ids[ 0 ]):].tolist() content = tokenizer.decode(output_ids, skip_special_tokens= True ) print ( \"content:\" , content)\n\nFor Thinking model you can decode both thinking response and content using the next code:\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer model_name = \"jupyter-agent/jupyter-agent-qwen3-4b-thinking\" try : index = len (output_ids) - output_ids[::- 1 ].index( 151668 ) except ValueError: index = 0 thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens= True ).strip( \"\n\n\" ) content = tokenizer.decode(output_ids[index:], skip_special_tokens= True ).strip( \"\n\n\" )\n\n\ud83d\udd2e Next Steps\n\nHarder tasks: Generate more challenging, multi-step questions that better reflect real-world analysis.\n\nScaling up: Train on larger volumes of curated traces to push beyond the current 3.4% performance on the hard split.\n\nDistillation: Investigate knowledge distillation, which has shown strong results for improving small models.\n\nReinforcement Learning (RL): Build an RL environment, which has been shown to achieve state-of-the-art performance on agentic tasks. Since our QA setup already provides a verifiable environment, we could leverage it directly for RL training.\n\nMaybe this will lead to\u2026 Jupyter-Agent 3. \ud83d\ude09\n\nWe hope that our findings will inspire others to continue progress in developing more powerful notebook coding agents and we're excited to see what the community builds next. Dive into our jupyter-agent dataset on the \ud83e\udd17 Hub and explore the codebase at https://github.com/huggingface/jupyter-agent to start your own experiments on agents for jupyter notebooks.",
    "link": "https://huggingface.co/blog/jupyter-agent-2",
    "Summary": "Jupyter Agents: training LLMs to reason with notebooksPublished September 10, 2025 Update on GitHubThe past year has been all about giving LLMs more tools and autonomy to solve more complex and open ended tasks.\nKaggle notebooks offer a wealth of high quality data analysis notebooks and are made available by Kaggle:Datasets:Kaggle Notebooks dataset : ~2TB of notebooks.\nAs part of the StarCoder2 training data processing the notebooks (without output cells) were already deduplicated.\nTraining thinking models on short reasoning texts may disrupt model capabilities \u2192 full-parameter training works better comparing to PEFT in this case.\nTry Jupyter Agent YourselfThese results demonstrate that even small models can become powerful data science agents with the right training approach.",
    "Keywords": [
      "data",
      "tasks",
      "agents",
      "models",
      "jupyter",
      "training",
      "notebooks",
      "llms",
      "reason",
      "dataset",
      "notebook",
      "datasets",
      "code",
      "model"
    ]
  },
  {
    "Title": "Tricks from OpenAI gpt-oss YOU \ud83e\udef5 can use with transformers",
    "Authors": [],
    "Publish Date": null,
    "Text": "Tricks from OpenAI gpt-oss YOU \ud83e\udef5 can use with transformers\n\nPublished September 11, 2025 Update on GitHub\n\ntransformers\n\nOpenAI recently released their GPT-OSS series of models . The models feature some novel techniques like MXFP4 quantization, efficient kernels, a brand new chat format, and more. To enable the release of gpt-oss through, we have upgraded the library considerably. The updates make it very efficient to, andthe models.\n\nIn this blog post, we talk about all the upgrades in-depth, and how they become part of the transformers toolkit so other models (current and future) can benefit from them. Providing clean implementations of new methods in transformers also allows the community to quickly understand and adopt them. Frameworks such as MLX , llama.cpp or vLLM can use the transformers code as a reference to build their own implementations.\n\nFor this release, we worked on:\n\nBest part: Most of these features should work across all major models within transformers !\n\nZero-build Kernels, downloadable from the Hub\n\nA kernel is a specialized, compact program that runs on accelerators to execute tasks like matrix multiplications, activations, or normalizations. In eager PyTorch, operations trigger individual kernels sequentially, which is straightforward but can incur extra memory transfers and launch overheads. PyTorch 2.0's torch.compile with backends like TorchInductor addresses this by automatically fusing and optimizing kernels, delivering 2\u201310\u00d7 performance gains.\n\nIn addition, the community has created custom kernels for frequent combinations of operations, not just individual PyTorch ops like matmul. For example, Flash Attention was created to optimize the critical attention block that defines the transformers architecture, and is present in many models including most LLMs. By carefully combining all the attention operations inside a single kernel, memory transfers are minimized, memory use is reduced, and speedups can be achieved.\n\nThe problem is that all these various kernels are available in separate libraries, which creates a dependency bloat if they were to be added to the transformers library. Furthermore, these kernels are not just Python code, they consist of low-level cuda code, glued together with C++ and exposed through a Python layer. This means they have to be compiled in the target system, which in turn requires whatever build system is required by each kernel library.\n\nThe kernels package solves this problem by downloading pre-built binaries of supported kernels from the Hub. You just indicate the kernel you want to use, and kernels will look for a version compatible with your system and download it on first use.\n\nCustom Kernels for GPT-OSS\n\nGPT-OSS, a Mixture of Experts (MoE) model, is a big user of Kernels from the Hub. It leverages several custom kernels:\n\nLiger RMSNorm, used as @use_kernel_forward_from_hub(\"RMSNorm\") ` Megablocks MoE kernels: @use_kernel_forward_from_hub(\"MegaBlocksMoeMLP\") Flash Attention 3 with support for attention sinks. MXFP4 triton kernels (covered later)\n\nLet's take a look at the first two ones.\n\nBehind the scenes, the decorators (1 and 2) simply point to community-contributed kernels. For example, RMSNorm comes from liger_kernels , while the MegaBlocksMoeMLP kernel comes from megablocks . Depending on your device (CUDA or ROCm) and whether you\u2019re training or running inference, the right kernel is pulled in automatically.\n\nThis design is both specific and general: the RMSNorm liger kernels are already being reused across multiple models, and the MoE kernel could be applied to future MoEs as well.\n\nBecause kernels pulls code from the Hub, you have to opt-in to this feature by passing use_kernels=True in your model instantiation, as shown below. We enable INFO logging in the example so you can easily verify that downloadable kernels are in use.\n\nThese kernels are not compatible with mxfp4 , so inference will happen in bfloat16 if you use them. Please, benchmark your system for the best combination in memory and throughput that suits your project!\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM import logging logging.basicConfig(level=logging.INFO) model_id = \"openai/gpt-oss-20b\" tokenizer = AutoTokenizer.from_pretrained(model_id) model = AutoModelForCausalLM.from_pretrained( model_id, dtype= \"auto\" , device_map= \"auto\" , use_kernels= True , )\n\nRunning a quick generation yields log messages like\n\nINFO:root:Using layer `LigerRMSNorm` from repo `kernels-community/liger_kernels` INFO:root:Using layer `MegaBlocksMoeMLP` from repo `kernels-community/megablocks`\n\nFigure 1 shows that, in the system we tested, these kernels work best for larger batch sizes. We always recommend to benchmark any performance-related changes as closely to your production conditions as possible.\n\nFigure 1: Benchmarking results of custom kernels\n\nYou can explore and play with the benchmarking script here\n\nFlash Attention 3\n\nOpenAI gpt-oss models use attention sinks, which improves quality and facilitates the use of longer contexts. The vLLM team added this feature to the latest version of Flash Attention (Flash Attention 3), and the resulting custom kernel is available on the Hub. Currently, this kernel is compatible with the Hopper architecture. If you have one, this is the way to enable it:\n\nmodel = AutoModelForCausalLM.from_pretrained( model_id, dtype=\"auto\", device_map=\"auto\", + # Flash Attention with Sinks + attn_implementation=\"kernels-community/vllm-flash-attn3\", )\n\nMXFP4 Quantization\n\nLarge language models are memory-hungry. Quantization reduces memory footprint by storing weights (and sometimes activations) in lower-precision formats. For reference, FP32 uses 32 bits per number and BF16 uses 16. By reducing bit width, we trade some precision for smaller models and faster memory movement.\n\nIf you want a visual primer on quantization trade-offs, Maarten Grootendorst\u2019s article is excellent: A Visual Guide to Quantization.\n\nWhat is MXFP4\n\nFigure 2: The E2M1 format used in the MXFP4 format\n\nMXFP4 is a 4-bit floating format with E2M1 layout: 1 sign bit, 2 exponent bits, and 1 mantissa bit, as shown in Figure 2. On its own, E2M1 is very coarse. MXFP4 compensates with blockwise scaling:\n\nVectors are grouped into blocks of 32 elements.\n\nEach block stores a shared scale that restores dynamic range when dequantizing.\n\nInside each block, 4-bit values represent numbers relative to that scale.\n\nThis blockwise scheme lets MXFP4 keep range while using very few bits. In practice, GPT-OSS 20B fits in roughly 16 GB of VRAM and GPT-OSS 120B fits in roughly 80 GB when MXFP4 is active, which is the difference between \u201ccannot load\u201d and \u201ccan run on a single GPU.\u201d The catch is that matrix multiplies now have to respect block scales. Doing this efficiently at scale requires dedicated kernels.\n\nMXFP4 in transformers\n\ntransformers now includes native support for MXFP4, leveraging optimized triton (MXFP4) kernels for enhanced performance. This builds on the community-driven kernel distribution discussed earlier, utilizing pre-compiled kernels from the Hub to simplify deployment.\n\nKey implementation details:\n\nQuantizer logic: Found in the MXFP4 quantizer file, this handles the core quantization process for MXFP4.\n\nIntegration hooks: The MXFP4 integration file enables seamless use of MXFP4 within the transformers framework.\n\nTo check if a model supports MXFP4 , inspect its configuration:\n\nfrom transformers import GptOssConfig model_id = \"openai/gpt-oss-120b\" cfg = GptOssConfig.from_pretrained(model_id) print (cfg.quantization_config)\n\nIf 'quant_method': 'mxfp4' is present, the model will automatically use the MXFP4 pathway with Triton kernels when supported.\n\nThanks to this pull request, you can fine-tune gpt-oss models and save them directly to the Hub in MXFP4 format, streamlining deployment with optimized performance.\n\nRequirements and fallbacks\n\nTo run MXFP4 on GPU you need:\n\naccelerate , kernels , and triton>=3.4 installed. Note that Pytorch 2.8 already comes with triton 3.4 , so you only need to manually install triton if using Pytorch 2.7 . NVIDIA GPU with compute capability \u2265 7.5 . This goes all the way back to Tesla, so you can run gpt-oss-20b on the free tiers of Google Colab and Kaggle, and on many consumer GPUs.\n\nIf these constraints are not met, transformers falls back to a higher-precision path ( bfloat16 is used by default), which requires about 4\u00d7 the memory of MXFP4.\n\nThe snippet loads GPT-OSS twice on CUDA: once with Mxfp4Config(dequantize=True) (memory intensive) and once in the default quantized path (memory efficient). Figure 3 shows the amount of used VRAM after each load so you can visualize the savings.\n\nFigure 3: Memory requirements for the quantized and dequantized models\n\nKernels for MXFP4\n\nEfficient MXFP4 requires kernels that understand 32-element blocks and their scales during GEMMs and fused ops. This is where Kernels from the Hub comes in again. transformers automatically pulls in the MXFP4 -aware Triton kernels from the community repository when you load a model that needs them. The repository will appear in your local cache and will be used during the forward pass. For the MXFP4 kernels one does not need to use the use_kernels=True parameter like before, it is set to default in transformers .\n\nQuick sanity check with the Hugging Face cache CLI, after running gpt-oss-20b on a GPU compatible with the triton MXFP4 kernels:\n\nhf cache scan\n\nSample output:\n\nREPO ID REPO TYPE SIZE ON DISK -------------------------------- --------- ------------ kernels-community/triton_kernels model 536.2K openai/gpt-oss-20b model 13.8G\n\nThis indicates the MXFP4 kernels were fetched and are available for execution.\n\nLet's run some benchmarks and see how well the MXFP4 kernels perform. In Figure 4, we see that the MXFP4 kernels are even better than the custom MoE and RMSNorm kernels for larger batches.\n\nFigure 4: MXFP4 kernel benchmark\n\nYou can explore and play with the benchmarking script here\n\nTensor Parallelism\n\nFigure 5: Explanation of tensor parallelism.\n\nTensor Parallelism (TP) splits tensors inside a layer across multiple GPUs (as shown in Figure 5). Each GPU multiplies its shard in parallel, and then partial results are collected using all-gather or all-reduce operations. This reduces per-GPU memory and keeps all GPUs working on the same layer, which improves throughput as sequence length or batch size grow. TP is communication-intensive and generally works best on a single machine with fast intra-node links.\n\nWhat this enables in transformers\n\ntransformers implements TP directly in from_pretrained . You can start with the predefined plan:\n\nimport torch from transformers import PreTrainedTokenizerFast, GptOssForCausalLM model_id = \"openai/gpt-oss-120b\" tokenizer = PreTrainedTokenizerFast.from_pretrained(model_id) model = GptOssForCausalLM.from_pretrained( model_id, tp_plan= \"auto\" , dtype= \"auto\" , ). eval () messages = [ { \"role\" : \"system\" , \"content\" : \"Be concise.\" }, { \"role\" : \"user\" , \"content\" : \"Explain KV caching briefly.\" }, ] inputs = tokenizer.apply_chat_template( messages, add_generation_prompt= True , return_tensors= \"pt\" , return_dict= True , reasoning_effort= \"low\" , ).to(model.device) with torch.inference_mode(): generations = model.generate(**inputs, max_new_tokens= 128 ) print (tokenizer.decode(generations[ 0 ][inputs[ \"input_ids\" ].shape[- 1 ]:]))\n\nIf you don\u2019t have the infrastructure to run the above, you can just spawn a process on our GPUs using Hugging Face Jobs!\n\nhf jobs run --detach --flavor l4x4 ghcr.io/astral-sh/uv:debian /bin/bash -c \\ \"uv venv .venv --python 3.12 && \\ source .venv/bin/activate && \\ uv pip install --upgrade torch numpy transformers accelerate triton kernels && \\ wget https://huggingface.co/datasets/ariG23498/distributed/raw/main/tp_gpt_oss.py && \\ torchrun --nproc-per-node=4 tp_gpt_oss.py\"\n\nhf jobs is available for all Hugging Face PRO & Enterprise users.\n\nUnder the hood, tp_plan=\"auto\" selects a predefined sharding recipe for each layer and wires the necessary collectives. You can inspect the active plan with print(model._tp_plan) if you want to verify what is being sharded.\n\nWhen to reach for TP\n\nUse TP when the model is too large for one GPU and you want parallel compute, not only memory placement. TP tends to scale throughput with more GPUs, especially for long sequences or larger batches.\n\nIf you are curious about how TP differs from device_map=\"auto\" (memory placement), this short Stack Overflow answer explains the distinction and when to use each.\n\nTo learn more about TP, here are two must-read resources:\n\ntransformers guide: Tensor parallelism, supported models, plans, and extension points.\n\nguide: Tensor parallelism, supported models, plans, and extension points. Ultra-Scale Playbook: background on TP and its relationship to other parallelism modes.\n\nExpert Parallelism\n\nExpert Parallelism (EP) shards experts inside MoE layers across GPUs. Each token is routed to one or a few experts, so only those experts run their feed-forward pass. Since experts are independent MLPs, we can place different experts on different ranks and exchange only the hidden states for the routed tokens. This keeps the matrix multiplies intact on each rank and replaces tensor slicing with routing and collectives.\n\nRun with multiple processes using torchrun . EP is enabled via the distributed configuration and works with GPT-OSS MoE layers out of the box in transformers.\n\nimport torch from transformers import PreTrainedTokenizerFast, GptOssForCausalLM from transformers.distributed import DistributedConfig model_id = \"openai/gpt-oss-120b\" tokenizer = PreTrainedTokenizerFast.from_pretrained(model_id) model = GptOssForCausalLM.from_pretrained( model_id, distributed_config=DistributedConfig(enable_expert_parallel= True ), dtype= \"auto\" , ). eval () messages = [ { \"role\" : \"system\" , \"content\" : \"Be concise.\" }, { \"role\" : \"user\" , \"content\" : \"Explain KV caching briefly.\" }, ] inputs = tokenizer.apply_chat_template( messages, add_generation_prompt= True , return_tensors= \"pt\" , return_dict= True , reasoning_effort= \"low\" , ).to(model.device) with torch.inference_mode(): generations = model.generate(**inputs, max_new_tokens= 128 ) print (tokenizer.decode(generations[ 0 ][inputs[ \"input_ids\" ].shape[- 1 ]:]))\n\nHere is how you would run using hf jobs\n\nhf jobs run --detach --flavor l4x4 ghcr.io/astral-sh/uv:debian /bin/bash -c \\ \"uv venv .venv --python 3.12 && \\ source .venv/bin/activate && \\ uv pip install --upgrade torch numpy transformers accelerate triton kernels && \\ wget https://huggingface.co/datasets/ariG23498/distributed/raw/main/ep_gpt_oss.py && \\ torchrun --nproc-per-node=4 ep_gpt_oss.py\"\n\nWhen you enable Expert Parallelism, Tensor Parallelism is also activated. This means you enjoy the best of both worlds!\n\nDynamic Sliding Window Layer & Cache\n\nMany recent LLMs use sliding window attention, or a combination of sliding and global attention layers, as a means to save memory and reduce those expensive quadratic matmuls that grow with sequence length. However, the dynamic KV cache implementation in transformers used to continue to allocate space according to sequence length, without looking at the individual attention layers. You could always optimize memory using compilation (meaning, fixed shapes), but that's a separate scenario altogether.\n\ntransformers now has a DynamicSlidingWindowLayer and a config\u2011aware DynamicCache . If the model config declares sliding\u2011window or hybrid attention (both sliding and global attention layers are used), the cache stops growing past the window for the sliding layers. If you don\u2019t pass the config, behavior stays as before (full, ever\u2011growing KV as sequence length grows).\n\nFor models that only use sliding window layers, such as Mistral 7B, cache memory stops growing when the sequence reaches the window size (4096, in this case). This makes sense, because the sliding layers can't look past the previous 4K tokens anyway.\n\nOpenAI gpt-oss alternates between sliding and global attention layers, which results in total KV cache memory being halved, as we'll see, as sequence length increases. This provides us with:\n\nMuch lower KV\u2011cache memory for models with sliding or hybrid attention (e.g. GPT\u2011OSS). Cache growth plateaus once the window is reached (e.g., 4K for Mistral; 128 for GPT\u2011OSS sliding layers), instead of scaling linearly with total generated tokens. (GitHub, Transformers)\n\nfor models with sliding or hybrid attention (e.g. GPT\u2011OSS). Cache growth plateaus once the window is reached (e.g., 4K for Mistral; 128 for GPT\u2011OSS sliding layers), instead of scaling linearly with total generated tokens. (GitHub, Transformers) Speed/latency wins on long prompts/long generations: smaller KV tensors mean lighter attention reads/writes and less memory bandwidth pressure, especially after the window is hit. (This is the central motivation behind sliding\u2011window/hybrid LLMs.) (AI21, vLLM Blog)\n\nHow to use it\n\nThe optimized cache is set by default, that means you don't have to make any changes to your existing code. If you want to create the DynamicCache explicitly here is how you would do it:\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, DynamicCache model_id = \"openai/gpt-oss-20b\" tokenizer = AutoTokenizer.from_pretrained(model_id) model = AutoModelForCausalLM.from_pretrained( model_id, dtype= \"auto\" , device_map= \"auto\" , ). eval () messages = [ { \"role\" : \"system\" , \"content\" : \"Always respond in riddles\" }, { \"role\" : \"user\" , \"content\" : \"What is the weather like in Madrid?\" }, ] inputs = tokenizer.apply_chat_template( messages, add_generation_prompt= True , return_tensors= \"pt\" , return_dict= True , reasoning_effort= \"low\" , ).to(model.device) cache = DynamicCache(config=model.config) generated = model.generate( **inputs, max_new_tokens= 500 , past_key_values=cache ) print (tokenizer.decode(generated[ 0 ][inputs[ \"input_ids\" ].shape[- 1 ]:]))\n\nFigure 6 showcases how much of a difference it makes for us to use the Dynamic KV Cache with sliding window attention.\n\nFigure 6: The memory analysis of dynamic cache with sliding window attention\n\nContinuous Batching & Paged Attention\n\nA typical autoregressive generation process looks like Figure 7. You input the prefill tokens, and the model predicts each new token one after the other until it predicts the EOS (End of Sequence) token.\n\nFigure 7: Autoregressive token generation\n\nLet\u2019s see what the generation process looks like when we pass a batch of inputs. In Figure 8 you notice that some generations finish off earlier than the others. This mismatch of length underutilizes the GPUs.\n\nFigure 8: Static batching of sequences\n\nThis type of batching sequences is called static batching. While this is simple and easy to understand, it inherently comes with inefficiencies. Only after each sentence is completely generated can we move on to the next batch.\n\nTo bypass this issue, we use dynamic batching (also known as continuous batching). Instead of waiting for all the generation to finish, we schedule incoming requests to the completed generations. That way, as soon as a generation in a batch is complete, we prefill the batch with the next request. The process looks like Figure 9.\n\nFigure 9: Continuous Batching of sequences\n\nTransformers supports continuous batching with the generate_batch API. This is not meant for production-grade model serving \u2013frameworks like vLLM and SGLang are great at that\u2013, but can be very helpful for evaluation and experimentation. Here is an example script that runs CB end to end on Qwen/Qwen3-4B-Instruct-2507 .\n\nWe have also performed a benchmark between Continuous Batching and Static Batching with 100 samples. In Figure 9, we note that CB is quite faster than SB.\n\nFigure 9: Continuous vs Static Batching Tokens/Second\n\nYou can play around with the benchmark here: SB, CB\n\nLoad larger models faster\n\nWhen you load a large model into your GPU, PyTorch needs to reserve GPU memory for each layer\u2019s weights. Each of these requests (per layer) takes time, and for multi-billion-parameter models it can mean thousands of tiny memory allocations, adding up to a long wait before the model is ready. Instead of asking the GPU for new memory every single time, it can hold on to a big chunk once and then hand out slices from it quickly.\n\nPyTorch allocators can do exactly this. The catch is that the allocator only gets fast after you\u2019ve given it some memory to work with. If you don\u2019t \u201cstock the pantry\u201d first, you still end up doing many slow trips to the market. This PR (\ud83c\udf89 #36380) taught transformers to pre-stock the pantry before it starts copying model weights.\n\nIt:\n\nLooks at the device_map (where each layer will live).\n\n(where each layer will live). Pre-allocates a big enough block on each GPU .\n\n. Then, as layers are copied in, they just slot neatly into this pre-reserved space.\n\nYou have to make no changes to your existing code, as this is default behaviour in transformers . If you use device_map=\"auto\" or provide your own device map, your model will now load faster automatically. If you\u2019re running with Tensor Parallel ( tp_plan=\"auto\" ) and torchrun you also benefit from companion changes that make multi-GPU loading smarter.\n\nConclusion\n\ntransformers moves quickly and it is community-first. The library evolves at the pace of the field because contributors shape it in the open. Pieces added for new models become part of the toolkit and are reused in future integrations.\n\nThis velocity enables day-zero integrations like the GPT-OSS series. As the stack becomes increasingly PyTorch-first, it trims bloat and doubles down on the PyTorch paths that matter in practice. The result is a cleaner core that unlocks new capabilities through community kernels, quantization, and parallelism plans, while also standardizing model definitions so that architectures supported in transformers are a reference and extend across the wider ecosystem.\n\nThis post is a one-time snapshot of a process we repeatedly iterate on towards the same direction: serve the needs of the community. To be up to date with the latest additions to transformers, check the docs and release notes. And please, keep sharing your feedback and releasing your models in transformers for the community to enjoy \ud83e\udd17\n\nRead More\n\nIf you want to go further into particular topics, here is a list of links that one should visit:",
    "link": "https://huggingface.co/blog/faster-transformers",
    "Summary": "Tricks from OpenAI gpt-oss YOU \ud83e\udef5 can use with transformersPublished September 11, 2025 Update on GitHubtransformersOpenAI recently released their GPT-OSS series of models .\nMXFP4 in transformerstransformers now includes native support for MXFP4, leveraging optimized triton (MXFP4) kernels for enhanced performance.\nOpenAI gpt-oss alternates between sliding and global attention layers, which results in total KV cache memory being halved, as we'll see, as sequence length increases.\nCache growth plateaus once the window is reached (e.g., 4K for Mistral; 128 for GPT\u2011OSS sliding layers), instead of scaling linearly with total generated tokens.\nCache growth plateaus once the window is reached (e.g., 4K for Mistral; 128 for GPT\u2011OSS sliding layers), instead of scaling linearly with total generated tokens.",
    "Keywords": [
      "models",
      "openai",
      "mxfp4",
      "attention",
      "gptoss",
      "layers",
      "sliding",
      "transformers",
      "tricks",
      "kernels",
      "model",
      "memory"
    ]
  },
  {
    "Title": "mmBERT: ModernBERT goes Multilingual",
    "Authors": [],
    "Publish Date": null,
    "Text": "mmBERT: ModernBERT goes Multilingual\n\nPublished September 9, 2025 Update on GitHub\n\nThis blog post introduces mmBERT, a state-of-the-art massively multilingual encoder model trained on 3T+ tokens of text in over 1800 languages. It shows significant performance and speed improvements over previous multilingual models, being the first to improve upon XLM-R, while also developing new strategies for effectively learning low-resource languages. mmBERT builds upon ModernBERT for a blazingly fast architecture, and adds novel components to enable efficient multilingual learning.\n\nIf you are interested in trying out the models yourself, some example boilerplate is available at the end of this blogpost!\n\nTraining Data\n\nFigure 1: the training data is progressively annealed to include more languages and more uniform sampling throughout training.\n\nmmBERT was trained on a carefully curated multilingual dataset totaling over 3T tokens across three distinct training phases. The foundation of our training data consists of three primary open-source and high-quality web crawls that enable both multilingual coverage and data quality:\n\nDCLM and Filtered DCLM provides the highest quality English content available, serving as the backbone for strong English performance (with the filtered data coming from Dolmino). This dataset represents state-of-the-art web filtering techniques and forms a crucial component. Due to the high quality of this data, we use a signficantly higher proportion of English than previous generation multilingual encoder models (up to 18%).\n\nFineWeb2 delivers broad multilingual web content covering over 1,800 languages. This dataset enables our extensive multilingual coverage while maintaining reasonable quality standards across diverse language families and scripts.\n\nFineWeb2-HQ consists of a filtered subset of FineWeb2 focusing on 20 high-resource languages. This filtered version provides higher-quality multilingual content that bridges the gap between English-only filtered data and broad multilingual coverage.\n\nThe training data also incorporates specialized corpora from Dolma, MegaWika v2, ProLong and more: code repositories (StarCoder, ProLong), academic content (ArXiv, PeS2o), reference materials (Wikipedia, textbooks), and community discussions (StackExchange), along with instruction and mathematical datasets.\n\nThe key innovation in our data approach is the progressive language inclusion strategy shown in Figure 1. At each phase we progressively sample from a flatter distribution (i.e. closer to uniform), while also adding new languages. This means that high resource languages like Russian start off with a high percentage of the data (i.e. 9%) and then in the last phase of training end around half of that. We start with 60 high-resource languages during pre-training, expand to 110 languages during mid-training, and finally include all 1,833 languages from FineWeb2 during the decay phase. This allows us to maximize the impact of limited low-resource language data without excess reptitions and while maintaining high overall data quality.\n\nTraining Recipe and Novel Components\n\nmmBERT builds upon the ModernBERT architecture but introduces several key innovations for multilingual learning:\n\nArchitecture\n\nWe use the same core architecture as ModernBERT-base with 22 layers and 1152 intermediate dimensions, but switch to the Gemma 2 tokenizer to better handle multilingual text. The base model has 110M non-embedding parameters (307M total due to the larger vocabulary), while the small variant has 42M non-embedding parameters (140M total).\n\nThree-Phase Training Approach\n\nOur training follows a carefully designed three-phase schedule:\n\nPre-training (2.3T tokens): Warmup and stable learning rate phase using 60 languages with 30% mask rate Mid-training (600B tokens): Context extension to 8192 tokens, higher-quality data, expanded to 110 languages with 15% mask rate Decay phase (100B tokens): Inverse square root learning rate decay, all 1,833 languages included with 5% mask rate\n\nNovel Training Techniques\n\nInverse Mask Ratio Schedule: Instead of using a fixed masking rate, we progressively reduce the mask ratio from 30% \u2192 15% \u2192 5% across training phases. This allows the model to learn basic representations with higher masking early on, then focus on more nuanced understanding with lower masking rates.\n\nAnnealed Language Learning: We dynamically adjust the temperature for multilingual data sampling from \u03c4=0.7 \u2192 0.5 \u2192 0.3. This creates a progression from high-resource language bias toward more uniform sampling, enabling the model to build a strong multilingual foundation before learning low-resource languages.\n\nProgressive Language Addition: Rather than training on all languages simultaneously, we strategically add languages at each phase (60 \u2192 110 \u2192 1,833). This maximizes learning efficiency by avoiding excessive epochs on limited low-resource data while still achieving strong performance.\n\nModel Merging: We train three different variants during the decay phase (English-focused, 110-language, and all-language) and use TIES merging to combine their strengths into the final model.\n\nResults\n\nNatural Language Understanding (NLU)\n\nTable 1: Performance on GLUE (English)\n\nEnglish Performance: On the English GLUE benchmark (Table 1), mmBERT base achieves strong performance, substantially outperforming other multilingual models like XLM-R (multilingual RoBERTa) base and mGTE base, while remaining competitive to English-only models despite less than 25% of the mmBERT training data being English.\n\nTable 2: Performance on XTREME (Multilingual)\n\nMultilingual Performance: mmBERT shows significant improvements on XTREME benchmark compared to XLM-R as demonstrated in Table 2. Notable gains include strong performance on XNLI classification, substantial improvements in question answering tasks like TyDiQA, and competitive results across PAWS-X and XCOPA for cross-lingual understanding.\n\nThe model performs well across most categories, with the exception of some structured prediction tasks like NER and POS tagging, likely due to tokenizer differences that affect word boundary detection. On these categories, it performs about the same as the previous generation, but can be applied to more languages.\n\nRetrieval Performance\n\nTable 3: Performance on MTEB v2 English\n\nEnglish Retrieval: Even though mmBERT is designed for massively multilingual settings, in the MTEB v2 English benchmarks (Table 3), mmBERT shows significant gains over previous multilingual models and even ties the capabilities of English-only models like ModernBERT!\n\nTable 4: Performance on MTEB v2 Multilingual\n\nMultilingual Retrieval: mmBERT shows consistent improvements on MTEB v2 multilingual benchmarks compared to other models (Table 4).\n\nTable 5: Performance on CoIR code benchmark\n\nCode Retrieval: Due to the modern tokenizer (based on Gemma 2) mmBERT also shows strong coding performance (Table 5), making mmBERT suitable for any type of textual data. The only model that outperforms it is EuroBERT, which was able to use the non-publicly accessible Stack v2 dataset.\n\nLearning Languages in the Decay Phase\n\nOne of mmBERT's most significant novel features is demonstrating that low-resource languages can be effectively learned during the short decay phase of training. We validated this approach by testing on languages only introduced during the final 100B token decay phase.\n\nFigure 2: adding more than 1700 languages in the decay phase allows for rapid learning, which we keep through model merging.\n\nDramatic Performance Gains: Testing on TiQuaD (Tigrinya) and FoQA (Faroese), we observed substantial improvements when these languages were included in the decay phase, as shown in Figure 2. The results demonstrate the effectiveness of our progressive language learning approach.\n\nCompetitive with Large Models: Despite only seeing these languages in the final training phase, mmBERT achieves performance levels that exceed much larger models. On Faroese question answering where LLMs have been benchmarked, mmBERT outperforms Google Gemini 2.5 Pro and OpenAI o3.\n\nRapid Learning Mechanism: The success of decay-phase language learning stems from the model's ability to leverage its strong multilingual foundation built during earlier phases. When exposed to new languages, the model can quickly adapt existing cross-lingual representations rather than learning from scratch.\n\nModel Merging Benefits: The final mmBERT models successfully retain most of the decay-phase improvements while benefiting from the English-focused and high-resource variants through TIES merging.\n\nEfficiency Improvements\n\nmmBERT delivers substantial efficiency gains over previous multilingual encoder models through architectural improvements inherited from ModernBERT:\n\nFigure 3: mmBERT is significantly more efficient than previous multilingual models, up to 2-4x as much!\n\nThroughput Performance: mmBERT processes text significantly faster than existing multilingual models across various sequence lengths, as demonstrated in Figure 3. Both the small and base models show substantial speed improvements over previous multilingual encoders.\n\nModern Architecture Benefits: The efficiency gains come from two main technical improvements:\n\nFlash Attention 2 : Optimized attention computation for better memory usage and speed\n\n: Optimized attention computation for better memory usage and speed Unpadding techniques: Elimination of unnecessary padding tokens during processing\n\nSequence Length Scaling: Unlike older models limited to 512 tokens, mmBERT handles up to 8,192 tokens efficiently while maintaining high throughput. This makes it suitable for longer document processing tasks that are increasingly common in multilingual applications.\n\nEnergy Efficiency: The combination of better throughput and modern architecture results in lower computational costs for inference, making mmBERT more practical for production deployments where multilingual support is needed at scale.\n\nThese efficiency improvements make mmBERT not just more accurate than previous multilingual encoders, but also significantly more practical for real usage.\n\nUsage Examples\n\nYou can use these models with just a few lines of code!\n\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM import torch tokenizer = AutoTokenizer.from_pretrained( \"jhu-clsp/mmBERT-base\" ) model = AutoModelForMaskedLM.from_pretrained( \"jhu-clsp/mmBERT-base\" ) def predict_masked_token ( text ): inputs = tokenizer(text, return_tensors= \"pt\" ) with torch.no_grad(): outputs = model(**inputs) mask_indices = torch.where(inputs[ \"input_ids\" ] == tokenizer.mask_token_id) predictions = outputs.logits[mask_indices] top_tokens, top_indices = torch.topk(predictions, 5 , dim=- 1 ) return [tokenizer.decode(token) for token in top_indices[ 0 ]] texts = [ \"The capital of France is <mask>.\" , \"La capital de Espa\u00f1a es <mask>.\" , \"Die Hauptstadt von Deutschland ist <mask>.\" , ] for text in texts: predictions = predict_masked_token(text) print ( f\"Text: {text} \" ) print ( f\"Predictions: {predictions}\n\n\" )\n\nFine-tuning Examples\n\nEncoders\n\nClick to see how to finetune this into a dense embedding model using Sentence Transformers import argparse from datasets import load_dataset from sentence_transformers import ( SentenceTransformer, SentenceTransformerTrainer, SentenceTransformerTrainingArguments, ) from sentence_transformers.evaluation import TripletEvaluator from sentence_transformers.losses import CachedMultipleNegativesRankingLoss from sentence_transformers.training_args import BatchSamplers def main (): parser = argparse.ArgumentParser() parser.add_argument( \"--lr\" , type = float , default= 8e-5 ) parser.add_argument( \"--model_name\" , type = str , default= \"jhu-clsp/mmBERT-small\" ) args = parser.parse_args() lr = args.lr model_name = args.model_name model_shortname = model_name.split( \"/\" )[- 1 ] model = SentenceTransformer(model_name) dataset = load_dataset( \"sentence-transformers/msmarco-co-condenser-margin-mse-sym-mnrl-mean-v1\" , \"triplet-hard\" , split= \"train\" , ) dataset_dict = dataset.train_test_split(test_size= 1_000 , seed= 12 ) train_dataset = dataset_dict[ \"train\" ].select( range ( 1_250_000 )) eval_dataset = dataset_dict[ \"test\" ] loss = CachedMultipleNegativesRankingLoss(model, mini_batch_size= 16 ) run_name = f\" {model_shortname} -DPR- {lr} \" args = SentenceTransformerTrainingArguments( output_dir= f\"output/ {model_shortname} / {run_name} \" , num_train_epochs= 1 , per_device_train_batch_size= 512 , per_device_eval_batch_size= 512 , warmup_ratio= 0.05 , fp16= False , bf16= True , batch_sampler=BatchSamplers.NO_DUPLICATES, learning_rate=lr, save_strategy= \"steps\" , save_steps= 500 , save_total_limit= 2 , logging_steps= 500 , run_name=run_name, ) dev_evaluator = TripletEvaluator( anchors=eval_dataset[ \"query\" ], positives=eval_dataset[ \"positive\" ], negatives=eval_dataset[ \"negative\" ], name= \"msmarco-co-condenser-dev\" , ) dev_evaluator(model) trainer = SentenceTransformerTrainer( model=model, args=args, train_dataset=train_dataset, eval_dataset=eval_dataset, loss=loss, evaluator=dev_evaluator, ) trainer.train() dev_evaluator(model) model.save_pretrained( f\"output/ {model_shortname} / {run_name} /final\" ) model.push_to_hub(run_name, private= False ) if __name__ == \"__main__\" : main()\n\nClick to see how to finetune this into a multi-vector embedding model with PyLate from datasets import load_dataset from pylate import losses, models, utils from sentence_transformers import ( SentenceTransformerTrainer, SentenceTransformerTrainingArguments, ) def main (): train = load_dataset( path= \"lightonai/ms-marco-en-bge\" , name= \"train\" , ) queries = load_dataset( path= \"lightonai/ms-marco-en-bge\" , name= \"queries\" , ) documents = load_dataset( path= \"lightonai/ms-marco-en-bge\" , name= \"documents\" , ) train.set_transform( utils.KDProcessing(queries=queries, documents=documents).transform, ) num_train_epochs = 1 lr = 8e-5 batch_size = 16 accum_steps = 1 model_name = \"jhu-clsp/mmBERT-small\" model_shortname = model_name.split( \"/\" )[- 1 ] run_name = f\" {model_shortname} -colbert-KD- {lr} \" output_dir = f\"output/ {model_shortname} / {run_name} \" model = models.ColBERT(model_name_or_path=model_name) args = SentenceTransformerTrainingArguments( output_dir=output_dir, num_train_epochs=num_train_epochs, per_device_train_batch_size=batch_size, fp16= False , bf16= True , run_name=run_name, logging_steps= 10 , learning_rate=lr, gradient_accumulation_steps=accum_steps, warmup_ratio= 0.05 , ) train_loss = losses.Distillation(model=model) trainer = SentenceTransformerTrainer( model=model, args=args, train_dataset=train, loss=train_loss, data_collator=utils.ColBERTCollator(tokenize_fn=model.tokenize), ) trainer.train() model.save_pretrained( f\" {output_dir} /final\" ) if __name__ == \"__main__\" : main()\n\nClick to see how to finetune this into a sparse retrieval model using Sentence Transformers import logging from datasets import load_dataset from sentence_transformers import ( SparseEncoder, SparseEncoderModelCardData, SparseEncoderTrainer, SparseEncoderTrainingArguments, ) from sentence_transformers.sparse_encoder.evaluation import SparseNanoBEIREvaluator from sentence_transformers.sparse_encoder.losses import SparseMultipleNegativesRankingLoss, SpladeLoss from sentence_transformers.training_args import BatchSamplers logging.basicConfig( format = \"%(asctime)s - %(message)s\" , datefmt= \"%Y-%m-%d %H:%M:%S\" , level=logging.INFO) model = SparseEncoder( \"jhu-clsp/mmBERT-small\" , model_card_data=SparseEncoderModelCardData( language= \"en\" , license= \"apache-2.0\" , ) ) full_dataset = load_dataset( \"sentence-transformers/natural-questions\" , split= \"train\" ).select( range ( 100_000 )) dataset_dict = full_dataset.train_test_split(test_size= 1_000 , seed= 12 ) train_dataset = dataset_dict[ \"train\" ] eval_dataset = dataset_dict[ \"test\" ] loss = SpladeLoss( model=model, loss=SparseMultipleNegativesRankingLoss(model=model), query_regularizer_weight= 5e-5 , document_regularizer_weight= 3e-5 , ) run_name = \"splade-distilbert-base-uncased-nq\" args = SparseEncoderTrainingArguments( output_dir= f\"models/ {run_name} \" , num_train_epochs= 1 , per_device_train_batch_size= 16 , per_device_eval_batch_size= 16 , learning_rate= 2e-5 , warmup_ratio= 0.1 , fp16= True , bf16= False , batch_sampler=BatchSamplers.NO_DUPLICATES, eval_strategy= \"steps\" , eval_steps= 1000 , save_strategy= \"steps\" , save_steps= 1000 , save_total_limit= 2 , logging_steps= 200 , run_name=run_name, ) dev_evaluator = SparseNanoBEIREvaluator(dataset_names=[ \"msmarco\" , \"nfcorpus\" , \"nq\" ], batch_size= 16 ) trainer = SparseEncoderTrainer( model=model, args=args, train_dataset=train_dataset, eval_dataset=eval_dataset, loss=loss, evaluator=dev_evaluator, ) trainer.train() dev_evaluator(model) model.save_pretrained( f\"models/ {run_name} /final\" ) model.push_to_hub(run_name)\n\nClick to see how to finetune this into a reranker model using Sentence Transformers import logging import traceback import torch from datasets import load_dataset from sentence_transformers import SentenceTransformer from sentence_transformers.cross_encoder import ( CrossEncoder, CrossEncoderModelCardData, CrossEncoderTrainer, CrossEncoderTrainingArguments, ) from sentence_transformers.cross_encoder.evaluation import ( CrossEncoderNanoBEIREvaluator, CrossEncoderRerankingEvaluator, ) from sentence_transformers.cross_encoder.losses import BinaryCrossEntropyLoss from sentence_transformers.evaluation import SequentialEvaluator from sentence_transformers.util import mine_hard_negatives logging.basicConfig( format = \"%(asctime)s - %(message)s\" , datefmt= \"%Y-%m-%d %H:%M:%S\" , level=logging.INFO) def main (): model_name = \"jhu-clsp/mmBERT-small\" train_batch_size = 64 num_epochs = 1 num_hard_negatives = 5 model = CrossEncoder( model_name, model_card_data=CrossEncoderModelCardData( language= \"en\" , license= \"apache-2.0\" , ), ) print ( \"Model max length:\" , model.max_length) print ( \"Model num labels:\" , model.num_labels) logging.info( \"Read the gooaq training dataset\" ) full_dataset = load_dataset( \"sentence-transformers/gooaq\" , split= \"train\" ).select( range ( 100_000 )) dataset_dict = full_dataset.train_test_split(test_size= 1_000 , seed= 12 ) train_dataset = dataset_dict[ \"train\" ] eval_dataset = dataset_dict[ \"test\" ] logging.info(train_dataset) logging.info(eval_dataset) embedding_model = SentenceTransformer( \"sentence-transformers/static-retrieval-mrl-en-v1\" , device= \"cpu\" ) hard_train_dataset = mine_hard_negatives( train_dataset, embedding_model, num_negatives=num_hard_negatives, margin= 0 , range_min= 0 , range_max= 100 , sampling_strategy= \"top\" , batch_size= 4096 , output_format= \"labeled-pair\" , use_faiss= True , ) logging.info(hard_train_dataset) loss = BinaryCrossEntropyLoss(model=model, pos_weight=torch.tensor(num_hard_negatives)) nano_beir_evaluator = CrossEncoderNanoBEIREvaluator( dataset_names=[ \"msmarco\" , \"nfcorpus\" , \"nq\" ], batch_size=train_batch_size, ) hard_eval_dataset = mine_hard_negatives( eval_dataset, embedding_model, corpus=full_dataset[ \"answer\" ], num_negatives= 30 , batch_size= 4096 , include_positives= True , output_format= \"n-tuple\" , use_faiss= True , ) logging.info(hard_eval_dataset) reranking_evaluator = CrossEncoderRerankingEvaluator( samples=[ { \"query\" : sample[ \"question\" ], \"positive\" : [sample[ \"answer\" ]], \"documents\" : [sample[column_name] for column_name in hard_eval_dataset.column_names[ 2 :]], } for sample in hard_eval_dataset ], batch_size=train_batch_size, name= \"gooaq-dev\" , always_rerank_positives= False , ) evaluator = SequentialEvaluator([reranking_evaluator, nano_beir_evaluator]) evaluator(model) short_model_name = model_name if \"/\" not in model_name else model_name.split( \"/\" )[- 1 ] run_name = f\"reranker- {short_model_name} -gooaq-bce\" args = CrossEncoderTrainingArguments( output_dir= f\"models/ {run_name} \" , num_train_epochs=num_epochs, per_device_train_batch_size=train_batch_size, per_device_eval_batch_size=train_batch_size, learning_rate= 2e-5 , warmup_ratio= 0.1 , fp16= False , bf16= True , dataloader_num_workers= 4 , load_best_model_at_end= True , metric_for_best_model= \"eval_gooaq-dev_ndcg@10\" , eval_strategy= \"steps\" , eval_steps= 1000 , save_strategy= \"steps\" , save_steps= 1000 , save_total_limit= 2 , logging_steps= 200 , logging_first_step= True , run_name=run_name, seed= 12 , ) trainer = CrossEncoderTrainer( model=model, args=args, train_dataset=hard_train_dataset, loss=loss, evaluator=evaluator, ) trainer.train() evaluator(model) final_output_dir = f\"models/ {run_name} /final\" model.save_pretrained(final_output_dir) try : model.push_to_hub(run_name) except Exception: logging.error( f\"Error uploading model to the Hugging Face Hub:\n\n{traceback.format_exc()} To upload it manually, you can run \" f\"`huggingface-cli login`, followed by loading the model using `model = CrossEncoder( {final_output_dir!r} )` \" f\"and saving it using `model.push_to_hub(' {run_name} ')`.\" ) if __name__ == \"__main__\" : main()\n\nModel Family and Links\n\nStandard Models:\n\nmmBERT-small (140M params total, 42M non-embed)\n\nmmBERT-base (307M params total, 110M non-embed)\n\nResearch Resources:",
    "link": "https://huggingface.co/blog/mmbert",
    "Summary": "mmBERT: ModernBERT goes MultilingualPublished September 9, 2025 Update on GitHubThis blog post introduces mmBERT, a state-of-the-art massively multilingual encoder model trained on 3T+ tokens of text in over 1800 languages.\nmmBERT builds upon ModernBERT for a blazingly fast architecture, and adds novel components to enable efficient multilingual learning.\nRetrieval PerformanceTable 3: Performance on MTEB v2 EnglishEnglish Retrieval: Even though mmBERT is designed for massively multilingual settings, in the MTEB v2 English benchmarks (Table 3), mmBERT shows significant gains over previous multilingual models and even ties the capabilities of English-only models like ModernBERT!\nEfficiency ImprovementsmmBERT delivers substantial efficiency gains over previous multilingual encoder models through architectural improvements inherited from ModernBERT:Figure 3: mmBERT is significantly more efficient than previous multilingual models, up to 2-4x as much!\nThroughput Performance: mmBERT processes text significantly faster than existing multilingual models across various sequence lengths, as demonstrated in Figure 3.",
    "Keywords": [
      "modernbert",
      "import",
      "data",
      "models",
      "training",
      "performance",
      "multilingual",
      "mmbert",
      "run_name",
      "goes",
      "languages",
      "model"
    ]
  },
  {
    "Title": "Make your ZeroGPU Spaces go brrr with ahead-of-time compilation",
    "Authors": [],
    "Publish Date": null,
    "Text": "Make your ZeroGPU Spaces go brrr with ahead-of-time compilation\n\nPublished September 2, 2025 Update on GitHub\n\nZeroGPU lets anyone spin up powerfulhardware in Hugging Face Spaces without keeping a GPU locked for idle traffic. It\u2019s efficient, flexible, and ideal for demos but it doesn\u2019t always make full use of everything the GPU and CUDA stack can offer. Generating images or videos can take a significant amount of time. Being able to squeeze out more performance, taking advantage of the H200 hardware, does matter in this case.\n\nThis is where PyTorch ahead-of-time (AoT) compilation comes in. Instead of compiling models on the fly (which doesn\u2019t play nicely with ZeroGPU\u2019s short-lived processes), AoT lets you optimize once and reload instantly.\n\nThe result: snappier demos and a smoother experience, with speedups ranging from 1.3\u00d7\u20131.8\u00d7 on models like Flux, Wan, and LTX \ud83d\udd25\n\nIn this post, we\u2019ll show how to wire up Ahead-of-Time (AoT) compilation in ZeroGPU Spaces. We'll explore advanced tricks like FP8 quantization and dynamic shapes, and share working demos you can try right away. If you cannot wait, we invite you to check out some ZeroGPU-powered demos on the zerogpu-aoti organization.\n\nPro users and Team / Enterprise org members can create ZeroGPU Spaces, while anyone can freely use them (Pro, Team and Enterprise users get 8x more ZeroGPU quota)\n\nTable of Contents\n\nWhat is ZeroGPU\n\nSpaces is a platform powered by Hugging Face that allows ML practitioners to easily publish demo apps.\n\nTypical demo apps on Spaces look like:\n\nimport gradio as gr from diffusers import DiffusionPipeline pipe = DiffusionPipeline.from_pretrained(...).to( 'cuda' ) def generate ( prompt ): return pipe(prompt).images gr.Interface(generate, \"text\" , \"gallery\" ).launch()\n\nThis works great, but ends up reserving a GPU for the Space during its entire lifetime \u2013 even when it has no user activity.\n\nWhen executing .to('cuda') on this line:\n\npipe = DiffusionPipeline.from_pretrained(...).to( 'cuda' )\n\nPyTorch initializes the NVIDIA driver, which sets up the process on CUDA forever. This is not very resource-efficient given that app traffic is not perfectly smooth, but is rather extremely sparse and spiky.\n\nZeroGPU takes a just-in-time approach to GPU initialization. Instead of setting up the main process on CUDA, it automatically forks the process, sets it up on CUDA, runs the GPU tasks, and finally kills the fork when the GPU needs to be released.\n\nThis means that:\n\nWhen the app does not receive traffic, it doesn't use any GPU\n\nWhen it is actually performing a task, it will use one GPU\n\nIt can use multiple GPUs as needed to perform tasks concurrently\n\nThanks to the Python spaces package, the only code change needed to get this behaviour is as follows:\n\nimport gradio as gr + import spaces from diffusers import DiffusionPipeline pipe = DiffusionPipeline.from_pretrained(...).to('cuda') + @spaces.GPU def generate(prompt): return pipe(prompt).images gr.Interface(generate, \"text\", \"gallery\").launch()\n\nBy importing spaces and adding the @spaces.GPU decorator, we:\n\nIntercept PyTorch API calls to postpone CUDA operations\n\nMake the decorated function run in a fork when later called\n\n(Call an internal API to make the right device visible to the fork but this is not in the scope of this blogpost)\n\nZeroGPU currently allocates an MIG slice of H200 ( 3g.71gb profile). Additional MIG sizes including full slice ( 7g.141gb profile) will come in late 2025.\n\nPyTorch compilation\n\nModern ML frameworks like PyTorch and JAX have the concept of compilation that can be used to optimize model latency or inference time. Behind the scenes, compilation applies a series of (often hardware-dependent) optimization steps such as operator fusion, constant folding, etc.\n\nPyTorch (from 2.0 onwards) currently has two major interfaces for compilation:\n\nJust-in-time with torch.compile\n\nAhead-of-time with torch.export + AOTInductor\n\ntorch.compile works great in standard environments: it compiles your model the first time it runs, and reuses the optimized version for subsequent calls.\n\nHowever, on ZeroGPU, given that the process is freshly spun up for (almost) every GPU task, it means that torch.compile can\u2019t efficiently re-use compilation and is thus forced to rely on its filesystem cache to restore compiled models. Depending on the model being compiled, this process takes from a few dozen seconds to a couple of minutes, which is way too much for practical GPU tasks in Spaces.\n\nThis is where ahead-of-time (AoT) compilation shines.\n\nWith AoT, we can export a compiled model once, save it, and later reload it instantly in any process, which is exactly what we need for ZeroGPU. This helps us reduce framework overhead and also eliminates cold-start timings typically incurred in just-in-time compilation.\n\nBut how can we do ahead-of-time compilation on ZeroGPU? Let\u2019s dive in.\n\nAhead-of-time compilation on ZeroGPU\n\nLet's go back to our ZeroGPU base example and unpack what we need to enable AoT compilation. For the purpose of this demo, we will use the black-forest-labs/FLUX.1-dev model:\n\nimport gradio as gr import spaces import torch from diffusers import DiffusionPipeline MODEL_ID = 'black-forest-labs/FLUX.1-dev' pipe = DiffusionPipeline.from_pretrained(MODEL_ID, torch_dtype=torch.bfloat16) pipe.to( 'cuda' ) def generate ( prompt ): return pipe(prompt).images gr.Interface(generate, \"text\" , \"gallery\" ).launch()\n\nIn the discussion below, we only compile the transformer component of pipe since, in these generative models, the transformer (or more generally, the denoiser) is the most computationally heavy component.\n\nCompiling a model ahead-of-time with PyTorch involves multiple steps:\n\n1. Getting example inputs\n\nRecall that we\u2019re going to compile the model ahead of time. Therefore, we need to derive example inputs for the model. Note that these are the same kinds of inputs we expect to see during the actual runs. To capture those inputs, we will leverage the spaces.aoti_capture helper from the spaces package:\n\nwith spaces.aoti_capture(pipe.transformer) as call: pipe( \"arbitrary example prompt\" )\n\nWhen used as a context manager, aoti_capture intercepts the call to any callable ( pipe.transformer in our case), prevents it from executing, captures the input arguments that would have been passed to it, and stores their values in call.args and call.kwargs .\n\n2. Exporting the model\n\nNow that we have example args and kwargs for our transformer component, we can export it to a PyTorch ExportedProgram by using torch.export.export utility:\n\nexported_transformer = torch.export.export( pipe.transformer, args=call.args, kwargs=call.kwargs, )\n\nAn exported PyTorch program is a computation graph that represents the tensor computations along with the original model parameter values.\n\n3. Compiling the exported model\n\nOnce the model is exported, compiling it is pretty straightforward.\n\nA traditional AoT compilation in PyTorch often requires saving the model on disk so it can be later reloaded. In our case, we\u2019ll leverage a helper function part of the spaces package: spaces.aoti_compile . It's a tiny wrapper around torch._inductor.aot_compile that manages saving and lazy-loading the model as needed. It's meant to be used like this:\n\ncompiled_transformer = spaces.aoti_compile(exported_transformer)\n\nThis compiled_transformer is now an AoT-compiled binary ready to be used for inference.\n\n4. Using the compiled model in the pipeline\n\nNow we need to bind our compiled transformer to our original pipeline, i.e., the pipeline .\n\nA naive and almost working approach is to simply patch our pipeline like pipe.transformer = compiled_transformer . Unfortunately, this approach does not work because it deletes important attributes like dtype , config , etc. Only patching the forward method does not work well either because we are then keeping original model parameters in memory, often leading to OOM errors at runtime.\n\nspaces package provides a utility for this, too -- spaces.aoti_apply :\n\nspaces.aoti_apply(compiled_transformer, pipe.transformer)\n\nEt voil\u00e0! It will take care of patching pipe.transformer.forward with our compiled model, as well as cleaning old model parameters out of memory.\n\n5. Wrapping it all together\n\nTo perform the first three steps (intercepting input examples, exporting the model, and compiling it with PyTorch inductor), we need a real GPU. CUDA emulation that you get outside of @spaces.GPU function is not enough because compilation is truly hardware-dependent, for instance, relying on micro-benchmark runs to tune the generated code. This is why we need to wrap it all inside a @spaces.GPU function and then get our compiled model back to the root of our app. Starting from our original demo code, this gives:\n\nimport gradio as gr import spaces import torch from diffusers import DiffusionPipeline MODEL_ID = 'black-forest-labs/FLUX.1-dev' pipe = DiffusionPipeline.from_pretrained(MODEL_ID, torch_dtype=torch.bfloat16) pipe.to('cuda') + @spaces.GPU(duration=1500) # maximum duration allowed during startup + def compile_transformer(): + with spaces.aoti_capture(pipe.transformer) as call: + pipe(\"arbitrary example prompt\") + + exported = torch.export.export( + pipe.transformer, + args=call.args, + kwargs=call.kwargs, + ) + return spaces.aoti_compile(exported) + + compiled_transformer = compile_transformer() + spaces.aoti_apply(compiled_transformer, pipe.transformer) @spaces.GPU def generate(prompt): return pipe(prompt).images gr.Interface(generate, \"text\", \"gallery\").launch()\n\nWith just a dozen lines of additional code, we\u2019ve successfully made our demo quite faster (1.7x faster in the case of FLUX.1-dev).\n\nIf you want to learn more about AoT compilation, you can read PyTorch's AOTInductor tutorial\n\nGotchas\n\nNow that we have demonstrated the speedups one can realize under the constraints of operating with ZeroGPUs, we will discuss a few gotchas that came up while working with this setup.\n\nQuantization\n\nAoT can be combined with quantization to deliver even greater speedups. For image and video generation, the FP8 post-training dynamic quantization schemes deliver good speed-quality trade-offs. However, FP8 requires a CUDA compute capability of at least 9.0 to work. Thankfully, for ZeroGPUs, since they\u2019re based on H200s, we can already take advantage of the FP8 quantization schemes.\n\nTo enable FP8 quantization within our AoT compilation workflow, we can leverage the APIs provided by torchao like so:\n\n+ from torchao.quantization import quantize_, Float8DynamicActivationFloat8WeightConfig + # Quantize the transformer just before the export step. + quantize_(pipe.transformer, Float8DynamicActivationFloat8WeightConfig()) exported_transformer = torch.export.export( pipe.transformer, args=call.args, kwargs=call.kwargs, )\n\n(You can find more details about TorchAO here.)\n\nAnd we can then proceed with the rest of the steps as outlined above. Using quantization provides another 1.2x of speedup.\n\nDynamic shapes\n\nImages and videos can come in different shapes and sizes. Hence, it\u2019s important to also account for shape dynamism when performing AoT compilation. The primitives provided by torch.export.export make it easily configurable to provide what inputs should be treated accordingly for dynamic shapes, as shown below.\n\nFor the case of Flux.1-Dev transformer, changes in different image resolutions will affect two of its forward arguments:\n\nhidden_states : The noisy input latents, which the transformer is supposed to denoise. It\u2019s a 3D tensor, representing batch_size, flattened_latent_dim, embed_dim . When the batch size is fixed, it\u2019s the flattened_latent_dim that will change for any changes made to image resolutions.\n\n: The noisy input latents, which the transformer is supposed to denoise. It\u2019s a 3D tensor, representing . When the batch size is fixed, it\u2019s the that will change for any changes made to image resolutions. img_ids : A 2D array of encoded pixel coordinates having a shape of height * width, 3 . In this case, we want to make height * width dynamic.\n\nWe start by defining a range in which we want to let the (latent) image resolutions vary. To derive these value ranges, we inspected the shapes of hidden_states in the pipeline with respect to varied image resolutions. The exact values are model-dependent and require manual inspection and some intuition. For Flux.1-Dev, we ended up with:\n\ntransformer_hidden_dim = torch.export.Dim( 'hidden' , min = 4096 , max = 8212 )\n\nWe then define a map of argument names and which dimensions in their input values we expect to be dynamic:\n\ntransformer_dynamic_shapes = { \"hidden_states\" : { 1 : transformer_hidden_dim}, \"img_ids\" : { 0 : transformer_hidden_dim}, }\n\nThen we need to make our dynamic shapes object replicate the structure of our example inputs. The inputs that do not need dynamic shapes must be set to None . This can be done very easily with PyTorch tree_map utility:\n\nfrom torch.utils._pytree import tree_map dynamic_shapes = tree_map( lambda v: None , call.kwargs) dynamic_shapes |= transformer_dynamic_shapes\n\nNow, when performing the export step, we simply supply transformer_dynamic_shapes to torch.export.export :\n\nexported_transformer = torch.export.export( pipe.transformer, args=call.args, kwargs=call.kwargs, dynamic_shapes=dynamic_shapes, )\n\nCheck out this Space that shows how to use both quantization and dynamic shapes during the export step.\n\nMulti-compile / shared weights\n\nDynamic shapes is sometimes not enough when dynamism is too important.\n\nThis is, for instance, the case with the Wan family of video generation models if you want your compiled model to generate different resolutions. One thing can be done in this case: compile one model per resolution while keeping the model parameters shared and dispatching the right one at runtime\n\nHere is a minimal example of this approach: zerogpu-aoti-multi.py. You can also see a fully working implementation of this paradigm in the Wan 2.2 Space.\n\nSince the ZeroGPU hardware and CUDA drivers are perfectly compatible with Flash-Attention 3 (FA3), we can use it in our ZeroGPU Spaces to speed things up even further. FA3 works with ahead-of-time compilation. So, this is ideal for our case.\n\nCompiling and building FA3 from source can take several minutes, and this process is hardware-dependent. As users, we wouldn\u2019t want to lose precious ZeroGPU compute hours. This is where Hugging Face kernels library comes to the rescue. It provides access to pre-built kernels that are compatible for a given hardware. For example, when we try to run:\n\nfrom kernels import get_kernel vllm_flash_attn3 = get_kernel( \"kernels-community/vllm-flash-attn3\" )\n\nIt tries to load a kernel from the kernels-community/vllm-flash-attn3 repository, which is compatible with the current setup. Otherwise, it will error out due to incompatibility issues. Luckily for us, this works seamlessly on the ZeroGPU Spaces. This means we can leverage the power of FA3 on ZeroGPU, using the kernels library.\n\nHere is a fully working example of an FA3 attention processor for the Qwen-Image model.\n\nRegional compilation\n\nSo far, we have been compiling the full model. Depending on the model, full model compilation can lead to significantly long cold start times. Long cold start times make the development experience unpleasant.\n\nWe can also choose to compile regions within a model, significantly reducing the cold start times, while retaining almost all the benefits of full model compilation. Regional compilation becomes promising when a model has repeated blocks of computation. A standard language model, for example, has a number of identically structured Transformer blocks.\n\nIn our example, we can compile the repeated blocks of the Flux transformer ahead of time, and propagate the compiled graph to the remaining repeated blocks. The Flux Transformer has two kinds of repeated blocks: FluxTransformerBlock and FluxSingleTransformerBlock .\n\nYou can check out this Space for a complete example.\n\n\ud83d\udca1 For Flux.1-Dev, switching to regional compilation reduces the compilation time from 6 minutes to just 30 seconds while delivering identical speedups.\n\nUse a compiled graph from the Hub\n\nOnce a model (or even a model block) is compiled ahead of time, we can serialize the compiled graph module as an artifact and reuse later. In the context of a ZeroGPU-powered demo on Spaces, this will significantly cut down the demo startup time by skipping the compilation time.\n\nTo keep the storage light, we can just save the compiled model graph without including any model parameters inside the artifact.\n\nCheck out this collection that shows a full workflow of obtaining compiled model graph, pushing it to the Hub, and then using it to build a demo.\n\nAoT compiled ZeroGPU Spaces demos\n\nSpeedup comparison\n\nFeatured AoTI Spaces\n\nRegional compilation\n\nConclusion\n\nZeroGPU within Hugging Face Spaces is a powerful feature that enables AI builders by providing access to powerful compute. In this post, we showed how users can benefit from PyTorch\u2019s ahead-of-time compilation techniques to speed up their applications that leverage ZeroGPU.\n\nWe demonstrate speedups with Flux.1-Dev, but these techniques are not limited to just this model. Therefore, we encourage you to give these techniques a try and provide us with feedback in this community discussion.\n\nResources\n\nVisit our ZeroGPU-AOTI org on the Hub to refer to a collection of demos that leverage the techniques discussed in this post.\n\nBrowse spaces.aoti_* APIs source code to learn more about the interface\n\nAPIs source code to learn more about the interface Check out Kernels Community org on the hub\n\nLearn more about regional compilation from here\n\nUpgrade to Pro on Hugging Face to create your own ZeroGPU Spaces (and get 25 minutes of H200 usage every day)\n\nAcknowledgements: Thanks to ChunTe Lee for creating an awesome thumbnail for this post. Thanks to Pedro and Vaibhav for providing feedback on the post. Thanks to Angela Yi from the PyTorch team for helping us with AOT guidance.",
    "link": "https://huggingface.co/blog/zerogpu-aoti",
    "Summary": "Make your ZeroGPU Spaces go brrr with ahead-of-time compilationPublished September 2, 2025 Update on GitHubZeroGPU lets anyone spin up powerfulhardware in Hugging Face Spaces without keeping a GPU locked for idle traffic.\nThe result: snappier demos and a smoother experience, with speedups ranging from 1.3\u00d7\u20131.8\u00d7 on models like Flux, Wan, and LTX \ud83d\udd25In this post, we\u2019ll show how to wire up Ahead-of-Time (AoT) compilation in ZeroGPU Spaces.\nAhead-of-time compilation on ZeroGPULet's go back to our ZeroGPU base example and unpack what we need to enable AoT compilation.\nDepending on the model, full model compilation can lead to significantly long cold start times.\nAoT compiled ZeroGPU Spaces demosSpeedup comparisonFeatured AoTI SpacesRegional compilationConclusionZeroGPU within Hugging Face Spaces is a powerful feature that enables AI builders by providing access to powerful compute.",
    "Keywords": [
      "import",
      "compiled",
      "example",
      "aheadoftime",
      "compilation",
      "cuda",
      "pytorch",
      "brrr",
      "zerogpu",
      "transformer",
      "spaces",
      "model"
    ]
  },
  {
    "Title": "Welcome EmbeddingGemma, Google's new efficient embedding model",
    "Authors": [],
    "Publish Date": null,
    "Text": "Welcome EmbeddingGemma, Google's new efficient embedding model\n\nPublished September 4, 2025 Update on GitHub\n\nToday, Google releases EmbeddingGemma, a state-of-the-art multilingual embedding model perfect for on-device use cases. Designed for speed and efficiency, the model features a compact size of 308M parameters and a 2K context window, unlocking new possibilities for mobile RAG pipelines, agents, and more. EmbeddingGemma is trained to support over 100 languages and is the highest-ranking text-only multilingual embedding model under 500M on the Massive Text Embedding Benchmark (MTEB) at the time of writing.\n\nTable of Contents\n\nIntroduction\n\nText embeddings have become the backbone of modern natural\u2011language applications, turning words, sentences, and documents into dense vectors that capture meaning, sentiment, and intent. These vectors enable fast similarity search, clustering, classification, and retrieval across massive corpora, powering everything from recommendation engines and semantic search to retrieval-augmented generation and code\u2011search tools. Embedding models that calculate these embeddings are widely used, with well over 200 million monthly downloads on Hugging Face.\n\nBuilding on this foundation, Google DeepMind\u2019s EmbeddingGemma arrives as the newest, most capable small multilingual embedding model yet. With just 308M parameters, a 2k\u2011token context window, and support for over 100 languages, EmbeddingGemma delivers state\u2011of\u2011the\u2011art performance on the Massive Multilingual Text Embedding Benchmark (MMTEB) while staying under 200 MB of RAM when quantized.\n\nThe various design choices result in a very practical, open-source tool for computing high-quality multilingual embeddings on everyday devices.\n\nIn this blogpost, we describe the EmbeddingGemma architecture and training, and show you how to use the model with various frameworks like Sentence Transformers, LangChain, LlamaIndex, Haystack, txtai, Transformers.js, Text Embedding Inference, and ONNX.\n\nAfterwards, we demonstrate how to finetune EmbeddingGemma on your domain for even stronger performance. In our example, we finetune EmbeddingGemma on the Medical Instruction and Retrieval Dataset (MIRIAD). The resulting model, sentence-transformers/embeddinggemma-300m-medical, achieves state-of-the-art performance on our task: retrieving passages of scientific medical papers in response to detailed medical questions. It even outperforms models twice as big on this task.\n\nArchitecture\n\nEmbeddingGemma builds on the Gemma3 transformers backbone, but modified to use bi-directional attention instead of causal (one-way) attention. This means that earlier tokens in the sequence can attend to later tokens, effectively turning the architecture from a decoder into an encoder. Encoder models can outperform LLMs, which are decoders, on embedding tasks like retrieval (Weller et al., 2025). With this backbone, the model can process a sizable 2048 tokens at once, sufficient for typical retrieval inputs, especially given that larger inputs often result in information loss in the text embeddings.\n\nBeyond the new Gemma3-based encoder backbone, which produces token embeddings, a mean pooling layer converts these token embeddings into text embeddings. Lastly, two dense layers transform the text embeddings into their final form, a 768-dimensional vector.\n\nThe EmbeddingGemma model has been trained with Matryoshka Representation Learning (MRL), allowing you to truncate the 768\u2011dimensional output to 512, 256, or 128 dimensions on demand. This results in faster downstream processing and lower memory and disk space utilization. See the Sentence Transformers usage for a snippet showing how to perform this truncation.\n\nThe model has been trained using a carefully curated, multilingual corpus totalling approximately 320 billion tokens. The proprietary dataset is a blend of publicly available web text, code and technical documentation, and synthetic task\u2011specific examples. It has been filtered to avoid Child Sexual Abuse Material (CSAM), sensitive data, and low-quality or unsafe content.\n\nEvaluation\n\nEmbeddingGemma was benchmarked on the MMTEB (Multilingual, v2) and MTEB (English, v2) suites, which span a wide range of tasks, domains, and languages. Despite its modest 308M\u2011parameter size, the model consistently beats comparable baselines while keeping a very small memory footprint.\n\nMTEB (Multilingual, v2) Performance MTEB (English, v2) Performance\n\nThe results will be listed on the official MTEB Leaderboard. We exclude any model that has been trained on more than 20% of the MTEB data, to mitigate potential over\u2011fitting.\n\nDemo\n\nThe demo can also be experienced in full screen.\n\nYour browser does not support the video tag. Experience the demo yourself on a Desktop device.\n\nUsage\n\nEmbeddingGemma is integrated with many popular tools, making it easy to incorporate into your existing workflows and applications. The model has been integrated in Sentence Transformers, and thus also in projects that use Sentence Transformers behind the scenes, such as LangChain, LlamaIndex, Haystack, and txtai. See the examples below to get started with your preferred framework.\n\nFor production deployments, you can use Text Embeddings Inference (TEI) to serve the model efficiently on various hardware configurations, and you can use Transformers.js for use in web applications.\n\nRegardless of your framework choice, you should be mindful of the prompts. For embedding models, prompts are prepended to the input text to allow the model to distinguish between different tasks. EmbeddingGemma was trained with these prompt names and prompts, so they should also be included when using the model:\n\nquery : \"task: search result | query: \" ,\n\n: , document : \"title: none | text: \" ,\n\n: , BitextMining : \"task: search result | query: \" ,\n\n: , Clustering : \"task: clustering | query: \" ,\n\n: , Classification : \"task: classification | query: \" ,\n\n: , InstructionRetrieval : \"task: code retrieval | query: \" ,\n\n: , MultilabelClassification : \"task: classification | query: \" ,\n\n: , PairClassification : \"task: sentence similarity | query: \" ,\n\n: , Reranking : \"task: search result | query: \" ,\n\n: , Retrieval-query : \"task: search result | query: \" ,\n\n: , Retrieval-document : \"title: none | text: \" ,\n\n: , STS : \"task: sentence similarity | query: \" ,\n\n: , Summarization : \"task: summarization | query: \"\n\nIn Sentence Transformers, the query and document prompts are used automatically when calling model.encode_query and model.encode_document , but for other frameworks you might have to: $\n\nspecify prompt names (e.g. \"Reranking\"), specify prompt strings (e.g. \"task: search result | query: \"), or manually prepend the prompts to your input text.\n\nThe following example scripts will demonstrate this with various frameworks.\n\nSentence Transformers\n\nYou will need to install the following packages:\n\npip install git+https://github.com/huggingface/transformers@v4.56.0-Embedding-Gemma-preview pip install sentence-transformers>=5.0.0\n\nRetrieval\n\nInference using Sentence Transformers is rather simple, see this example for semantic search:\n\nfrom sentence_transformers import SentenceTransformer model = SentenceTransformer( \"google/embeddinggemma-300m\" ) query = \"Which planet is known as the Red Planet?\" documents = [ \"Venus is often called Earth's twin because of its similar size and proximity.\" , \"Mars, known for its reddish appearance, is often referred to as the Red Planet.\" , \"Jupiter, the largest planet in our solar system, has a prominent red spot.\" , \"Saturn, famous for its rings, is sometimes mistaken for the Red Planet.\" ] query_embeddings = model.encode_query(query) document_embeddings = model.encode_document(documents) print (query_embeddings.shape, document_embeddings.shape) similarities = model.similarity(query_embeddings, document_embeddings) print (similarities) ranking = similarities.argsort(descending= True )[ 0 ] print (ranking)\n\nClick to see non-retrieval code If you\u2019re not looking to use this model for Information Retrieval, then you\u2019re likely best off using the most general encode method together with the model prompt that best describes your downstream task out of these options: BitextMining : Find translated sentence pairs in two languages.\n\n: Find translated sentence pairs in two languages. Clustering : Find similar texts to group them together.\n\n: Find similar texts to group them together. Classification : Assign predefined labels to texts.\n\n: Assign predefined labels to texts. InstructionRetrieval : Retrieve relevant code snippets based on natural language instructions.\n\n: Retrieve relevant code snippets based on natural language instructions. MultilabelClassification : Assign multiple labels to texts.\n\n: Assign multiple labels to texts. PairClassification : Assign predefined labels to texts.\n\n: Assign predefined labels to texts. Reranking : Reorder search results based on relevance.\n\n: Reorder search results based on relevance. Retrieval-query : Retrieve documents based on a query.\n\n: Retrieve documents based on a query. Retrieval-document : Retrieve documents based on their content.\n\n: Retrieve documents based on their content. STS : Compute semantic textual similarity between texts.\n\n: Compute semantic textual similarity between texts. Summarization : Generate concise summaries of texts. from sentence_transformers import SentenceTransformer model = SentenceTransformer( \"google/embeddinggemma-300m\" ) print (model.prompts) texts = [ \"The weather is beautiful today.\" , \"It's a lovely day outside.\" , \"The stock market crashed yesterday.\" , \"I enjoy programming with Python.\" ] embeddings = model.encode(texts, prompt_name= \"STS\" ) print (embeddings.shape) similarities = model.similarity(embeddings, embeddings) print (similarities) \"\"\" tensor([[1.0000, 0.9305, 0.4660, 0.4326], [0.9305, 1.0000, 0.4227, 0.4434], [0.4660, 0.4227, 1.0000, 0.2638], [0.4326, 0.4434, 0.2638, 1.0000]]) \"\"\" Sentence Transformers encode method documentation\n\nmethod documentation Sentence Transformers similarity method documentation\n\nClick to see how to truncate embedding dimensionality for faster and cheaper search Because google/embeddinggemma-300m was trained with MRL, the embeddings generated by this model can be truncated to lower dimensionalities without considerably hurting the evaluation performance. Embeddings with lower dimensionalities are both cheaper to store on disk and in memory, as well as faster for downstream tasks like retrieval, clustering, or classification. In Sentence Transformers, you can set a lower dimensionality using the truncate_dim parameter on either the SentenceTransformer initialization or when calling model.encode / model.encode_query / model.encode_document : from sentence_transformers import SentenceTransformer model = SentenceTransformer( \"google/embeddinggemma-300m\" , truncate_dim= 256 ) query = \"Which planet is known as the Red Planet?\" documents = [ \"Venus is often called Earth's twin because of its similar size and proximity.\" , \"Mars, known for its reddish appearance, is often referred to as the Red Planet.\" , \"Jupiter, the largest planet in our solar system, has a prominent red spot.\" , \"Saturn, famous for its rings, is sometimes mistaken for the Red Planet.\" ] query_embeddings = model.encode_query(query) document_embeddings = model.encode_document(documents) print (query_embeddings.shape, document_embeddings.shape) similarities = model.similarity(query_embeddings, document_embeddings) print (similarities) ranking = similarities.argsort(descending= True )[ 0 ] print (ranking) Note that the ranking is preserved despite using 3x smaller embeddings compared to the full-sized embeddings. Sentence Transformers Matryoshka Embeddings documentation\n\nLangChain\n\nIf you prefer, you can also use the LangChain HuggingFaceEmbeddings , which uses Sentence Transformers behind the scenes. Note that you'll have to tell LangChain to use the prompts called \"query\" and \"document\" for queries and documents, respectively. This example involves a simple information retrieval setup, but the same embedding model can be used in more complex scenarios.\n\nYou will need to install the following packages:\n\npip install git+https://github.com/huggingface/transformers@v4.56.0-Embedding-Gemma-preview pip install sentence-transformers pip install langchain pip install langchain-community pip install langchain-huggingface pip install faiss-cpu\n\nfrom langchain.docstore.document import Document from langchain_community.vectorstores import FAISS from langchain_huggingface.embeddings import HuggingFaceEmbeddings embedder = HuggingFaceEmbeddings( model_name= \"google/embeddinggemma-300m\" , query_encode_kwargs={ \"prompt_name\" : \"query\" }, encode_kwargs={ \"prompt_name\" : \"document\" } ) data = [ \"Venus is often called Earth's twin because of its similar size and proximity.\" , \"Mars, known for its reddish appearance, is often referred to as the Red Planet.\" , \"Jupiter, the largest planet in our solar system, has a prominent red spot.\" , \"Saturn, famous for its rings, is sometimes mistaken for the Red Planet.\" ] documents = [Document(page_content=text, metadata={ \"id\" : i}) for i, text in enumerate (data)] vector_store = FAISS.from_documents(documents, embedder, distance_strategy= \"MAX_INNER_PRODUCT\" ) query = \"Which planet is known as the Red Planet?\" results = vector_store.similarity_search_with_score(query, k= 3 ) for doc, score in results: print ( f\"Text: {doc.page_content} (score: {score: .4 f} )\" ) \"\"\" Text: Mars, known for its reddish appearance, is often referred to as the Red Planet. (score: 0.6359) Text: Jupiter, the largest planet in our solar system, has a prominent red spot. (score: 0.4930) Text: Saturn, famous for its rings, is sometimes mistaken for the Red Planet. (score: 0.4889) \"\"\"\n\nLlamaIndex\n\nEmbeddingGemma is also supported in LlamaIndex as it uses Sentence Transformers under the hood. For the correct behaviour, you need to specify the query and document prompts as defined in the model configuration. Otherwise, your performance will be suboptimal. This script shows a rudimentary example of using EmbeddingGemma with LlamaIndex, but you can use the HuggingFaceEmbedding class in more difficult settings also.\n\nYou will need to install the following packages:\n\npip install git+https://github.com/huggingface/transformers@v4.56.0-Embedding-Gemma-preview pip install sentence-transformers pip install llama-index pip install llama-index-embeddings-huggingface pip install llama-index-vector-stores-faiss\n\nimport faiss from llama_index.core.schema import TextNode from llama_index.core.vector_stores import VectorStoreQuery from llama_index.embeddings.huggingface import HuggingFaceEmbedding from llama_index.vector_stores.faiss import FaissVectorStore embeddings = HuggingFaceEmbedding( model_name= \"google/embeddinggemma-300m\" , query_instruction= \"task: search result | query: \" , text_instruction= \"title: none | text: \" , ) data = [ \"Venus is often called Earth's twin because of its similar size and proximity.\" , \"Mars, known for its reddish appearance, is often referred to as the Red Planet.\" , \"Jupiter, the largest planet in our solar system, has a prominent red spot.\" , \"Saturn, famous for its rings, is sometimes mistaken for the Red Planet.\" ] store = FaissVectorStore(faiss_index=faiss.IndexFlatIP( 768 )) store.add([TextNode( id =i, text=text, embedding=embeddings.get_text_embedding(text)) for i, text in enumerate (data)]) query = \"Which planet is known as the Red Planet?\" query_embedding = embeddings.get_query_embedding(query) results = store.query(VectorStoreQuery(query_embedding=query_embedding, similarity_top_k= 3 )) for idx, score in zip (results.ids, results.similarities): print ( f\"Text: {data[ int (idx)]} (score: {score: .4 f} )\" ) \"\"\" Text: Mars, known for its reddish appearance, is often referred to as the Red Planet. (score: 0.6359) Text: Jupiter, the largest planet in our solar system, has a prominent red spot. (score: 0.4930) Text: Saturn, famous for its rings, is sometimes mistaken for the Red Planet. (score: 0.4889) \"\"\"\n\nHaystack\n\nEmbeddingGemma can also be used with Haystack, a framework for building production-ready search and language applications. Like LangChain and LlamaIndex, Haystack uses Sentence Transformers behind the scenes and requires you to specify the appropriate prompts. The following example shows how to set up a basic retrieval pipeline using EmbeddingGemma with Haystack.\n\nYou will need to install the following packages:\n\npip install git+https://github.com/huggingface/transformers@v4.56.0-Embedding-Gemma-preview pip install sentence-transformers pip install haystack-ai\n\nfrom haystack import Document, Pipeline from haystack.components.embedders import SentenceTransformersDocumentEmbedder, SentenceTransformersTextEmbedder from haystack.components.retrievers import InMemoryEmbeddingRetriever from haystack.document_stores.in_memory import InMemoryDocumentStore document_store = InMemoryDocumentStore() document_embedder = SentenceTransformersDocumentEmbedder( model= \"google/embeddinggemma-300m\" , encode_kwargs={ \"prompt_name\" : \"document\" } ) query_embedder = SentenceTransformersTextEmbedder( model= \"google/embeddinggemma-300m\" , encode_kwargs={ \"prompt_name\" : \"query\" } ) document_embedder.warm_up() query_embedder.warm_up() data = [ \"Venus is often called Earth's twin because of its similar size and proximity.\" , \"Mars, known for its reddish appearance, is often referred to as the Red Planet.\" , \"Jupiter, the largest planet in our solar system, has a prominent red spot.\" , \"Saturn, famous for its rings, is sometimes mistaken for the Red Planet.\" , ] documents = [Document(content=text, id = str (i)) for i, text in enumerate (data)] documents_with_embeddings = document_embedder.run(documents=documents)[ \"documents\" ] document_store.write_documents(documents_with_embeddings) query_pipeline = Pipeline() query_pipeline.add_component( \"text_embedder\" , query_embedder) query_pipeline.add_component( \"retriever\" , InMemoryEmbeddingRetriever(document_store=document_store, top_k= 3 )) query_pipeline.connect( \"text_embedder.embedding\" , \"retriever.query_embedding\" ) query = \"Which planet is known as the Red Planet?\" results = query_pipeline.run({ \"text_embedder\" : { \"text\" : query}}) for document in results[ \"retriever\" ][ \"documents\" ]: print ( f\"Text: {document.content} (score: {document.score: .4 f} )\" ) \"\"\" Text: Mars, known for its reddish appearance, is often referred to as the Red Planet. (score: 0.6359) Text: Jupiter, the largest planet in our solar system, has a prominent red spot. (score: 0.4930) Text: Saturn, famous for its rings, is sometimes mistaken for the Red Planet. (score: 0.4889) \"\"\"\n\ntxtai\n\ntxtai is also compatible with EmbeddingGemma. Like other frameworks, txtai utilizes Sentence Transformers under the hood and needs the appropriate prompts for optimal performance with EmbeddingGemma. The following example demonstrates how to set up a basic retrieval system with txtai.\n\nYou will need to install the following packages:\n\npip install git+https://github.com/huggingface/transformers@v4.56.0-Embedding-Gemma-preview pip install sentence-transformers pip install txtai\n\nfrom txtai import Embeddings embeddings = Embeddings( path= \"google/embeddinggemma-300m\" , method= \"sentence-transformers\" , instructions={ \"query\" : \"task: search result | query: \" , \"data\" : \"title: none | text: \" , } ) data = [ \"Venus is often called Earth's twin because of its similar size and proximity.\" , \"Mars, known for its reddish appearance, is often referred to as the Red Planet.\" , \"Jupiter, the largest planet in our solar system, has a prominent red spot.\" , \"Saturn, famous for its rings, is sometimes mistaken for the Red Planet.\" ] embeddings.index(data) query = \"Which planet is known as the Red Planet?\" results = embeddings.search(query, 3 ) for idx, score in results: print ( f\"Text: {data[ int (idx)]} (score: {score: .4 f} )\" ) \"\"\" Text: Mars, known for its reddish appearance, is often referred to as the Red Planet. (score: 0.6359) Text: Jupiter, the largest planet in our solar system, has a prominent red spot. (score: 0.4930) Text: Saturn, famous for its rings, is sometimes mistaken for the Red Planet. (score: 0.4889) \"\"\"\n\nYou can even run EmbeddingGemma 100% locally in your browser with Transformers.js! If you haven't already, you can install the library from NPM using:\n\nnpm i @huggingface/transformers\n\nYou can then compute embeddings as follows:\n\nimport { AutoModel , AutoTokenizer , matmul } from \"@huggingface/transformers\" ; const model_id = \"onnx-community/embeddinggemma-300m-ONNX\" ; const tokenizer = await AutoTokenizer . from_pretrained (model_id); const model = await AutoModel . from_pretrained (model_id, { dtype : \"fp32\" , }); const prefixes = { query : \"task: search result | query: \" , document : \"title: none | text: \" , }; const query = prefixes. query + \"Which planet is known as the Red Planet?\" ; const documents = [ \"Venus is often called Earth's twin because of its similar size and proximity.\" , \"Mars, known for its reddish appearance, is often referred to as the Red Planet.\" , \"Jupiter, the largest planet in our solar system, has a prominent red spot.\" , \"Saturn, famous for its rings, is sometimes mistaken for the Red Planet.\" , ]. map ( ( x ) => prefixes. document + x); const inputs = await tokenizer ([query, ...documents], { padding : true }); const { sentence_embedding } = await model (inputs); const scores = await matmul (sentence_embedding, sentence_embedding. transpose ( 1 , 0 )); const similarities = scores. tolist ()[ 0 ]. slice ( 1 ); console . log (similarities); const ranking = similarities. map ( ( score, index ) => ({ index, score })). sort ( ( a, b ) => b. score - a. score ); console . log (ranking);\n\nText Embeddings Inference\n\nYou can easily deploy EmbeddingGemma for both development and production using Text Embeddings Inference (TEI) version 1.8.1 or later.\n\nCPU:\n\ndocker run -p 8080:80 ghcr.io/huggingface/text-embeddings-inference:cpu-1.8.1 --model-id google/embeddinggemma-300m --dtype float32\n\nCPU with ONNX Runtime:\n\ndocker run -p 8080:80 ghcr.io/huggingface/text-embeddings-inference:cpu-1.8.1 --model-id onnx-community/embeddinggemma-300m-ONNX --dtype float32 --pooling mean\n\nNVIDIA CUDA:\n\ndocker run --gpus all --shm-size 1g -p 8080:80 ghcr.io/huggingface/text-embeddings-inference:cuda-1.8.1 --model-id google/embeddinggemma-300m --dtype float32\n\nIf you run the Docker container with the cuda-1.8.1 tag, it includes support for multiple GPU architectures: Turing, Ampere, Ada Lovelace, and Hopper. For a lighter image tailored to just your GPU, you can instead use a specific tag such as turing-1.8.1 , 1.8.1 and 86-1.8.1 (Ampere), 89-1.8.1 (Ada Lovelace), or hopper-1.8.1 .\n\nOnce deployed, regardless of the device or runtime, you can leverage the /v1/embeddings endpoint based on the OpenAI Embeddings API Specification to generate embeddings.\n\ncurl http://0.0.0.0:8080/v1/embeddings -H \"Content-Type: application/json\" -d '{\"model\":\"google/embeddinggemma-300m\",\"input\":[\"task: search result | query: Which planet is known as the Red Planet?\",\"task: search result | query: Where did Amelia Earhart first fly?\"]}'\n\nAlternatively, you can also leverage the /embed endpoint from the Text Embeddings Inference Embeddings API, which supports the prompt_name parameter, meaning there\u2019s no need to manually prepend the prompt to the inputs but select it via prompt_name instead.\n\ncurl http://0.0.0.0:8080/embed -H \"Content-Type: application/json\" -d '{\"inputs\":[\"Which planet is known as the Red Planet?\",\"Where did Amelia Earthart first fly?\"],\"prompt_name\":\"query\",\"normalize\":true}'\n\nAdditionally, note that since google/embeddinggemma-300m was trained with Matryoshka Representation Learning (MRL), you can also leverage the dimensions parameter, on both /v1/embeddings and /embed , to truncate the embeddings to lower dimensionalities (512, 256, and 128) without hurting the evaluation performance.\n\nONNX Runtime\n\nYou can also run the model directly with ONNX Runtime, making it highly portable and cross-platform compatible. The example below shows usage in Python, but the same approach can be applied in other languages (Java, C#, C++, etc.) as well.\n\nfrom huggingface_hub import hf_hub_download import onnxruntime as ort from transformers import AutoTokenizer model_id = \"onnx-community/embeddinggemma-300m-ONNX\" model_path = hf_hub_download(model_id, subfolder= \"onnx\" , filename= \"model.onnx\" ) hf_hub_download(model_id, subfolder= \"onnx\" , filename= \"model.onnx_data\" ) session = ort.InferenceSession(model_path) tokenizer = AutoTokenizer.from_pretrained(model_id) prefixes = { \"query\" : \"task: search result | query: \" , \"document\" : \"title: none | text: \" , } query = prefixes[ \"query\" ] + \"Which planet is known as the Red Planet?\" documents = [ \"Venus is often called Earth's twin because of its similar size and proximity.\" , \"Mars, known for its reddish appearance, is often referred to as the Red Planet.\" , \"Jupiter, the largest planet in our solar system, has a prominent red spot.\" , \"Saturn, famous for its rings, is sometimes mistaken for the Red Planet.\" ] documents = [prefixes[ \"document\" ] + x for x in documents] inputs = tokenizer([query] + documents, padding= True , return_tensors= \"np\" ) _, sentence_embedding = session.run( None , inputs.data) print (sentence_embedding.shape) query_embeddings = sentence_embedding[ 0 ] document_embeddings = sentence_embedding[ 1 :] similarities = query_embeddings @ document_embeddings.T print (similarities) ranking = similarities.argsort()[::- 1 ] print (ranking)\n\nFinetuning\n\nAs with all models compatible with the Sentence Transformers library, EmbeddingGemma can be easily fine-tuned on your specific dataset. To showcase this, we'll be finetuning google/embeddinggemma-300m on the Medical Instruction and RetrIeval Dataset (MIRIAD) dataset, such that our finetuned model becomes particularly adept at finding passages up to 1000 tokens from scientific medical papers given detailed medical questions. These passages can be used as crucial context for a generative model to answer questions more effectively.\n\nBelow, you can explore each key component of the finetuning process using expandable tabs. Each tab contains the relevant code and a detailed explanation.\n\nFull Finetuning Script\n\nBelow is the complete script, combining all components above:\n\nimport logging import traceback from datasets import load_dataset from sentence_transformers import ( SentenceTransformer, SentenceTransformerModelCardData, SentenceTransformerTrainer, SentenceTransformerTrainingArguments, ) from sentence_transformers.evaluation import InformationRetrievalEvaluator from sentence_transformers.losses import CachedMultipleNegativesRankingLoss from sentence_transformers.training_args import BatchSamplers logging.basicConfig( format = \"%(asctime)s - %(message)s\" , datefmt= \"%Y-%m-%d %H:%M:%S\" , level=logging.INFO) model = SentenceTransformer( \"google/embeddinggemma-300m\" , model_card_data=SentenceTransformerModelCardData( language= \"en\" , license= \"apache-2.0\" , model_name= \"EmbeddingGemma-300m trained on the Medical Instruction and RetrIeval Dataset (MIRIAD)\" , ), ) train_dataset = load_dataset( \"tomaarsen/miriad-4.4M-split\" , split= \"train\" ).select( range ( 100_000 )) eval_dataset = load_dataset( \"tomaarsen/miriad-4.4M-split\" , split= \"eval\" ).select( range ( 1_000 )) test_dataset = load_dataset( \"tomaarsen/miriad-4.4M-split\" , split= \"test\" ).select( range ( 1_000 )) loss = CachedMultipleNegativesRankingLoss(model, mini_batch_size= 8 ) run_name = \"embeddinggemma-300m-medical-100k\" args = SentenceTransformerTrainingArguments( output_dir= f\"models/ {run_name} \" , num_train_epochs= 1 , per_device_train_batch_size= 128 , per_device_eval_batch_size= 128 , learning_rate= 2e-5 , warmup_ratio= 0.1 , fp16= True , bf16= False , batch_sampler=BatchSamplers.NO_DUPLICATES, prompts={ \"question\" : model.prompts[ \"query\" ], \"passage_text\" : model.prompts[ \"document\" ], }, eval_strategy= \"steps\" , eval_steps= 100 , save_strategy= \"steps\" , save_steps= 100 , save_total_limit= 2 , logging_steps= 20 , run_name=run_name, ) queries = dict ( enumerate (eval_dataset[ \"question\" ])) corpus = dict ( enumerate (eval_dataset[ \"passage_text\" ] + train_dataset[ \"passage_text\" ][: 30_000 ])) relevant_docs = {idx: [idx] for idx in queries} dev_evaluator = InformationRetrievalEvaluator( queries=queries, corpus=corpus, relevant_docs=relevant_docs, name= \"miriad-eval-1kq-31kd\" , show_progress_bar= True , ) dev_evaluator(model) trainer = SentenceTransformerTrainer( model=model, args=args, train_dataset=train_dataset, eval_dataset=eval_dataset, loss=loss, evaluator=dev_evaluator, ) trainer.train() dev_evaluator(model) queries = dict ( enumerate (test_dataset[ \"question\" ])) corpus = dict ( enumerate (test_dataset[ \"passage_text\" ] + train_dataset[ \"passage_text\" ][: 30_000 ])) relevant_docs = {idx: [idx] for idx in queries} test_evaluator = InformationRetrievalEvaluator( queries=queries, corpus=corpus, relevant_docs=relevant_docs, name= \"miriad-test-1kq-31kd\" , show_progress_bar= True , ) test_evaluator(model) final_output_dir = f\"models/ {run_name} /final\" model.save_pretrained(final_output_dir) try : model.push_to_hub(run_name) except Exception: logging.error( f\"Error uploading model to the Hugging Face Hub:\n\n{traceback.format_exc()} To upload it manually, you can run \" f\"`huggingface-cli login`, followed by loading the model using `model = SentenceTransformer( {final_output_dir!r} )` \" f\"and saving it using `model.push_to_hub(' {run_name} ')`.\" )\n\nTraining\n\nWe ran the full training script on an RTX 3090 with 24GB of VRAM, and the completed training and evaluating scripts took 5.5 hours. If desired, you can further reduce the memory footprint by reducing mini_batch_size on the CachedMultipleNegativesRankingLoss and batch_size on the InformationRetrievalEvaluator instances. See here the logs from our training run:\n\nEpoch Step Training Loss Validation Loss miriad-eval-1kq-31kd_cosine_ndcg@10 miriad-test-1kq-31kd_cosine_ndcg@10 -1 -1 - - 0.8474 0.8340 0.0256 20 0.1019 - - - 0.0512 40 0.0444 - - - 0.0767 60 0.0408 - - - 0.1023 80 0.0462 - - - 0.1279 100 0.0542 0.0525 0.8616 - 0.1535 120 0.0454 - - - 0.1790 140 0.0403 - - - 0.2046 160 0.0463 - - - 0.2302 180 0.0508 - - - 0.2558 200 0.0497 0.0449 0.8643 - 0.2813 220 0.0451 - - - 0.3069 240 0.0445 - - - 0.3325 260 0.0489 - - - 0.3581 280 0.0452 - - - 0.3836 300 0.0461 0.0406 0.8832 - 0.4092 320 0.0415 - - - 0.4348 340 0.04 - - - 0.4604 360 0.0399 - - - 0.4859 380 0.0423 - - - 0.5115 400 0.0352 0.0316 0.8823 - 0.5371 420 0.0408 - - - 0.5627 440 0.0356 - - - 0.5882 460 0.0371 - - - 0.6138 480 0.0276 - - - 0.6394 500 0.028 0.0280 0.8807 - 0.6650 520 0.0302 - - - 0.6905 540 0.0345 - - - 0.7161 560 0.0325 - - - 0.7417 580 0.033 - - - 0.7673 600 0.0314 0.0264 0.8910 - 0.7928 620 0.033 - - - 0.8184 640 0.029 - - - 0.8440 660 0.0396 - - - 0.8696 680 0.0266 - - - 0.8951 700 0.0262 0.0240 0.8968 - 0.9207 720 0.0262 - - - 0.9463 740 0.0327 - - - 0.9719 760 0.0293 - - - 0.9974 780 0.0304 - - - -1 -1 - - 0.9026 0.8862\n\nFinetuned Evaluation\n\nThe performance of the base model was already excellent, with a strong 0.8340 NDCG@10 on our MIRIAD test set. Despite that, we were able to increase it considerably on this domain-specific dataset.\n\nModel Number of Parameters NDCG@10 on miriad-test-1kq-31kd BAAI/bge-base-en-v1.5 109M 0.7541 intfloat/multilingual-e5-small 118M 0.6852 ibm-granite/granite-embedding-125m-english 125M 0.7745 Snowflake/snowflake-arctic-embed-m-long 137M 0.7514 intfloat/multilingual-e5-base 278M 0.7052 Snowflake/snowflake-arctic-embed-m-v2.0 305M 0.8467 BAAI/bge-large-en-v1.5 335M 0.7727 mixedbread-ai/mxbai-embed-large-v1 335M 0.7851 intfloat/multilingual-e5-large 560M 0.7318 Snowflake/snowflake-arctic-embed-l-v2.0 568M 0.8433 Qwen/Qwen3-Embedding-0.6B 596M 0.8493 google/embeddinggemma-300m (base) 268M 0.8340 sentence-transformers/embeddinggemma-300m-medical (fine-tuned) 268M 0.8862\n\nOur fine-tuning process achieved a significant improvement of +0.0522 NDCG@10 on the test set, resulting in a model that comfortably outperforms any existing general-purpose embedding model on our specific task, at this model size. Additional time and compute investment would allow for even stronger results, such as hard negatives mining or training with more than 100k data pairs.",
    "link": "https://huggingface.co/blog/embeddinggemma",
    "Summary": "Welcome EmbeddingGemma, Google's new efficient embedding modelPublished September 4, 2025 Update on GitHubToday, Google releases EmbeddingGemma, a state-of-the-art multilingual embedding model perfect for on-device use cases.\nBuilding on this foundation, Google DeepMind\u2019s EmbeddingGemma arrives as the newest, most capable small multilingual embedding model yet.\nBeyond the new Gemma3-based encoder backbone, which produces token embeddings, a mean pooling layer converts these token embeddings into text embeddings.\nLastly, two dense layers transform the text embeddings into their final form, a 768-dimensional vector.\nlog (ranking);Text Embeddings InferenceYou can easily deploy EmbeddingGemma for both development and production using Text Embeddings Inference (TEI) version 1.8.1 or later.",
    "Keywords": [
      "embeddinggemma",
      "import",
      "install",
      "googles",
      "efficient",
      "welcome",
      "red",
      "query",
      "embeddings",
      "known",
      "planet",
      "embedding",
      "text",
      "model",
      "score"
    ]
  },
  {
    "Title": "Generate Images with Claude and Hugging Face",
    "Authors": [],
    "Publish Date": null,
    "Text": "Generate Images with Claude and Hugging Face\n\nPublished August 19, 2025 Update on GitHub\n\nIntroduction\n\nNatural Images with Flux.1 Krea Dev\n\nQwen Image\n\nConclusion TL;DR: It's easier than ever to generate detailed pictures with state-of-the-art AI models by connecting Claude to Hugging Face Spaces. This article describes how and why, and introduces recently launched models which excel at producing natural images or images that include text.\n\nUpdate October 2025: Following an update to Anthropic's Connector Directory Policy, you will need to use the \"Add custom connector\" option and set the Remote MCP server URL to https://huggingface.co/mcp?login to use Image Generation and other Gradio Spaces from Claude.\n\nIntroduction\n\nRecent advances in image generation models have improved their ability to produce realistic outputs and incorporate high quality text. It's easier than ever to use these models by connecting them directly to Claude.\n\nThe advantages of generating pictures this way are:\n\nThe AI can assist in building detailed prompts that may improve the quality of generated images.\n\nThe AI can \"see\" the generated images, then help iterate on designs and techniques to get perfect results.\n\nYou can easily swap in the latest models or the one best suited for your needs.\n\nTo get started, create a free Hugging Face account, then connect Claude from its \"Search and tools\" menu in the chat input box. The video below shows the exact steps needed:\n\nConnecting Claude to Hugging Face\n\nBehind the scenes, Claude is now set up to use tools from the Hugging Face MCP Server, seamlessly extending its capabilities. This includes the latest AI Applications running on our innovative ZeroGPU powered Spaces. Your Hugging Face account provides free credits to use these large, powerful models.\n\nAfter Claude has been connected with Hugging Face, we'll show how to configure image generation tools that Claude can use.\n\nNatural Images with Flux.1 Krea Dev\n\nFLUX.1 Krea [dev] aims to eliminate the telltale \"AI look\" that often plagues generated images - for example plastic skin, oversaturated colors, or overly smooth textures. If you want images that look like they were shot by a professional photographer rather than generated by a computer, Krea delivers the realistic textures, natural lighting, and authentic aesthetics that other AI models struggle with. You can read more about how they did this on their blog.\n\nExamples from the Krea Image Generator\n\nTo use Krea with Claude, go to huggingface.co/mcp/settings and add mcp-tools/FLUX.1-Krea-dev to your \"Spaces Tools\". That's all you need to enable Claude to generate beautiful, realistic looking images.\n\nAdding a Space in MCP Settings\n\nYou can then try a prompt in Claude like:\n\n\"Use Krea to create an image of a vibrant garden with victorian house\".\n\nTo begin generating pictures.\n\nQwen Image\n\nQwen-Image is a powerful AI image generator that excels at following prompts and accurate text rendering, making it ideal for designing posters, signs, infographics, and marketing materials where text quality matters. Read more about the Qwen-Image model from their blog post.\n\nExamples from the Qwen-Image Generator\n\nTo use Qwen-Image add mcp-tools/qwen-image from your MCP Servers setting page, then check it is enabled in Claude to start using it.\n\nQwen-Image comes with a Prompt Enhancer that helps write detailed prompts suitable for it. To try it, select \"Qwen Prompt Enhancer\" from the prompts menu and enter your idea before submitting it to Claude.\n\nUsing the Qwen Prompt Enhancer from Claude\n\nYou can even keep both Krea and Qwen-Image enabled and ask Claude to use both so you can compare results. For example: \"Use Krea and Qwen to generate a street scene with 'Hugging Face' graffiti sprayed on the wall\".\n\nHugging Face Graffiti Street Scene\n\nConclusion\n\nConnecting Claude to Hugging Face Spaces makes using state-of-the-art models as easy as clicking a button, even the day new models are released. Explore the AI App Directory at https://huggingface.co/spaces and build amazing projects with Video Generation, Web Search, Image Editing and many thousands more. With a Pro account you get increased usage limits and more. Let us know what you find and create in the comments below!",
    "link": "https://huggingface.co/blog/claude-and-mcp",
    "Summary": "Generate Images with Claude and Hugging FacePublished August 19, 2025 Update on GitHubIntroductionNatural Images with Flux.1 Krea DevQwen ImageConclusion TL;DR: It's easier than ever to generate detailed pictures with state-of-the-art AI models by connecting Claude to Hugging Face Spaces.\nThe video below shows the exact steps needed:Connecting Claude to Hugging FaceBehind the scenes, Claude is now set up to use tools from the Hugging Face MCP Server, seamlessly extending its capabilities.\nAfter Claude has been connected with Hugging Face, we'll show how to configure image generation tools that Claude can use.\nFor example: \"Use Krea and Qwen to generate a street scene with 'Hugging Face' graffiti sprayed on the wall\".\nHugging Face Graffiti Street SceneConclusionConnecting Claude to Hugging Face Spaces makes using state-of-the-art models as easy as clicking a button, even the day new models are released.",
    "Keywords": [
      "images",
      "models",
      "image",
      "claude",
      "face",
      "hugging",
      "ai",
      "krea",
      "generate",
      "tools",
      "spaces"
    ]
  },
  {
    "Title": "MCP for Research: How to Connect AI to Research Tools",
    "Authors": [],
    "Publish Date": null,
    "Text": "MCP for Research: How to Connect AI to Research Tools\n\nPublished August 18, 2025 Update on GitHub\n\nAcademic research involves frequent: finding papers, code, related models and datasets. This typically means switching between platforms like arXiv GitHub , and Hugging Face , manually piecing together connections.\n\nThe Model Context Protocol (MCP) is a standard that allows agentic models to communicate with external tools and data sources. For research discovery, this means AI can use research tools through natural language requests, automating platform switching and cross-referencing.\n\nResearch Discovery: Three Layers of Abstraction\n\nMuch like software development, research discovery can be framed in terms of layers of abstraction.\n\n1. Manual Research\n\nAt the lowest level of abstraction, researchers search manually and cross-reference by hand.\n\n1. Find paper on arXiv 2. Search GitHub for implementations 3. Check Hugging Face for models/datasets 4. Cross-reference authors and citations 5. Organize findings manually\n\nThis manual approach becomes inefficient when tracking multiple research threads or conducting systematic literature reviews. The repetitive nature of searching across platforms, extracting metadata, and cross-referencing information naturally leads to automation through scripting.\n\n2. Scripted Tools\n\nPython scripts automate research discovery by handling web requests, parsing responses, and organizing results.\n\ndef gather_research_info ( paper_url ): paper_data = scrape_arxiv(paper_url) github_repos = search_github(paper_data[ 'title' ]) hf_models = search_huggingface(paper_data[ 'authors' ]) return consolidate_results(paper_data, github_repos, hf_models) results = gather_research_info( \"https://arxiv.org/abs/2103.00020\" )\n\nThe research tracker demonstrates systematic research discovery built from these types of scripts.\n\nWhile scripts are faster than manual research, they often fail to automatically collect data due to changing APIs, rate limits, or parsing errors. Without human oversight, scripts may miss relevant results or return incomplete information.\n\n3. MCP Integration\n\nMCP makes these same Python tools accessible to AI systems through natural language.\n\n# Example research directive Find recent transformer architecture papers published in the last 6 months: - Must have available implementation code - Focus on papers with pretrained models - Include performance benchmarks when available\n\nThe AI orchestrates multiple tools, fills information gaps, and reasons about results:\n\nuser: \"Find all relevant information (code, models, etc.) on this paper: https://huggingface.co/papers/2010.11929\" ai:\n\nThis can be viewed as an additional layer of abstraction above scripting, where the \"programming language\" is natural language. This follows the Software 3.0 Analogy, where the natural language research direction is the software implementation.\n\nThis comes with the same caveats as scripting:\n\nFaster than manual research, but error-prone without human guidance\n\nQuality depends on the implementation\n\nUnderstanding the lower layers (both manual and scripted) leads to better implementations\n\nSetup and Usage\n\nQuick Setup\n\nThe easiest way to add the Research Tracker MCP is through Hugging Face MCP Settings:\n\nVisit huggingface.co/settings/mcp Search for \"research-tracker-mcp\" in the available tools Click to add it to your tools Follow the provided setup instructions for your specific client (Claude Desktop, Cursor, Claude Code, VS Code, etc.)\n\nThis workflow leverages the Hugging Face MCP server, which is the standard way to use Hugging Face Spaces as MCP tools. The settings page provides client-specific configuration that's automatically generated and always up-to-date.\n\nLearn More\n\nGet Started:\n\nHugging Face MCP Course - Complete guide from basics to building your own tools\n\nMCP Official Documentation - Protocol specifications and architecture\n\nBuild Your Own:\n\nGradio MCP Guide - Turn Python functions into MCP tools\n\nBuilding the Hugging Face MCP Server - Production implementation case study\n\nCommunity:\n\nHugging Face Discord - MCP development discussions\n\nReady to automate your research discovery? Try the Research Tracker MCP or build your own research tools with the resources above.",
    "link": "https://huggingface.co/blog/mcp-for-research",
    "Summary": "MCP for Research: How to Connect AI to Research ToolsPublished August 18, 2025 Update on GitHubAcademic research involves frequent: finding papers, code, related models and datasets.\nFor research discovery, this means AI can use research tools through natural language requests, automating platform switching and cross-referencing.\nResearch Discovery: Three Layers of AbstractionMuch like software development, research discovery can be framed in terms of layers of abstraction.\nThis workflow leverages the Hugging Face MCP server, which is the standard way to use Hugging Face Spaces as MCP tools.\nTry the Research Tracker MCP or build your own research tools with the resources above.",
    "Keywords": [
      "natural",
      "research",
      "manual",
      "language",
      "face",
      "hugging",
      "ai",
      "code",
      "tools",
      "mcp",
      "discovery",
      "connect"
    ]
  },
  {
    "Title": "From Zero to GPU: A Guide to Building and Scaling Production-Ready CUDA Kernels",
    "Authors": [],
    "Publish Date": null,
    "Text": "From Zero to GPU: A Guide to Building and Scaling Production-Ready CUDA Kernels\n\nPublished August 18, 2025 Update on GitHub\n\nCustom CUDA kernels give your models a serious performance edge, but building them for the real world can feel daunting. How do you move beyond a simple GPU function to create a robust, scalable system without getting bogged down by endless build times and dependency nightmares?\n\nWe created the kernel-builder library for this purpose. You can develop a custom kernel locally, and then build it for multiple architectures and make it available for the world to use.\n\nIn this guide we'll show you how to build a complete, modern CUDA kernel from the ground up. Then, we\u2019ll tackle the tough production and deployment challenges, drawing on real-world engineering strategies to show you how to build systems that are not just fast, but also efficient and maintainable.\n\nWhat You\u2019ll Learn\n\nWhen you're done, other developers will be able to use your kernels directly from the hub like this:\n\nimport torch from kernels import get_kernel optimized_kernel = get_kernel( \"your-username/optimized-kernel\" ) some_input = torch.randn(( 10 , 10 ), device= \"cuda\" ) out = optimized_kernel.my_kernel_function(some_input) print (out)\n\nRather watch a video? Check out the YouTube video that accompanies this guide.\n\nLet's Get Started! \ud83d\ude80\n\nPart 1: Anatomy of a Modern CUDA Kernel\n\nLet's build a practical kernel that converts an image from RGB to grayscale. This example uses PyTorch's modern C++ API to register our function as a first-class, native operator.\n\nStep 1: Project Structure\n\nA clean, predictable structure is the foundation of a good project. The Hugging Face Kernel Builder expects your files to be organized like this:\n\nimg2gray/ \u251c\u2500\u2500 build.toml \u251c\u2500\u2500 csrc \u2502 \u2514\u2500\u2500 img2gray.cu \u251c\u2500\u2500 flake.nix \u2514\u2500\u2500 torch-ext \u251c\u2500\u2500 torch_binding.cpp \u251c\u2500\u2500 torch_binding.h \u2514\u2500\u2500 img2gray \u2514\u2500\u2500 __init__.py\n\nbuild.toml : The project manifest; it\u2019s the brain of the build process.\n\n: The project manifest; it\u2019s the brain of the build process. csrc/ : Your raw CUDA source code where the GPU magic happens.\n\n: Your raw CUDA source code where the GPU magic happens. flake.nix : The key to a perfectly reproducible* build environment.\n\n: The key to a perfectly reproducible* build environment. torch-ext/img2gray/ : The Python wrapper for the raw PyTorch operators.\n\nStep 2: The build.toml Manifest\n\nThis file orchestrates the entire build. It tells the kernel-builder what to compile and how everything connects.\n\n[general] name = \"img2gray\" universal = false [torch] src = [ \"torch-ext/torch_binding.cpp\" , \"torch-ext/torch_binding.h\" ] [kernel.img2gray] backend = \"cuda\" depends = [ \"torch\" ] src = [ \"csrc/img2gray.cu\" , ]\n\nStep 3: The flake.nix Reproducibility File\n\nTo ensure anyone can build your kernel on any machine, we use a flake.nix file. It locks the exact version of the kernel-builder and its dependencies, eliminating \"it works on my machine\" issues.\n\n{ description = \"Flake for img2gray kernel\" ; inputs = { kernel-builder. url = \"github:huggingface/kernel-builder\" ; }; outputs = { self, kernel-builder, }: kernel-builder.lib.genFlakeOutputs { path = ./.; rev = self.shortRev or self.dirtyShortRev or self.lastModifiedDate; }; }\n\nStep 4: Writing the CUDA Kernel\n\nNow for the GPU code. Inside csrc/img2gray.cu , we'll define a kernel that uses a 2D grid of threads\u2014a natural and efficient fit for processing images.\n\n__global__ void img2gray_kernel ( const uint8_t * input, uint8_t * output, int width, int height) { int x = blockIdx.x * blockDim.x + threadIdx.x; int y = blockIdx.y * blockDim.y + threadIdx.y; if (x < width && y < height) { int idx = (y * width + x) * 3 ; uint8_t r = input[idx]; uint8_t g = input[idx + 1 ]; uint8_t b = input[idx + 2 ]; uint8_t gray = static_cast < uint8_t >( 0.21f * r + 0.72f * g + 0.07f * b); output[y * width + x] = gray; } } void img2gray_cuda (torch::Tensor const &input, torch::Tensor &output) { const int width = input. size ( 1 ); const int height = input. size ( 0 ); const dim3 blockSize ( 16 , 16 ) ; const dim3 gridSize ((width + blockSize.x - 1 ) / blockSize.x, (height + blockSize.y - 1 ) / blockSize.y) ; img2gray_kernel<<<gridSize, blockSize>>>( input. data_ptr < uint8_t >(), output. data_ptr < uint8_t >(), width, height ); }\n\nStep 5: Registering a Native PyTorch Operator\n\nThis is the most important step. We're not just binding to Python; we're registering our function as a native PyTorch operator. This makes it a first-class citizen in the PyTorch ecosystem, visible under the torch.ops namespace.\n\nThe file torch-ext/torch_binding.cpp handles this registration.\n\nTORCH_LIBRARY_EXPAND (TORCH_EXTENSION_NAME, ops) { ops. def ( \"img2gray(Tensor input, Tensor! output) -> ()\" ); ops. impl ( \"img2gray\" , torch::kCUDA, &img2gray_cuda); } REGISTER_EXTENSION (TORCH_EXTENSION_NAME)\n\nIn simple terms, TORCH_LIBRARY_EXPAND allows us to define our operator in a way that can be easily extended or modified in the future.\n\nWhy This Matters\n\nThis approach is crucial for two main reasons:\n\nCompatibility with torch.compile : By registering our kernel this way, torch.compile can \"see\" it. This allows PyTorch to fuse your custom operator into larger computation graphs, minimizing overhead and maximizing performance. It's the key to making your custom code work seamlessly with PyTorch's broader performance ecosystem.\n\nHardware-Specific Implementations: This system allows you to provide different backends for the same operator. You could add another TORCH_LIBRARY_IMPL(img2gray, CPU, ...) block pointing to a C++ CPU function. PyTorch's dispatcher would then automatically call the correct implementation (CUDA or CPU) based on the input tensor's device, making your code powerful and portable. Setting up the __init__.py wrapper\n\nIn torch-ext/img2gray/ we need an __init__.py file to make this directory a Python package and to expose our custom operator in a user-friendly way.\n\nThe _ops module is auto-generated by kernel-builder from a template to provide a standard namespace for your registered C++ functions.\n\nimport torch from ._ops import ops def img2gray ( input : torch.Tensor ) -> torch.Tensor: height, width, channels = input .shape assert channels == 3 , \"Input image must have 3 channels (RGB)\" output = torch.empty((height, width), device= input .device, dtype= input .dtype) ops.img2gray( input , output) return output\n\nStep 6: Building the Kernel\n\nNow that our kernel and its bindings are ready, it's time to build them. The kernel-builder tool simplifies this process.\n\nYou can build your kernel with a single command, nix build . -L ; however, as developers, we'll want a faster, more iterative workflow. For that, we'll use the nix develop command to enter a development shell with all the necessary dependencies pre-installed.\n\nMore specifically, we can choose the exact CUDA and PyTorch versions we want to use. For example, to build our kernel for PyTorch 2.7 with CUDA 12.6, we can use the following command:\n\nDrop into a Nix Shell\n\nnix develop .\n\nNote that the devShell name above can be deciphered as:\n\nnix develop . \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500 Architecture: x86_64 (Linux) \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 CUDA version: 12.6 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 C++ ABI: cxx11 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Torch version: 2.7\n\nAt this point, we'll be inside a Nix shell with all dependencies installed. We can now build the kernel for this particular architecture and test it. Later on, we'll deal with multiple architectures before distributing the final version of the kernel.\n\nSet Up Build Artifacts\n\nbuild2cmake generate-torch build.toml\n\nThis command creates a handful of files used to build the kernel: CMakeLists.txt , pyproject.toml , setup.py , and a cmake directory. The CMakeLists.txt file is the main entry point for CMake to build the kernel.\n\nCreate a Python Virtual Environment\n\npython -m venv .venv source .venv/bin/activate\n\nNow you can install the kernel in editable mode.\n\nCompile the Kernel and Install the Python Package\n\npip install --no-build-isolation -e .\n\n\ud83d\ude4c Amazing! We now have a custom built kernel that follows best practices for PyTorch bindings, with a fully reproducible build process.\n\nDevelopment Cycle\n\nTo ensure everything is working correctly, we can run a simple test to check that the kernel is registered and it works as expected. If it doesn't, you can iterate by editing the source files and repeating the build, reusing the nix environment you created.\n\nimport torch import img2gray from PIL import Image import numpy as np print ( dir (img2gray)) img = Image. open ( \"kernel-builder-logo-color.png\" ).convert( \"RGB\" ) img = np.array(img) img_tensor = torch.from_numpy(img).cuda() print (img_tensor.shape) gray_tensor = img2gray.img2gray(img_tensor).squeeze() print (gray_tensor.shape) gray_img = Image.fromarray(gray_tensor.cpu().numpy().astype(np.uint8), mode= \"L\" ) gray_img.save( \"kernel-builder-logo-gray.png\" )\n\nStep 7: Sharing with the World\n\nNow that we have a working kernel, it's time to share it with other developers and the world!\n\nOne small thing we'll want to do before we share, is clean up all of the development artifacts that were generated during the build process to avoid uploading unnecessary files.\n\nbuild2cmake clean build.toml\n\nBuilding the Kernel for All PyTorch and CUDA Versions\n\nEarlier, we built the kernel for a specific version of PyTorch and CUDA. However, to make it available to a wider audience, we need to build it for all supported versions. The kernel-builder tool can help us with that.\n\nThis is also where the concept of a compliant kernel comes into play. A compliant kernel is one that can be built and run for all supported versions of PyTorch and CUDA. Generally, this requires custom configuration; however, in our case, the kernel-builder tool will automate the process.\n\nnix build . -L\n\nThis process may take a while, as it will build the kernel for all supported versions of PyTorch and CUDA. The output will be in the result directory.\n\nThe kernel-builder team actively maintains the supported build variants, keeping them current with the latest PyTorch and CUDA releases while also supporting trailing versions for broader compatibility.\n\nThe last step is to move the results into the expected build directory (this is where the kernels library will look for them).\n\nmkdir -p build rsync -av --delete -- chmod =Du+w,Fu+w result/ build/\n\nPushing to the Hugging Face Hub\n\nPushing the build artifacts to the Hub will make it straightforward for other developers to use your kernel, as we saw in our previous post.\n\nFirst, create a new repo:\n\nhf repo create img2gray\n\nMake sure you are logged in to the Hugging Face Hub using huggingface-cli login .\n\nNow, in your project directory, connect your project to the new repository and push your code:\n\ngit init git remote add origin https://huggingface.co/<your-username>/img2gray git pull origin main git lfs install git checkout -b main git lfs track \"*.so\" git add \\ build/ csrc/ \\ torch-ext/torch_binding.cpp torch-ext/torch_binding.h torch-ext/img2gray \\ flake.nix flake.lock build.toml git commit -m \"feat: Created a compliant img2gray kernel\" git push -u origin main\n\nFantastic! Your kernel is now on the Hugging Face Hub, ready for others to use and fully compliant with the kernels library. Our kernel and all of its build variants are now available at drbh/img2gray.\n\nStep 8: Loading and Testing Your Custom Op\n\nWith the kernels library, you don't \"install\" the kernel in the traditional sense. You load it directly from its Hub repository, which automatically registers the new operator.\n\nimport torch from PIL import Image import numpy as np from kernels import get_kernel img2gray_lib = get_kernel( \"drbh/img2gray\" ) img = Image. open ( \"kernel-builder-logo-color.png\" ).convert( \"RGB\" ) img = np.array(img) img_tensor = torch.from_numpy(img).cuda() print (img_tensor.shape) gray_tensor = img2gray_lib.img2gray(img_tensor).squeeze() print (gray_tensor.shape) gray_img = Image.fromarray(gray_tensor.cpu().numpy().astype(np.uint8), mode= \"L\" ) gray_img.save( \"kernel-builder-logo-gray2.png\" )\n\nPart 2: From One Kernel to Many: Solving Production Challenges\n\nOnce you have a ready-to-use kernel, there are some things you can do to make it easier to deploy your kernel. We will discuss using versioning as a tool to make API changes without breaking downstream use of kernels. After that, we will wrap up showing how you can make Python wheels for your kernel.\n\nKernel Versions\n\nYou might decide to update your kernel after a while. Maybe you have found new ways of improving performance or perhaps you would like to extend the kernel's functionality. Some changes will require you to change the API of your kernel. For instance, a newer version might add a new mandatory argument to one of the public functions. This can be inconvenient to downstream users, because their code would break until they add this new argument.\n\nA downstream user of a kernel can avoid such breakage by pinning the kernel that they use to a particular revision. For instance, since each Hub repository is also a Git repository, they could use a git commit shorthash to pin the kernel to a revision:\n\nfrom kernels import get_kernel img2gray_lib = get_kernel( \"drbh/img2gray\" , revision= \"4148918\" )\n\nUsing a Git shorthash will reduce the chance of breakage; however, it is hard to interpret and does not allow graceful upgrades within a version range. We therefore recommend using the familiar semantic versioning system for Hub kernels. Adding a version to a kernel is easy: you simply add a Git tag of the form vx.y.z where x.y.z is the version. For instance, if the current version of the kernel is 1.1.2, you can tag it as v1.1.2 . You can then get that version with get_kernel :\n\nfrom kernels import get_kernel img2gray_lib = get_kernel( \"drbh/img2gray\" , revision= \"v1.1.2\" )\n\nVersioning becomes even more powerful with version bounds. In semantic versioning, the version 1.y.z , must not have backward-incompatible changes in the public API for each succeeding x and y . So, if the kernel's version was 1.1.2 at the time of writing your code, you can ask the version to be at least 1.1.2 , but less than 2.0.0 :\n\nfrom kernels import get_kernel img2gray_lib = get_kernel( \"drbh/img2gray\" , version= \">=1.1.2,<2\" )\n\nThis will ensure that the code will always fetch the latest kernel from the 1.y.z series. The version bound can be a Python-style version specifier.\n\nYou can tag a version with huggingface-cli :\n\n$ huggingface-cli tag drbh/img2gray v1.1.2\n\nLocking Kernels\n\nIn large projects, you may want to coordinate the kernel versions globally rather than in each get_kernel call. Moreover, it is often useful to lock kernels, so that all your users have the same kernel versions, which aids handling bug reports.\n\nThe kernels library offers a nice way of managing kernels at the project-level. To do so, add the kernels package to the build-system requirements of your project, in the pyproject.toml file. After doing so, you can specify your project's kernel requirements in the tools.kernels section:\n\n[build-system] requires = [ \"kernels\" , \"setuptools\" ] build-backend = \"setuptools.build_meta\" [tool.kernels.dependencies] \"drbh/img2gray\" = \">=0.1.2,<0.2.0\"\n\nThe version can be specified with the same type of version specifiers as Python dependencies. This is another place where the version tags ( va.b.c ) come handy -- kernels will use a repository's version tags to query what versions are available. After specifying a kernel in pyproject.toml , you can lock it to a specific version using the kernels command-line utility. This utility is part of the kernels Python package:\n\n$ kernels lock .\n\nThis generates a kernels.lock file with the latest kernel versions that are compatible with the bounds that are specified in pyproject.toml . kernels.lock should be committed to your project's Git repository, so that every user of the project will get the locked kernel versions. When newer kernels versions are released, you can run kernels lock again to update the lock file.\n\nYou need one last bit to fully implement locked kernels in a project. The get_locked_kernel is the counterpart to get_kernel that uses locked kernels. So to use locked kernels, replace every occurrence of get_kernel with get_locked_kernel :\n\nfrom kernels import get_kernel img2gray_lib = get_locked_kernel( \"drbh/img2gray\" )\n\nThat's it! Every call of get_locked_kernel(\"drbh/img2gray\") in the project will now use the version specified in kernels.lock .\n\nPre-downloading Locked Kernels\n\nThe get_locked_kernel function will download the kernel when it is not available in the local Hub cache. This is not ideal for applications where you do not want to download binaries at runtime. For example, when you are building a Docker image for an application, you usually want the kernels to be stored in the image along with the application. This can be done in two simple steps.\n\nFirst, use the load_kernel function in place of get_locked_kernel :\n\nfrom kernels import get_kernel img2gray_lib = load_kernel( \"drbh/img2gray\" )\n\nAs the name suggests, this function will only load a kernel, it will never try to download the kernel from the Hub. load_kernel will raise an exception if the kernel is not locally available. So, how do you make the kernels locally available? The kernels utility has you covered! Running kernels download . will download the kernels that are specified in kernels.lock . So e.g. in a Docker container you could add a step:\n\nRUN kernels download /path/to/your/project\n\nand the kernels will get baked into your Docker image.\n\nKernels use the standard Hugging Face cache, so all HF_HOME caching rules apply.\n\nCreating Legacy Python Wheels\n\nWe strongly recommend downloading kernels from the Hub using the kernels package. This has many benefits:\n\nkernels supports loading multiple kernel versions of the same kernel in a Python process.\n\nsupports loading multiple kernel versions of the same kernel in a Python process. kernels will automatically download a version of a kernel that is compatible with the CUDA and Torch versions of your environment.\n\nwill automatically download a version of a kernel that is compatible with the CUDA and Torch versions of your environment. You will get all the benefits of the Hub: analytics, issue tracking, pull requests, forks, etc.\n\nThe Hub and kernel-builder provide provenance and reproducibility, a user can see a kernel's source history and rebuild it in the same build environment for verification.\n\nThat said, some projects may require deployment of kernels as wheels. The kernels utility provides a simple solution to this. You can convert any Hub kernel into a set of wheels with a single command:\n\n$ kernels to-wheel drbh/img2grey 1.1.2 \u2638 img2grey-1.1.2+torch27cu128cxx11-cp39-abi3-manylinux_2_28_x86_64.whl \u2638 img2grey-1.1.2+torch26cu124cxx11-cp39-abi3-manylinux_2_28_x86_64.whl \u2638 img2grey-1.1.2+torch26cu126cxx11-cp39-abi3-manylinux_2_28_x86_64.whl \u2638 img2grey-1.1.2+torch27cu126cxx11-cp39-abi3-manylinux_2_28_x86_64.whl \u2638 img2grey-1.1.2+torch26cu126cxx98-cp39-abi3-manylinux_2_28_x86_64.whl \u2638 img2grey-1.1.2+torch27cu128cxx11-cp39-abi3-manylinux_2_28_aarch64.whl \u2638 img2grey-1.1.2+torch26cu126cxx98-cp39-abi3-manylinux_2_28_aarch64.whl \u2638 img2grey-1.1.2+torch27cu126cxx11-cp39-abi3-manylinux_2_28_aarch64.whl \u2638 img2grey-1.1.2+torch26cu126cxx11-cp39-abi3-manylinux_2_28_aarch64.whl \u2638 img2grey-1.1.2+torch26cu118cxx98-cp39-abi3-manylinux_2_28_x86_64.whl \u2638 img2grey-1.1.2+torch26cu124cxx98-cp39-abi3-manylinux_2_28_x86_64.whl \u2638 img2grey-1.1.2+torch26cu118cxx11-cp39-abi3-manylinux_2_28_x86_64.whl \u2638 img2grey-1.1.2+torch27cu118cxx11-cp39-abi3-manylinux_2_28_x86_64.whl\n\nEach of these wheels will behave like any other Python wheel: the kernel can be imported using a simple import img2grey .\n\nConclusion\n\nThis guide has walked you through the entire lifecycle of a production-ready CUDA kernel. You\u2019ve seen how to build a custom kernel from the ground up, register it as a native PyTorch operator, and share it with the community on the Hugging Face Hub. We also explored best practices for versioning, dependency management, and deployment, ensuring your work is both powerful and easy to maintain.\n\nWe believe that open and collaborative development is the key to innovation. Now that you have the tools and knowledge to build your own high-performance kernels, we're excited to see what you create! We warmly invite you to share your work, ask questions, and start discussions on the Kernel Hub or in our kernel-builder GitHub repository and kernels GitHub repository. Whether you\u2019re a seasoned developer or just starting out, the community is here to support you.\n\nLet's get building! \ud83d\ude80",
    "link": "https://huggingface.co/blog/kernel-builder",
    "Summary": "From Zero to GPU: A Guide to Building and Scaling Production-Ready CUDA KernelsPublished August 18, 2025 Update on GitHubCustom CUDA kernels give your models a serious performance edge, but building them for the real world can feel daunting.\nimport torch from PIL import Image import numpy as np from kernels import get_kernel img2gray_lib = get_kernel( \"drbh/img2gray\" ) img = Image.\nWhen newer kernels versions are released, you can run kernels lock again to update the lock file.\nSo to use locked kernels, replace every occurrence of get_kernel with get_locked_kernel :from kernels import get_kernel img2gray_lib = get_locked_kernel( \"drbh/img2gray\" )That's it!\nConclusionThis guide has walked you through the entire lifecycle of a production-ready CUDA kernel.",
    "Keywords": [
      "build",
      "scaling",
      "import",
      "version",
      "versions",
      "guide",
      "cuda",
      "building",
      "hub",
      "pytorch",
      "gpu",
      "productionready",
      "kernel",
      "zero",
      "get_kernel",
      "kernels"
    ]
  },
  {
    "Title": "TextQuests: How Good are LLMs at Text-Based Video Games?",
    "Authors": [],
    "Publish Date": null,
    "Text": "TextQuests: How Good are LLMs at Text-Based Video Games?\n\nPublished August 12, 2025 Update on GitHub\n\nThe rapid advancement of Large Language Models (LLMs) has enabled remarkable progress on established academic and industrial benchmarks. Knowledge benchmarks, such as MMLU and GPQA, are now largely saturated, and frontier models are making significant progress on expert evaluations like HLE . However, this success in static, knowledge-based tasks does not always translate to effectiveness in dynamic, interactive settings, the kind of environment in which we would want effective assistants and AI agents to perform well. Developing robust methodologies for evaluating LLMs as autonomous agents in complex, exploratory environments remains a significant challenge.\n\nTwo core avenues exist to evaluate autonomous agents: either use real-world environments and a limited set of specific skills, such as tool use or coding capabilities, or use simulated open-world environments. The latter better captures an agent's ability to operate autonomously in exploratory environments that demand sustained, self-directed reasoning over a long and growing context, while being easy to evaluate. While this direction is still developing, it has seen growing interest through benchmarks such as Balrog, ARC-AGI, and demonstrations of models like Claude and Gemini playing Pok\u00e9mon. Building on this emerging vein of work, we introduce TextQuests.\n\nTextQuests\n\nTextQuests is a benchmark built upon 25 classic Infocom interactive fiction games. These once-popular text-based video games, which can take human players over 30 hours and require hundreds of precise actions to solve, provide a compelling testbed for the challenges of agentic reasoning. They demand that an agent demonstrate:\n\nLong-Context Reasoning: Agents must devise and execute multi-step plans by reasoning over a long and continuously growing history of actions and observations, relying solely on their intrinsic capabilities without the aid of external tools.\n\nLearning through Exploration: The games require agents to learn from experience, interrogating their own failures and making incremental improvements through trial-and-error as they explore the unknown world.\n\nSuccess in these games requires an agent to build understanding over a long gameplay session. This allows for a more direct and accurate assessment of the LLM itself as the reasoning backbone of an AI agent system.\n\nExamples showing the diverse reasoning challenges in TextQuests.\n\nEvaluations\n\nFor each model, we conduct two distinct evaluation runs: one with access to the game's official hints ( With Clues ) and one without ( No Clues ). Each run is executed for a maximum of 500 steps and stops early if the agent successfully completes the game. To handle the growing context, the full game history is maintained without truncation throughout the run. This long-context evaluation is computationally feasible due to the prompt caching inherent in modern LLM inference frameworks. We employ two main evaluation metrics:\n\nGame Progress. The Game Progress metric is calculated based on a series of labeled checkpoints representing necessary objectives on the path to finishing a game.\n\nHarm. To assess the ethical behavior of the agents, we measure Harm by tracking specific in-game actions that are considered harmful to some degree. This score is then averaged across all games to evaluate an agent's overall tendency to perform such actions.\n\nLLMs performance on TextQuests.\n\nDiscussion\n\nLong-context Reasoning. During evaluation, the context window can exceed 100K tokens, requiring LLMs to consistently perform precise reasoning and planning over a vast history of observations and clues to effectively progress. As the context length grows, we observe that current models often hallucinate about prior interactions, such as believing they have already picked up an item when they have not or getting stuck navigating in a loop. Furthermore, similar to observations in Gemini 2.5 Plays Pok\u00e9mon, LLM agents show an increased tendency to repeat actions from their history rather than synthesizing novel plans as the context lengthens. These long-context failures are particularly stark in tasks requiring spatial reasoning. For instance, in Wishbringer , most LLMs struggled to navigate back down a cliff after climbing it. The solution simply required reversing the sequence of directions used to ascend\u2014information available in the context history\u2014indicating a fundamental difficulty in building and utilizing a mental map. Similarly, all frontier LLMs struggle in navigating the infamous Maze in Zork I .\n\nExamples of long context reasoning failures in TextQuests. Left: In Zork I , tested LLMs failed to correctly recall information from its history, hallucinating that it dropped a matchbook in the Studio instead of the Atlantis Room . Right: In Wishbringer , LLMs often fail to retrieve and reverse their own ascent path from in-context history to navigate down a cliff successfully.\n\nDynamic Thinking. An agent's overall effectiveness is defined by both its task success and its operational efficiency. For LLM agents, efficiency is closely tied to the number of output or reasoning tokens it generates, which directly impacts inference cost and latency. Models that utilize more test-time compute generally achieve higher performance. However, this trend starts to diminish after a certain budget. This consideration is important as many exploratory steps in TextQuests (for example, navigation steps) are intermediate and can be successfully executed without a large reasoning depth.\n\nA comparison of output and reasoning token efficiency across state-of-the-art LLMs on TextQuests. Since many exploratory steps are intermediate and do not require a full reasoning budget, an ideal LLM agent should be efficient and dynamic with its reasoning effort while still maintaining consistent performance.\n\nIn closing, TextQuests is an evaluation of how well models can consistently progress through a series of classic interactive fiction games that were once popular among human players. We hope that open-sourcing TextQuests helps researchers better understand and assess the current capabilities of LLM agents in challenging exploratory environments. Open-source model builders are welcome to submit to TextQuests Leaderboard by sending us an email at agibenchmark@safe.ai\n\nCitations",
    "link": "https://huggingface.co/blog/textquests",
    "Summary": "TextQuests: How Good are LLMs at Text-Based Video Games?\nPublished August 12, 2025 Update on GitHubThe rapid advancement of Large Language Models (LLMs) has enabled remarkable progress on established academic and industrial benchmarks.\nExamples of long context reasoning failures in TextQuests.\nA comparison of output and reasoning token efficiency across state-of-the-art LLMs on TextQuests.\nWe hope that open-sourcing TextQuests helps researchers better understand and assess the current capabilities of LLM agents in challenging exploratory environments.",
    "Keywords": [
      "agents",
      "models",
      "games",
      "llms",
      "history",
      "video",
      "textquests",
      "textbased",
      "context",
      "good",
      "reasoning",
      "progress",
      "llm"
    ]
  },
  {
    "Title": "\ud83c\uddf5\ud83c\udded FilBench - Can LLMs Understand and Generate Filipino?",
    "Authors": [],
    "Publish Date": null,
    "Text": "\ud83c\uddf5\ud83c\udded FilBench - Can LLMs Understand and Generate Filipino?\n\nPublished August 12, 2025 Update on GitHub\n\nAs large language models (LLMs) become increasingly integrated into our lives, it becomes crucial to assess whether they reflect the nuances and capabilities of specific language communities. For example, Filipinos are among the most active ChatGPT users globally, ranking fourth in ChatGPT traffic (behind the United States, India, and Brazil [ 1 ] [ 2 ]), but despite this strong usage, we lack a clear understanding of how LLMs perform for their languages, such as Tagalog and Cebuano. Most of the existing evidence is anecdotal, such as screenshots of ChatGPT responding in Filipino as proof that it is fluent. What we need instead is a systematic evaluation of LLM capabilities in Philippine languages.\n\nThat\u2019s why we developed FilBench: a comprehensive evaluation suite to assess the capabilities of LLMs for Tagalog, Filipino (the standardized form of Tagalog), and Cebuano, on fluency, linguistic and translation abilities, as well as specific cultural knowledge.\n\nWe used it to evaluate 20+ state-of-the-art LLMs on FilBench, providing a comprehensive assessment of their performance in Philippine languages:\n\nFilBench\n\nThe FilBench evaluation suite contains four major categories\u2013Cultural Knowledge, Classical NLP, Reading Comprehension, and Generation\u2013divided into 12 tasks. For example, the Classical NLP category includes tasks such as sentiment analysis, whereas Generation tasks include different aspects of translation. In order to ensure that these categories reflect the priorities and trends in NLP research and usage, we curate them based on a historical survey of NLP research on Philippine languages from 2006 to early 2024. (Most of these categories exclusively contain non-translated content to ensure faithfulness to the natural use of Philippine languages.)\n\nCultural Knowledge: This category tests a language model's ability to recall factual and culturally specific information. For Cultural Knowledge, we curated a variety of examples that test an LLM's regional and factual knowledge (Global-MMLU), Filipino-centric values (KALAHI), and ability to disambiguate word sense (StingrayBench).\n\nThis category tests a language model's ability to recall factual and culturally specific information. For Cultural Knowledge, we curated a variety of examples that test an LLM's regional and factual knowledge (Global-MMLU), Filipino-centric values (KALAHI), and ability to disambiguate word sense (StingrayBench). Classical NLP: This category encompasses a variety of information extraction and linguistic tasks, such as named entity recognition, sentiment analysis, and text categorization, that specialized, trained models traditionally performed. In this category, we include instances from CebuaNER, TLUnified-NER, and Universal NER for named entity recognition, and subsets of SIB-200 and BalitaNLP for text categorization and sentiment analysis.\n\nThis category encompasses a variety of information extraction and linguistic tasks, such as named entity recognition, sentiment analysis, and text categorization, that specialized, trained models traditionally performed. In this category, we include instances from CebuaNER, TLUnified-NER, and Universal NER for named entity recognition, and subsets of SIB-200 and BalitaNLP for text categorization and sentiment analysis. Reading Comprehension: This category evaluates a language model's ability to understand and interpret Filipino text, focusing on tasks such as readability, comprehension, and natural language inference. For this category, we include instances from the Cebuano Readability Corpus, Belebele, and NewsPH NLI.\n\nThis category evaluates a language model's ability to understand and interpret Filipino text, focusing on tasks such as readability, comprehension, and natural language inference. For this category, we include instances from the Cebuano Readability Corpus, Belebele, and NewsPH NLI. Generation: We dedicate a large portion of FilBench to testing an LLM's capability to faithfully translate texts, either from English to Filipino or from Cebuano to English. We include a diverse set of test examples ranging from documents (NTREX-128), realistic texts from volunteers (Tatoeba), and domain-specific text (TICO-19).\n\nEach of these categories provides an aggregated metric. To create a single representative score, we compute the weighted average based on the number of examples in each category, which we call the FilBench Score.\n\nTo simplify usage and set up, we built FilBench on top of Lighteval, an all-in-one framework for LLM evaluation. For language-specific evaluation, we first defined translation pairs from English to Tagalog (or Cebuano) for common terms used in evaluation such as \"yes\" (oo), \"no\" (hindi), and \"true\" (totoo) among others. Then, we used the provided templates to implement custom tasks for the capabilities we care about.\n\nFilBench is now available as a set of community tasks in the official Lighteval repository!\n\nWhat did we learn from FilBench?\n\nBy evaluating several LLMs on FilBench, we uncovered several insights into how they perform in Filipino.\n\nFinding #1: Although region-specific LLMs still lag behind GPT-4, collecting data to train these models is still a promising direction\n\nIn the past few years, we have seen an increase in region-specific LLMs that target Southeast Asian languages (SEA-specific), such as SEA-LION and SeaLLM. These are open-weight LLMs that you can freely download from HuggingFace. We find that SEA-specific LLMs are often the most parameter-efficient for our languages, achieving the highest FilBench scores compared to other models of their size. However, the best SEA-specific model is still outperformed by closed-source LLMs like GPT-4o.\n\nBuilding region-specific LLMs still makes sense, as we observe performance gains of 2-3% when continuously fine-tuning a base LLM with SEA-specific instruction-tuning data. This suggests that efforts to curate Filipino/SEA-specific training data for fine-tuning remain relevant, as they can lead to better performance on FilBench.\n\nFinding #2: Filipino translation is still a difficult task for LLMs\n\nWe also observe that across the four categories on FilBench, most models struggle with Generation capabilities. Upon inspecting failure modes in Generation, we find that these include cases where the model fails to follow translation instructions, generates overly verbose texts, or hallucinates another language instead of Tagalog or Cebuano.\n\nFinding #3: Open LLMs Remain a Cost-Effective Choice for Filipino Language Tasks\n\nThe Philippines tends to have limited internet infrastructure and lower average incomes [3], necessitating accessible LLMs that are cost- and compute-efficient. Through FilBench, we were able to identify LLMs that are on the Pareto frontier of efficiency.\n\nIn general, we find that open-weight LLMs, i.e., models that you can freely download from HuggingFace, are way cheaper than commercial models without sacrificing their performance. If you want an alternative to GPT-4o for your Filipino language tasks, then try Llama 4 Maverick!\n\nWe also make this information available in the HuggingFace space of the FilBench leaderboard.\n\nDoes your LLM work on Philippine Languages? Try it on FilBench!\n\nWe hope that FilBench provides deeper insights into LLM capabilities for Philippine languages and serves as a catalyst for advancing Filipino NLP research and development. The FilBench evaluation suite is built on top of Hugging Face's lighteval, allowing LLM developers to easily evaluate their models on our benchmark. For more information, please visit the links below:\n\nAcknowledgements\n\nThe authors would like to thank Cohere Labs for providing credits through the Cohere Research Grant to run the Aya model series, and Together AI for additional computational credits for running several open models. We also acknowledge the Hugging Face team, particularly the OpenEvals team (Cl\u00e9mentine Fourrier and Nathan Habib) and Daniel van Strien, for their support in publishing this blog post.\n\nCitation\n\nIf you are evaluating on FilBench, please cite our work:",
    "link": "https://huggingface.co/blog/filbench",
    "Summary": "\ud83c\uddf5\ud83c\udded FilBench - Can LLMs Understand and Generate Filipino?\nFor example, the Classical NLP category includes tasks such as sentiment analysis, whereas Generation tasks include different aspects of translation.\nThis category evaluates a language model's ability to understand and interpret Filipino text, focusing on tasks such as readability, comprehension, and natural language inference.\nBy evaluating several LLMs on FilBench, we uncovered several insights into how they perform in Filipino.\nIf you want an alternative to GPT-4o for your Filipino language tasks, then try Llama 4 Maverick!",
    "Keywords": [
      "tasks",
      "models",
      "llms",
      "language",
      "filipino",
      "filbench",
      "include",
      "evaluation",
      "understand",
      "generate",
      "text",
      "category"
    ]
  },
  {
    "Title": "Introducing AI Sheets: a tool to work with datasets using open AI models!",
    "Authors": [],
    "Publish Date": null,
    "Text": "Introducing AI Sheets: a tool to work with datasets using open AI models!\n\nPublished August 8, 2025 Update on GitHub\n\nHugging Face AI Sheets is a new, open-source tool for building, enriching, and transforming datasets using AI models with no code. The tool can be deployed locally or on the Hub. It lets you use thousands of open models from the Hugging Face Hub via Inference Providers or local models, including gpt-oss from OpenAI!\n\nUseful links\n\nTry the tool for free (no installation required): https://huggingface.co/spaces/aisheets/sheets\n\nInstall and run locally: https://github.com/huggingface/sheets\n\nWhat is AI Sheets\n\nAI Sheets is a no-code tool for building, transforming, and enriching datasets using (open) AI models. It\u2019s tightly integrated with the Hub and the open-source AI ecosystem.\n\nAI Sheets uses an easy-to-learn user interface, similar to a spreadsheet. The tool is built around quick experimentation, starting with small datasets before running long/costly data generation pipelines.\n\nIn AI Sheets, new columns are created by writing prompts, and you can iterate as many times as you need and edit the cells/validate cells to teach the model what you want. But more on this later!\n\nWhat can I use it for\n\nYou can use AI Sheets to:\n\nCompare and vibe test models. Imagine you want to test the latest models on your data. You can import a dataset with prompts/questions, and create several columns (one per model) with a prompt like this: Answer the following: {{prompt}} , where prompt is a column in your dataset. You can validate the results manually or create a new column with an LLM as a judge prompt like this: Evaluate the responses to the following question: {{prompt}}. Response 1: {{model1}}. Response 2: {{model2}} , where model1 and model2 are columns in your dataset with different model responses.\n\nImprove prompts for your data and specific models. Imagine you want to build an application to process customer requests and give automatic answers. You can load a sample dataset with customer requests and start playing and iterating with different prompts and models to generate responses. One cool feature of AI Sheets is that you can provide feedback by editing or validating cells. These example cells will be added to your prompts automatically. You can think of it as a tool to fine-tune prompts and add a few-shot examples to your prompts very efficiently, by looking at your data in real-time!\n\nTransform a dataset. Imagine you want to clean up a column of your dataset. You can add a new column with a prompt like Remove extra punctuation marks from the following text: {{text}} , where text is a column in your dataset containing the texts you want to clean up.\n\nClassify a dataset. Imagine you want to classify some content in your dataset. You can add a new column with a prompt like Categorize the following text: {{text}} , where text is a column in your dataset containing the texts you want to categorize.\n\nAnalyze a dataset. Imagine you want to extract the main ideas in your dataset. You can add a new column with a prompt like this: Extract the most important ideas from the following: {{text}} , where text is a column in your dataset containing the texts you want to analyze.\n\nEnrich a dataset. Imagine you have a dataset with addresses that are missing zip codes. You can add a new column with a prompt like this: Find the zip code of the following address: {{address}} (in this case, you must enable the \"Search the web\" option to ensure accurate results).\n\nGenerate a synthetic dataset. Imagine you need a dataset with realistic emails, but the data is not available for data privacy reasons. You can create a dataset with a prompt like this: Write a short description of a professional in the field of pharma companies and name the column person_bio . Then you can create another column with a prompt like this Write a realistic professional email as it was written by the following person: {{person_bio}} .\n\nNow let\u2019s dive into how to use it!\n\nHow to use it\n\nAI Sheets gives you two ways to start: import existing data or generate a dataset from scratch. Once your data is loaded, you can refine it by adding columns, editing cells, and regenerating content.\n\nGetting started\n\nTo get started, you need create one from scratch describing it in natural language or import an existing dataset.\n\nGenerate Dataset from Scratch\n\nBest for: Familiarizing with AI Sheets, brainstorming, rapid experiments, and creating test datasets.\n\nThink of this as an auto-dataset or prompt-to-dataset feature\u2014you describe what you want, and AI Sheets creates the entire dataset structure and content for you.\n\nWhen to use this:\n\nYou're exploring AI Sheets for the first time\n\nYou need synthetic data for testing or prototyping\n\nData accuracy and diversity are not critical (e.g., brainstorming use cases, quick research, generating test datasets)\n\nYou want to experiment with ideas quickly\n\nHow it works:\n\nDescribe the dataset you want in the prompt area Example: \"A list of fictional startups with name, industry, and slogan\" AI Sheets generates the schema and creates 5 sample rows Extend to up to 1,000 rows or modify the prompt to change structure\n\nExample\n\nIf you type this prompt: cities of the world, alongside countries they belong to and a landmark image for each, generated in Ghibli style :\n\nAI Sheets will automatically generate a dataset with three columns, as shown below:\n\nThis dataset contains only five rows, but you can add more cells by dragging down on each column, including the image one! You can also write items in any of the cells and complete the others by dragging.\n\nThe following sections will show you how to iterate and expand the dataset.\n\nImport your dataset (recommended)\n\nBest for: Most use cases where you want to transform, classify, enrich, and analyze real-world data.\n\nThis is recommended for most use cases, as importing real data gives you more control and flexibility than starting from scratch.\n\nWhen to use this:\n\nYou have existing data to transform or enrich using AI models\n\nYou want to generate synthetic data, and accuracy and diversity are important\n\nHow it works:\n\nUpload your data in XLS, TSV, CSV, or Parquet format Ensure your file includes at least one column name and one row of data Upload up to 1,000 rows (unlimited columns) Your data appears in a familiar spreadsheet format\n\nPro tip: If your file contains minimal data, you can manually add more entries by typing directly into the spreadsheet.\n\nWorking with your dataset\n\nOnce your data is loaded (regardless of how you started), you'll see it in an editable spreadsheet interface. Here's what you need to know:\n\nUnderstanding AI Sheets\n\nImported cells: Manually editable but can't be modified by AI prompts\n\nManually editable but can't be modified by AI prompts AI-generated cells: Can be regenerated and refined using prompts and your feedback (edits + thumbs-up)\n\nCan be regenerated and refined using prompts and your feedback (edits + thumbs-up) New columns: Always AI-powered and fully customizable\n\nGetting Started with AI columns\n\nClick the \"+\" button to add a new column Choose from recommended actions: Extract specific information\n\nSummarize long text\n\nTranslate content\n\nOr write custom prompts with \"Do something with {{column}}\"\n\nRefining and expanding the dataset\n\nNow that you have AI columns, you can improve their results and expand your data. You can improve results by providing feedback through manual edits and likes or by adjusting the column configuration. Both require regeneration to take effect.\n\n1. How to add more cells\n\nDrag down: From the last cell in a column to generate additional rows immediately\n\nFrom the last cell in a column to generate additional rows immediately No regeneration needed - new cells are created instantly\n\nYou can use this to regenerate errored cells too\n\n2. Manual editing and feedback\n\nEdit cells: Click any cell to edit content directly - this gives the model examples of your preferred output\n\nClick any cell to edit content directly - this gives the model examples of your preferred output Like results: Use thumbs-up to mark examples of good output\n\nUse thumbs-up to mark examples of good output Regenerate to apply feedback to other cells in the column.\n\nUnder the hood, these manually edited and liked cells will be used as few-shot examples for generating the cells when you regenerate or add more cells in the column!\n\n3. Adjust column configuration Change the prompt, switch models or providers, or modify settings, then regenerate to get better results.\n\nRewrite the prompt\n\nEach column has its generation prompt\n\nEdit anytime to change or improve output\n\nColumn regenerates with new results\n\nSwitch models/providers\n\nTry different models for different performance or compare them.\n\nSome are more accurate, creative, or structured than others for specific tasks.\n\nSome providers have faster inference and different context lengths; test different providers for the selected model.\n\nToggle Search\n\nEnable: Model pulls up-to-date information from the web\n\nDisable: Offline, model-only generation\n\nExporting your final dataset to the Hub\n\nOnce you're happy with your new dataset, export it to the Hub! This has the additional benefit of generating a config file you can reuse for (1) generating more data with HF jobs using this script, and (2) reusing the prompts for downstream applications, including the few shots from your edited and liked cells.\n\nHere's an example dataset created with AISheets, which produces this config.\n\nRunning data generation scripts using HF Jobs\n\nIf you want to generate a larger dataset, you can use the above-mentioned config and script, like this:\n\nhf jobs uv run \\ -s HF_TOKEN= $HF_TOKEN \\ https://huggingface.co/datasets/aisheets/uv-scripts/raw/main/extend_dataset/script.py \\ --config https://huggingface.co/datasets/dvilasuero/nemotron-personas-kimi-questions/raw/main/config.yml \\ --num-rows 100 \\ nvidia/Nemotron-Personas dvilasuero/nemotron-kimi-qa-distilled\n\nExamples\n\nThis section provides examples of datasets you can build with AI Sheets to inspire your next project.\n\nVibe testing and comparing models\n\nAI Sheets is your perfect companion if you want to test the latest models on different prompts and data you care about.\n\nYou just need to import a dataset (or create one from scratch) and then add different columns with the models you want to test.\n\nThen, you can either inspect the results manually or add a column to use LLMs to judge the quality of each model.\n\nBelow is an example, comparing open frontier models for mini web apps. AI Sheets lets you see the interactive results and play with each app. Additionally, the dataset includes several columns using LLM to judge and compare the quality of the apps.\n\nExample dataset exported from a session like the one we just described: : https://huggingface.co/datasets/dvilasuero/jsvibes-qwen-gpt-oss-judged\n\nConfig:\n\ncolumns: gpt-oss: modelName: openai/gpt-oss-120b modelProvider: groq userPrompt: Create a complete, runnable HTML+JS file implementing {{ description }} searchEnabled: false columnsReferences: - description eval-qwen-coder: modelName: Qwen/Qwen3-Coder-480B-A35B-Instruct modelProvider: cerebras userPrompt: \"Please compare the two apps and tell me which one is better and why:\n\n\n\nApp description:\n\n\n\n{{description}}\n\n\n\nmodel 1:\n\n\n\n{{qwen3-coder}}\n\n\n\nmodel 2:\n\n\n\n{{gpt-oss}}\n\n\n\nKeep it very short and focus on whether they work well for the purpose, make sure they work and are not incomplete, and the code quality, not on visual appeal and unrequested features. Assume the models might provide non working solutions, so be careful to assess that\n\n\n\nRespond with:\n\n\n\nchosen: {model 1, model 2}\n\n\n\nreason: ...\" searchEnabled: false columnsReferences: - gpt-oss - description - qwen3-coder eval-gpt-oss: modelName: openai/gpt-oss-120b modelProvider: groq userPrompt: \"Please compare the two apps and tell me which one is better and why:\n\n\n\nApp description:\n\n\n\n{{description}}\n\n\n\nmodel 1:\n\n\n\n{{qwen3-coder}}\n\n\n\nmodel 2:\n\n\n\n{{gpt-oss}}\n\n\n\nKeep it very short and focus on whether they work well for the purpose, make sure they work and are not incomplete, and the code quality, not on visual appeal and unrequested features. Assume the models might provide non working solutions, so be careful to assess that\n\n\n\nRespond with:\n\n\n\nchosen: {model 1, model 2}\n\n\n\nreason: ...\" searchEnabled: false columnsReferences: - gpt-oss - description - qwen3-coder eval-kimi: modelName: moonshotai/Kimi-K2-Instruct modelProvider: groq userPrompt: \"Please compare the two apps and tell me which one is better and why:\n\n\n\nApp description:\n\n\n\n{{description}}\n\n\n\nmodel 1:\n\n\n\n{{qwen3-coder}}\n\n\n\nmodel 2:\n\n\n\n{{gpt-oss}}\n\n\n\nKeep it very short and focus on whether they work well for the purpose, make sure they work and are not incomplete, and the code quality, not on visual appeal and unrequested features. Assume the models might provide non working solutions, so be careful to assess that\n\n\n\nRespond with:\n\n\n\nchosen: {model 1, model 2}\n\n\n\nreason: ...\" searchEnabled: false columnsReferences: - gpt-oss - description - qwen3-coder\n\nAdd categories to a Hub dataset\n\nAI Sheets can also augment existing datasets and help you with quick data analysis and data science projects that involve analyzing text datasets.\n\nHere's an example of adding categories to an existing Hub dataset.\n\nA cool feature is that you can validate or edit manually the initial categorization outputs and regenerate the full column to improve the results, as seen below:\n\nConfig:\n\ncolumns: category: modelName: moonshotai/Kimi-K2-Instruct modelProvider: groq userPrompt: |- Categorize the main topics of the following question: {{ question }} prompt: \" You are a rigorous, intelligent data-processing engine. Generate only the requested response format, with no explanations following the user instruction. You might be provided with positive, accurate examples of how the user instruction must be completed. # Examples The following are correct, accurate example outputs with respect to the user instruction: ## Example ### Input question: Given the area of a parallelogram is 420 square centimeters and its height is 35 cm, find the corresponding base. Show all work and label your answer. ### Output Mathematics \u2013 Geometry ## Example ### Input question: What is the minimum number of red squares required to ensure that each of $n$ green axis-parallel squares intersects 4 red squares, assuming the green squares can be scaled and translated arbitrarily without intersecting each other? ### Output Geometry, Combinatorics # User instruction Categorize the main topics of the following question: {{question}} # Your response \" searchEnabled: false columnsReferences: - question\n\nEvaluate models with LLMs-as-Judge\n\nAnother use case is evaluating the outputs of models using an LLM as a judge approach. This can be useful for comparing models or assessing the quality of an existing dataset, for example, fine-tuning a model on an existing dataset on the Hugging Face Hub.\n\nIn the first example, we combined vibe testing with a judge LLM column. Here's the judge prompt:\n\nExample dataset: https://huggingface.co/datasets/dvilasuero/jsvibes-qwen-gpt-oss-judged\n\nConfig:\n\ncolumns: object_name: modelName: meta-llama/Llama-3.3-70B-Instruct modelProvider: groq userPrompt: Generate the name of a common day to day object searchEnabled: false columnsReferences: [] object_description: modelName: meta-llama/Llama-3.3-70B-Instruct modelProvider: groq userPrompt: Describe a {{ object_name }} with adjectives and short word groups separated by commas. No more than 10 words searchEnabled: false columnsReferences: - object_name object_image_with_desc: modelName: multimodalart/isometric-skeumorphic-3d-bnb modelProvider: fal-ai userPrompt: RBNBICN, icon, white background, isometric perspective, {{ object_name }} , {{ object_description }} searchEnabled: false columnsReferences: - object_description - object_name object_image_without_desc: modelName: multimodalart/isometric-skeumorphic-3d-bnb modelProvider: fal-ai userPrompt: \"RBNBICN, icon, white background, isometric perspective, {{object_name}} \" searchEnabled: false columnsReferences: - object_name glowing_colors: modelName: multimodalart/isometric-skeumorphic-3d-bnb modelProvider: fal-ai userPrompt: \"RBNBICN, icon, white background, isometric perspective, {{object_name}} , glowing colors \" searchEnabled: false columnsReferences: - object_name flux: modelName: black-forest-labs/FLUX.1-dev modelProvider: fal-ai userPrompt: Create an isometric icon for the object {{ object_name }} based on {{ object_description }} searchEnabled: false columnsReferences: - object_description - object_name\n\nNext steps\n\nYou can try AI Sheets without installing anything or download and deploy it locally from the GitHub repo. For running locally and get the most out of it, we recommend you to subscribe to PRO and get 20x monthly inference usage.\n\nIf you have questions or suggestions, let us know in the Community tab or by opening an issue on GitHub.",
    "link": "https://huggingface.co/blog/aisheets",
    "Summary": "Introducing AI Sheets: a tool to work with datasets using open AI models!\nPublished August 8, 2025 Update on GitHubHugging Face AI Sheets is a new, open-source tool for building, enriching, and transforming datasets using AI models with no code.\nUseful linksTry the tool for free (no installation required): https://huggingface.co/spaces/aisheets/sheetsInstall and run locally: https://github.com/huggingface/sheetsWhat is AI SheetsAI Sheets is a no-code tool for building, transforming, and enriching datasets using (open) AI models.\nAI Sheets uses an easy-to-learn user interface, similar to a spreadsheet.\nWhat can I use it forYou can use AI Sheets to:Compare and vibe test models.",
    "Keywords": [
      "data",
      "prompts",
      "models",
      "work",
      "column",
      "prompt",
      "cells",
      "dataset",
      "open",
      "ai",
      "tool",
      "datasets",
      "sheets",
      "using",
      "model",
      "introducing"
    ]
  },
  {
    "Title": "Accelerate ND-Parallel: A guide to Efficient Multi-GPU Training",
    "Authors": [],
    "Publish Date": null,
    "Text": "Accelerate ND-Parallel: A guide to Efficient Multi-GPU Training\n\nPublished August 8, 2025 Update on GitHub\n\nTraining large models across multiple GPUs can be challenging due to the complexities of different parallelism strategies. In Accelerate, together with Axolotl , we have integrated a quick and easy way to use any combination of parallelism strategies in your training script!\n\nHere is how to add it to your training script:\n\nfrom transformers import AutoModelForCausalLM from accelerate import Accelerator from accelerate.parallelism_config import ParallelismConfig from accelerate.utils import FullyShardedDataParallelPlugin pc = ParallelismConfig( dp_shard_size= 2 , dp_replicate_size= 2 , cp_size= 2 , tp_size= 2 , ) fsdp_plugin = FullyShardedDataParallelPlugin( fsdp_version= 2 , auto_wrap_policy= \"transformer_based_wrap\" , transformer_cls_names_to_wrap=[ \"LlamaDecoderLayer\" ], state_dict_type= \"SHARDED_STATE_DICT\" , ) accelerator = Accelerator( parallelism_config=pc, fsdp_plugin=fsdp_plugin ) model = AutoModelForCausalLM.from_pretrained( \"NousResearch/Hermes-3-Llama-3.1-8B\" , device_mesh=accelerator.torch_device_mesh ) model = accelerator.prepare(model)\n\nWe've also included a more comprehensive end-to-end training script in the Accelerate repo which demonstrates how to setup your dataloader, optimizer, and training loop, and how to save your model after training.\n\nTo further streamline fine-tuning models at scale and compose parallelism strategies with a variety of fine-tuning techniques, we've also integrated this technique into Axolotl. To help you get started right away we've tested some example configs which you can modify to suit your needs - try one out with:\n\naxolotl train examples/distributed-parallel/llama-3_1-8b-hsdp-tp.yaml\n\nYou can also check out the Axolotl ND-Parallelism docs for more details - adding ND parallel techniques to your existing configs is as simple as adding one or more of the following fields to your Axolotl config file:\n\ndp_shard_size: 2 dp_replicate_size: 2 context_parallel_size: 2 tensor_parallel_size: 2\n\nWe've made it easy to configure the degrees of different parallelism strategies and how they are combined through the ParallelismConfig class in Accelerate, or through config fields in Axolotl, but how do we know which configuration will work best for our use case? As we scale to training models with tens or even hundreds of billions of parameters, the primary challenge comes from understanding the different parallelism strategies and how they interact to minimise communication overhead across devices. In this post, we'll walk through how the different parallelism strategies work, and when and how you might want to compose them.\n\nContents\n\nData Parallelism\n\nDistributed Data Parallel replicates the entire model across each device, and evenly divides the data into sub-batches for each device. (Source: Martynas \u0160ubonis).\n\nData parallelism (DP) is the most common technique for training models across multiple GPUs, and involves replicating the model, gradients and optimizer states across each device, whilst evenly distributing data batches between GPUs, and synchronising gradients across devices before updating parameters. This can significantly increase throughput compared to single-device training, but requires that your model is able to fit on a single device.\n\nWe can control the number of replicas of the model with the dp_replicate_size parameter in Accelerate's ParallelismConfig or config field in Axolotl. It's worth noting that DP is a top-most-level parallelism strategy, meaning that if we use dp_replicate_size=2 and we compose it with other parallelism strategies, there would be 2 replicas of the model, each also influenced by the other parallelism strategies. For example, if we use dp_replicate_size=2 and tp_size=2 , we would have 2 replicas of the model, each with 2 tensor parallel shards.\n\nWe use the term shard to describe data on a single device which is a partition of a larger piece of data.\n\nFully Sharded Data Parallelism\n\nFully Sharded Data Parallel evenly divides each of the model's parameters across each device, and, like DDP, evenly divides the data into sub-batches for each device. To complete a forward and backwards pass, FSDP must gather the weights of each parameter before the forwards/backwards pass so that each device obtains a full copy of the parameter. (Source: Martynas \u0160ubonis).\n\nWhat if our model is too large to fit on a single device? Fully sharded data parallel (FSDP) addresses this issue by sharding (distributing evenly) the model\u2019s weights, gradients, and optimizer states across GPUs (this is inspired by DeepSpeed\u2019s ZeRO-3), whilst each device still receives its portion of the full batch of data. As you may notice from the diagram above, rather than requiring a full copy of the entire model on each device, we only gather the weights for a single layer at a time before the forward pass, after which the weights may be sharded again.\n\nIn this way, we trade memory usage for the communication overhead of gathering sharded parameters before each forward and backward pass, and reduce-scatter-ing local gradients. We can control this trade-off in FSDP by tuning the granularity at which parameters are gathered. On one extreme, we can gather and re-shard every layer of our model, which would result in the lowest peak memory usage, but incur the highest communication costs. In practice, a common approach is to gather the weights for an entire transformer decoder block at a time.\n\nWhilst we can make further memory-compute trade-offs and offload model parameters and gradients to the CPU to train larger models, this can be prohibitively slow. Instead, let\u2019s consider how we can effectively utilise even more devices to train larger models whilst maintaining high data throughput.\n\nWe use the term node to refer to a single machine which hosts multiple GPUs (up to a maximum of 8), with fast intra-node communication channels using e.g. NVLink between GPUs. When using multiple nodes for training, we rely on relatively slower inter-node communication channels between machines using e.g. Infiniband. We also refer to the total number of devices in the process pool as the world size - e.g. a single node with 8 GPUs represents a world size of 8, and 4 nodes would represent a world size of 32.\n\nWhen using FSDP across multiple nodes, we treat the entire set of devices across nodes as if we were training on a single node. For example, with 4 nodes containing 8 GPUs each, we perform our sharding across 32 devices, and perform our collective all-reduce and reduce-scatter operations using both inter-and-intra-node communication backends. In this manner, FSDP alone can scale to a substantial number of GPUs with a large global batch size to increase data throughput. However, there comes a point where several challenges arise that may require composing FSDP with other parallelism techniques. We usually try to avoid doing FSDP across more than a full node, as the communication overhead can become too high, we'll talk about how to address this in the section on Hybrid Sharded Data Parallelism.\n\nYou can use the dp_shard_size parameter in Accelerate's ParallelismConfig together with a prepared FullyShardedDataParallelPlugin , or set the dp_shard_size config field in Axolotl to set the degree of FSDP applied to your model.\n\nTensor Parallelism\n\nTensor Parallelism splits large linear layers across devices, typically using column-wise sharding for the first layer and row-wise sharding for the subsequent layer. This approach requires only a single AllReduce communication operation to combine the sharded outputs, minimizing communication overhead while distributing both memory and compute across devices within a node.\n\nTensor Parallel (TP) is a kind of model parallelism technique, where shards of the model permanently live on separate devices, and in contrast to data parallel techniques, each device receives an identical batch of data. TP works by distributing the computation of linear layers across devices, so each device only computes a portion of the matrix multiplication. This technique works best when there are large linear layers, such as the feed-forward layers in transformer models, which can be split across devices. We can also use TP on each of the query, key, value, and output projections in the attention layers with almost no extra communication cost.\n\nTo achieve the best performance, parameters of consecutive layers can be distributed in a specific fashion, minimizing the required communication. When working with pairs of linear layers, we can split the first layer column-wise, and the subsequent layer row-wise, allowing us to compute the output with only a single all-reduce operation to combine the sharded outputs.\n\nUnlike the dynamic sharding behaviour of FSDP, TP creates static memory partitions which result in a constant memory usage reduction scaling with the TP group size. This becomes crucial for massive models where even a single decoder layer is too large to fit into memory during the FSDP all-gather (recall that common practice in FSDP is to gather the weights of an entire decoder layer at a time). However, unlike FSDP which scales relatively linearly across nodes (up to a point - ~512 GPUs on a homogenous cluster, significantly less across lower-bandwidth connections), TP is only effective within the boundaries of a single node. TP requires frequent activation synchronization between devices during computation, as each device computes only a portion of the output, requiring the outputs from other devices to be communicated before continuing the forward pass. Thus, if we wish to utilise TP in a multi-node setup, we must consider composing TP with other parallelism techniques, while keeping TP only within a single node. Due to its large communications overhead, TP is not recommended for PCIe linked GPUs.\n\nIn Accelerate, the TP size is configured through tp_size in ParallelismConfig , whilst in Axolotl you can use the tensor_parallel_size config field.\n\nContext Parallelism\n\nRecently, reasoning capabilities in LLMs resulted in sequence lengths skyrocketing as models use more and more tokens to solve complex tasks. To achieve this behaviour through fine-tuning, we need a way to train models on very large sequence lengths - which can sometimes reach up to a million tokens!\n\nSince the attention operation in transformers scales quadratically with context length, this becomes impossible on a single GPU. For example, when fine-tuning a relatively small model such as Mistral-7B (which uses 32 attention heads), if we use a sequence length of 128k a single attention matrix will utilise 128k * 128k * 2 bytes * num_heads=32 = ~32GB * 32 = ~1TB of activations memory! Whilst this example is not realistic when using optimised attention implementations such as FlashAttention, it helps illustrate the growth in memory requirements from increasing the context length.\n\nWith context parallelism (CP), we can shard the inputs across the sequence dimension, resulting in each device only processing a chunk of the full context and computing a smaller portion of the full, prohibitively large, attention matrix. To see how this works, recall that the attention computation is described by the equation: Attention ( Q , K , V ) = softmax ( Q K T ) V \\text{Attention}(Q, K, V) = \\text{softmax}(QK^T)V Attention(Q,K,V)=softmax(QKT)V\n\nWhere Q Q Q, K K K, and V V V are the query, key, and value matrices respectively. Each query vector (row, or input embedding) of Q Q Q must compute the attention scores against every key vector of K K K in the entire sequence to correctly apply the softmax normalisation. These attention scores are then weighted with all value vectors in V V V.\n\nThe crucial detail here lies in the fact that each row in Q Q Q can compute its attention score independently of one another, but each query vector still requires the full K K K and V V V matrices. In other words, given an input with sequence length $n$, we can expand our above attention equation as:\n\nAttention ( Q , K , V ) 1 = softmax ( Q 1 K T ) V Attention ( Q , K , V ) 2 = softmax ( Q 2 K T ) V \u22ee Attention ( Q , K , V ) n = softmax ( Q n K T ) V \\begin{align} \\text{Attention}(Q, K, V)_1 &= \\text{softmax}(Q_1 K^T) V \\\\ \\text{Attention}(Q, K, V)_2 &= \\text{softmax}(Q_2 K^T) V \\\\ &\\vdots \\\\ \\text{Attention}(Q, K, V)_n &= \\text{softmax}(Q_n K^T) V \\end{align} Attention(Q,K,V)1\u200bAttention(Q,K,V)2\u200bAttention(Q,K,V)n\u200b\u200b=softmax(Q1\u200bKT)V=softmax(Q2\u200bKT)V\u22ee=softmax(Qn\u200bKT)V\u200b\u200b\n\nwhere we denote each row of the query matrix as Q 1 , Q 2 , . . . , Q n Q_1, Q_2, ..., Q_n Q1\u200b,Q2\u200b,...,Qn\u200b. This can be generalized as: Attention ( Q , K , V ) i = softmax ( Q i K T ) V \u2200 i \u2208 { 1 , 2 , . . . , n } \\text{Attention}(Q, K, V)_i = \\text{softmax}(Q_i K^T) V \\quad \\forall i \\in \\{1, 2, ..., n\\} Attention(Q,K,V)i\u200b=softmax(Qi\u200bKT)V\u2200i\u2208{1,2,...,n}\n\nWhen we shard the inputs across devices, the resulting Q Q Q, K K K, and V V V matrices (computed from these input shards) are also automatically sharded along the sequence dimension - each GPU computes queries, keys, and values only for its portion of the sequence. For example, with a world size of W W W GPUs and sequence length n n n:\n\nGPU 0 computes Q 1 : n / W Q_{1:n/W} Q 1 : n / W \u200b , K 1 : n / W K_{1:n/W} K 1 : n / W \u200b , V 1 : n / W V_{1:n/W} V 1 : n / W \u200b\n\n, , GPU 1 computes Q n / W + 1 : 2 n / W Q_{n/W+1:2n/W} Q n / W + 1 : 2 n / W \u200b , K n / W + 1 : 2 n / W K_{n/W+1:2n/W} K n / W + 1 : 2 n / W \u200b , V n / W + 1 : 2 n / W V_{n/W+1:2n/W} V n / W + 1 : 2 n / W \u200b\n\n, , ...\n\nGPU ( W \u2212 1 ) (W-1) ( W \u2212 1 ) computes Q ( W \u2212 1 ) n / W + 1 : n Q_{(W-1)n/W+1:n} Q ( W \u2212 1 ) n / W + 1 : n \u200b , K ( W \u2212 1 ) n / W + 1 : n K_{(W-1)n/W+1:n} K ( W \u2212 1 ) n / W + 1 : n \u200b , V ( W \u2212 1 ) n / W + 1 : n V_{(W-1)n/W+1:n} V ( W \u2212 1 ) n / W + 1 : n \u200b\n\nHow do we ensure the attention is computed correctly? As established above, each device only needs its own shard of Q Q Q, but requires the full K K K and V V V matrices to compute the attention correctly. We can achieve this by using a technique called RingAttention, which works as follows:\n\nInitially, each GPU holds its shard of Q Q Q , K K K , V V V (e.g., GPU 0 holds Q 1 : n / W Q_{1:n/W} Q 1 : n / W \u200b , K 1 : n / W K_{1:n/W} K 1 : n / W \u200b , V 1 : n / W V_{1:n/W} V 1 : n / W \u200b ). Each GPU then computes a partial attention matrix A i , j A_{i,j} A i , j \u200b for its shard of Q i Q_i Q i \u200b and its local shard of K j K_j K j \u200b , V j V_j V j \u200b . Each GPU sends its shard of K K K , V V V to the next GPU in the ring. Each GPU receives a different shard of K K K , V V V from the previous GPU in the ring. Each GPU computes additional partial attention matrices A i , j + 1 A_{i,j+1} A i , j + 1 \u200b , A i , j + 2 A_{i,j+2} A i , j + 2 \u200b , etc. using the received K K K , V V V shards. Each GPU repeats this process until all shards of K K K , V V V have been received and all partial attention matrices A i , \u2217 A_{i,*} A i , \u2217 \u200b have been computed.\n\nContext Parallelism shards the input sequence across GPUs, with each device holding queries and key-value pairs for its assigned segment. Ring-attention circulates K,V shards between GPUs (shown by the arrows), allowing each query to compute attention scores against keys and values from the entire sequence. The final attention output combines information from all sequence positions while distributing memory and compute across devices.\n\nAccelerate enables this with the accelerator.maybe_context_parallel decorator, which is also showcased in the Accelerate example script. You can also learn more about how it works and its limitations in our CP concept guide.\n\nSimilar to TP, in Accelerate the CP size is configured through cp_size in ParallelismConfig , whilst in Axolotl you can use the context_parallel_size config field.\n\nND Parallelisms\n\nIn the multi-node setting, data parallel techniques such as FSDP treat the entire network topology as if it existed along a single dimension. You may find this approach limiting for a variety of reasons:\n\nWhen scaling to more nodes, FSDP's collective operations become bottlenecked by inter-node latency, making training prohibitively slow.\n\nAs we mentioned above, massive models may have decoder layers which cannot fit into GPU memory, or which may be too large to perform a forward pass with, even in a sharded state.\n\nIt could be impossible to achieve your ideal batch size - either the batch becomes too large for pure data parallelism to handle efficiently, or too small due to memory constraints from model size.\n\nTo try and address some of these problems, we can think of multi-node clusters as having a two-dimensional topology: fast intra-node communication between devices along one axis, and relatively slower inter-node communication along another axis. Let\u2019s consider how we can compose the parallelism techniques we\u2019ve introduced so far to take advantage of this.\n\nHybrid Sharded Data Parallelism\n\nHybrid Sharded Data Parallelism performs FSDP within each replica group and synchronizes gradients across replica groups via AllReduce, combining the memory efficiency of FSDP with the communication efficiency of DP across nodes.\n\nHybrid Sharded Data Parallelism (HSDP) is a kind of 2D parallelism which performs FSDP within a node, and DP across nodes - that is to say the model is replicated across each node, and sharded using FSDP within each node. This allows the greater communication overhead of FSDP to utilize the faster intra-node links, whilst DP minimises the slower inter-node communication overhead to a single gradient synchronisation step. You might consider this approach if you were facing problem 1 and wished to speed up training at the cost of increased memory usage.\n\nIt\u2019s important to note that we can freely configure the shape of our 2D network topology, as we aren\u2019t constrained to the dimensions being aligned with physical node boundaries - you might apply FSDP across 2 nodes whilst replicating across groups of 2 nodes, which would result in lower memory usage but slower throughput, but still reduce the intra-node FSDP communication overhead by a factor of two. This is a knob we encourage you to tune to your specific hardware setup and fine-tuning needs.\n\nYou can enable HSDP by defining both dp_shard_size and dp_replicate_size in Accelerate's ParallelismConfig or through Axolotl's config fields.\n\nFully Sharded Data Parallelism + Tensor Parallelism\n\nAs we mentioned earlier, TP should be applied within a node to utilize the high-bandwidth intra-node communications, thus, combining TP and FSDP involves sharding the model across nodes using FSDP, and within a node using TP. To a certain degree, this potentially offers a neat solution to all three of the issues above: the latency costs from FSDP could be reduced by a factor of 8, layers that are too large to fit on a single device are now evenly distributed across devices, and since each TP group receives an identical batch of data, we can also reduce our global batch size by a factor of 8. However, if this remains insufficient, we are unable to increase the TP size across nodes and must consider an alternative approach.\n\nIn Accelerate you can combine TP and FSDP by defining both dp_shard_size and tp_size in ParallelismConfig , whilst in Axolotl you can add both of the dp_shard_size and tensor_parallel_size config fields.\n\nFully Sharded Data Parallelism + Context Parallelism\n\nThis is a 2D parallelism strategy that combines FSDP and CP, and while this is not very commonly used as CP already combines with FSDP (more on why in the accelerate concept guide), it can be useful in some cases i.e. when requiring a large sequence length, consequently requiring a large cp_size . If this still doesn't fit into your memory budget, you can apply FSDP on top of this, further reducing the memory usage.\n\nIn Accelerate you can combine CP and FSDP by defining both dp_shard_size and cp_size in ParallelismConfig , whilst in Axolotl you can add both of the dp_shard_size and context_parallel_size config fields.\n\nHybrid Sharded Data Parallelism + Tensor Parallelism\n\nWith a sufficiently large world size (note: while the minimum world size for 3D parallelism is 8, it is most effective at much larger scales), we can consider combining HSDP with TP which creates a hierarchy where DP first replicates the model across groups of nodes, FSDP then shards the model within each group, and TP splits individual layers within each node. You might consider this approach when facing all of the scaling constraints we mentioned above, as it provides the most flexibility to adapt to your specific training setup by making trade-offs between memory usage and throughput.\n\nIn Accelerate you can combine HSDP and TP by defining all of dp_shard_size , dp_replicate_size , and tp_size in ParallelismConfig . Similarly in Axolotl you can add all of the dp_shard_size , dp_replicate_size , and tensor_parallel_size config fields.\n\nUsage notes\n\nThere are additional ways to combine multiple parallelisms which we haven't covered, such as 4D parallel using HSDP + TP + CP, but they operate very similarly to the techniques we've already covered. Most of all, we encourage you to play with different techniques and configurations - this is the best way to gain an intuition for the different ways in which you can make memory/throughput trade-offs.\n\nBelow are some additional tips you may find useful when working in distributed settings:",
    "link": "https://huggingface.co/blog/accelerate-nd-parallel",
    "Summary": "To see how this works, recall that the attention computation is described by the equation: Attention ( Q , K , V ) = softmax ( Q K T ) V \\text{Attention}(Q, K, V) = \\text{softmax}(QK^T)V Attention(Q,K,V)=softmax(QKT)VWhere Q Q Q, K K K, and V V V are the query, key, and value matrices respectively.\nFor example, with a world size of W W W GPUs and sequence length n n n:GPU 0 computes Q 1 : n / W Q_{1:n/W} Q 1 : n / W \u200b , K 1 : n / W K_{1:n/W} K 1 : n / W \u200b , V 1 : n / W V_{1:n/W} V 1 : n / W \u200b, , GPU 1 computes Q n / W + 1 : 2 n / W Q_{n/W+1:2n/W} Q n / W + 1 : 2 n / W \u200b , K n / W + 1 : 2 n / W K_{n/W+1:2n/W} K n / W + 1 : 2 n / W \u200b , V n / W + 1 : 2 n / W V_{n/W+1:2n/W} V n / W + 1 : 2 n / W \u200b, , ...GPU ( W \u2212 1 ) (W-1) ( W \u2212 1 ) computes Q ( W \u2212 1 ) n / W + 1 : n Q_{(W-1)n/W+1:n} Q ( W \u2212 1 ) n / W + 1 : n \u200b , K ( W \u2212 1 ) n / W + 1 : n K_{(W-1)n/W+1:n} K ( W \u2212 1 ) n / W + 1 : n \u200b , V ( W \u2212 1 ) n / W + 1 : n V_{(W-1)n/W+1:n} V ( W \u2212 1 ) n / W + 1 : n \u200bHow do we ensure the attention is computed correctly?\nEach GPU sends its shard of K K K , V V V to the next GPU in the ring.\nEach GPU receives a different shard of K K K , V V V from the previous GPU in the ring.\nusing the received K K K , V V V shards.",
    "Keywords": [
      "data",
      "q",
      "training",
      "guide",
      "k",
      "ndparallel",
      "parallelism",
      "efficient",
      "fsdp",
      "attention",
      "tp",
      "accelerate",
      "w",
      "v",
      "n",
      "multigpu"
    ]
  },
  {
    "Title": "Vision Language Model Alignment in TRL \u26a1\ufe0f",
    "Authors": [],
    "Publish Date": null,
    "Text": "Vision Language Model Alignment in TRL \u26a1\ufe0f\n\nPublished August 7, 2025 Update on GitHub\n\nVision Language Models (VLMs) are getting stronger, but aligning them to human preferences still matters. In TRL, we already showed how to post-train VLMs with Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO). This time, we\u2019re going further.\n\ntl;dr Here\u2019s what\u2019s new in TRL:\n\nMixed Preference Optimization (MPO)\n\nGroup Relative Policy Optimization (GRPO)\n\nGroup Sequence Policy Optimization (GSPO) (a variant of GRPO)\n\nThese go beyond pairwise DPO, extracting richer signals from preference data and scaling better with modern VLMs.\n\nWe\u2019ve also extended existing methods to support VLMs:\n\nReinforce Leave One Out (RLOO)\n\nOnline Direct Preference Optimization (Online DPO)\n\nThis enables more efficient and scalable multimodal alignment.\n\nFinally:\n\nNative Supervised Fine-tuning support for Vision Language Models\n\nTraining scripts and demo notebooks to help you get started quickly\n\nTable of Contents\n\nAlignment for Vision Language Models\n\nTraditionally, you would take a base model, apply SFT to follow instructions, and then apply DPO to align it to preferential data. Previously, we adapted this approach to Vision Language Models (VLMs) and validated it on IDEFICS2, showing improvement in model responses.\n\nDPO works by optimizing preferences between pairs of model responses using a contrastive loss: you have a chosen and a rejected answer and you optimize your preferences based on what you want and don\u2019t want.\n\nBut in the last year, new multimodal alignment methods have gained popularity, GRPO and MPO, that can push VLM performance even further. At the end of the blog post you can find a table that showcases the differences between model responses.\n\nMixed Preference Optimization (MPO)\n\nAligning multimodal models with SFT to do reasoning tasks falls short due to distribution shift. Meanwhile, models aligned with DPO fail to generate coherent rationales and might generate repetitive responses. To address this, there\u2019s a new technique called Mixed Preference Optimization (MPO) specifically made for multimodal models. This method is essentially an extension of DPO with multiple losses: preference loss from DPO (sigmoid), quality loss from Binary Classifier Optimization (BCO), and generation loss from SFT. According to the paper, simply switching to this combined loss results in 6.2 pts improvement in MathVista!\n\nSince this is only modifying the loss, we added combined loss support to TRL's DPOTrainer class. To use it, you can initialize the DPOConfig as follows:\n\nmpo_config = DPOConfig( loss_type=[ \"sigmoid\" , \"bco_pair\" , \"sft\" ], loss_weights=[ 0.8 , 0.2 , 1.0 ], )\n\nThen initialize the DPOTrainer :\n\nmpo_trainer = DPOTrainer( model=model_id, args=mpo_config, processing_class=tokenizer, train_dataset=dataset, ) mpo_trainer.train()\n\nAnd that\u2019s it! If you want to explore further, you can find a complete notebook example here.\n\nMultimodal Group Relative Policy Optimization (GRPO)\n\nGroup Relative Policy Optimization (GRPO) is a cutting-edge alignment method initially introduced in DeepSeek Math paper and later integrated to DeepSeek R1, the groundbreaking LLM. It\u2019s an addition to PPO where the policy updates are done over groups (batches of trajectories that represent how a dialogue rolls out). This feature makes it more robust to reward noise, as the noise averages out within groups. Since the model learns broader sense of a good response rather than singular high reward samples, this method also makes the model highly performant.\n\nIn TRL, we now introduce GRPO support for vision language models. We will not provide a full training script example, as you can find it in the notebook. Instead, we'll focus on highlighting the key component and concepts.\n\nTo make the training script work effectively, we need to validate that the format of the answer is correct and that the solution itself is close to the completed parts, so we write two reward functions. In order to really see improvements in the latter reward, you would need a rather maximalist setup, where you have relatively larger models, a lot of generations, and a high-quality, diverse dataset.\n\nimport re from math_verify import LatexExtractionConfig, parse, verify def format_reward ( completions, **kwargs ): \"\"\"Reward function that checks if the completion has a specific format.\"\"\" pattern = r\"^<think>.*?</think>\\s*<answer>.*?</answer>$\" matches = [re. match (pattern, content) for content in completions] rewards_list = [ 1.0 if match else 0.0 for match in matches] rewards = [ 1.0 if match else 0.0 for match in matches] print (completions) print (rewards) return rewards def accuracy_reward ( completions, **kwargs ): \"\"\"Reward function that checks if the completion is the same as the ground truth.\"\"\" solutions = kwargs[ 'solution' ] completion_contents = [completion[ 0 ][ \"content\" ] for completion in completions] rewards = [] for content, solution in zip (completion_contents, solutions): gold_parsed = parse(solution, extraction_mode= \"first_match\" , extraction_config=[LatexExtractionConfig()]) answer_parsed = parse(content, extraction_mode= \"first_match\" , extraction_config=[LatexExtractionConfig()]) if len (gold_parsed) != 0 : try : rewards.append( float (verify(answer_parsed, gold_parsed))) except Exception: rewards.append( 0.0 ) else : rewards.append( 1.0 ) return rewards\n\nThen, you can initialize GRPOConfig and GRPOTrainer, pass in the reward functions we defined above and call train() to start training.\n\nfrom trl import GRPOConfig, GRPOTrainer training_args = GRPOConfig( learning_rate= 1e-5 , max_prompt_length= None , ... ) trainer = GRPOTrainer( model=model_id, reward_funcs=[format_reward, accuracy_reward], args=training_args, train_dataset=train_dataset, ) trainer.train()\n\nExplore the full notebook example here.\n\nGroup Sequence Policy Optimization (GSPO)\n\nGroup Sequence Policy Optimization (GSPO) is a RL alignment algorithm recently released by Qwen that overcomes some limitations of GRPO. It achieves a more stable training computing importance sampling weights at the sequence level instead of per-token. Its benefits are more relevant in MoE style models.\n\nLatest TRL also introduces support for GSPO and since it\u2019s a variant of GRPO's loss, it comes with multimodal support. To create the trainer, the process is the same as with GRPO, but adding the following extra params (values are extracted from the paper).\n\nfrom trl import GRPOConfig training_args = GRPOConfig( ... importance_sampling_level= \"sequence\" , epsilon= 3e-4 , epsilon_high= 4e-4 , beta= 0.0 , loss_type= \"grpo\" , steps_per_generation= 1 , steps_per_generation= 4 )\n\nComparison\n\nHere's a table summarizing model outputs for Qwen2.5VL-3B fine-tuned with the techniques discussed above. Note that we've done minimal runs on dataset subsets, and the models were fine-tuned on different datasets, so the comparison is made for vibe-check.\n\nCheck out the answers here Image: Question: In the given figure, if the angle APO is 25.0 degrees, what is the measure of angle AOB? Choices: A: 140\u00b0 B: 130\u00b0 C: 120\u00b0 D: 110\u00b0 Answer in the Dataset: To find the measure of angle AOB, we can use the property that the sum of the interior angles of a quadrilateral is equal to 360 degrees. Applying this property, we can calculate angle AOB as 360 - angle OAP - angle OBP - angle P. Since angle OAP and angle OBP are both 90 degrees, and angle P is given as 50 degrees, we can substitute these values into the equation to get angle AOB = 360 - 90 - 90 - 50 = 130 degrees. Therefore, the measure of angle AOB is 130 degrees. Therefore, the answer is option B. Answer:B Base Model Output: To find the measure of angle AOB, we need to use the properties of angles subtended by the same arc in a circle. Specifically, the angle subtended by an arc at the center of the circle (angle AOB) is twice the angle subtended by the same arc at any point on the circumference (angle APO).Given that angle APO = 25\u00b0, we can calculate angle AOB as follows: Since angle AOB is twice angle APO, we have:[ Angle AOB = 2 * Angle APO ][ Angle AOB = 2 * 25\u00b0 ][ Angle AOB = 50\u00b0 ]However, this calculation seems incorrect based on the provided choices. Let's re-evaluate using the correct property.The correct property states that the angle subtended by an arc at the center of the circle is twice the angle subtended by the same arc at any point on the circumference. Therefore, the correct calculation should be: Angle AOB = 2 * Angle APO = 2 * 25\u00b0 = 50\u00b0But since this does not match the given choices, let's consider another approach. If angle APO is 25\u00b0, and considering the properties of cyclic quadrilaterals or the fact that the sum of opposite angles in a cyclic quadrilateral is 180\u00b0, we might need to look at the supplementary angles formed by the arc and the chord.Let's assume the correct interpretation involves finding the supplementary angle to the given angle APO which would give us the measure of angle AOB. Angle AOB = 180\u00b0 - Angle APO = 180\u00b0 - 25\u00b0 = 155\u00b0This still doesn't match the choices. Given the choices provided, it seems there might be a misunderstanding or a different interpretation needed. However, based on standard geometric principles, the correct answer should be: Angle AOB = 2 * Angle APO = 2 * 25\u00b0 = 50\u00b0 If this is not the case, please provide additional context or clarify the problem statement. MPO Output: To find the measure of angle AOB, we need to use the properties of angles subtended by the same arc in a circle. Specifically, the angle subtended by an arc at the center of the circle (angle AOB) is twice the angle subtended by the same arc at any point on the circumference (angle APO).Given that angle APO = 25\u00b0, we can calculate angle AOB as follows: Since angle AOB is twice the angle APO, we have:[ Angle AOB = 2 * Angle APO ][ Angle AOB = 2 * 25\u00b0 ][ Angle AOB = 50\u00b0 ]However, this calculation seems incorrect based on the provided choices. Let's re-evaluate using the correct property.The correct property states that the angle subtended by an arc at the center of the circle is twice the angle subtended by the same arc at any point on the circumference. Therefore, the correct calculation should be: Angle AOB = 2 * Angle APO = 2 * 25\u00b0 = 50\u00b0But since this does not match the given choices, let's consider another approach. If we look at the triangle OAP, angle AOP is 90\u00b0 because it is a right angle (as it is inscribed in a semicircle). Now, angle APO is given as 25\u00b0, so angle AOA would be 65\u00b0 (since it is supplementary to angle APO). Therefore, angle AOB would be half of this, which is 32.5\u00b0. However, this also doesn't match the choices.Let's consider the possibility that the problem might involve a different interpretation or a different geometric configuration. Given the choices, let's assume there might be a mistake in the problem statement or the choices themselves. If we were to choose from the given options, none of them seem to fit correctly based on standard geometric principles for this type of problem. GRPO Output: To find the measure of angle AOB, we need to understand that angle AOB is an inscribed angle that intercepts the same arc as the central angle AOP. According to the Inscribed Angle Theorem, the measure of an inscribed angle is half the measure of the central angle that subtends the same arc.Given:- Angle APO = 25.0 degreesSince angle AOB is an inscribed angle intercepting the same arc as the central angle AOP, we can use the Inscribed Angle Theorem:The measure of angle AOB is half the measure of angle AOP.Therefore, the measure of angle AOB is 130 degrees. GSPO Output: To find the measure of angle AOB, we need to understand that angle AOB is an inscribed angle that intercepts the same arc as the central angle AOP. According to the Inscribed Angle Theorem, the measure of an inscribed angle is half the measure of the central angle that subtends the same arc.Given:- Angle APO = 25.0 degreesSince angle AOB is an inscribed angle that intercepts the same arc as the central angle AOP, we can use the Inscribed Angle Theorem to find the measure of angle AOB:The measure of angle AOB is half the measure of angle AOP.Therefore, the answer is B: 130\u00b0.\n\nFurther Extensions for VLMs\n\nIn addition to MPO, GRPO, and GSPO, TRL now supports Reinforce Leave One Out (RLOO) and Online Direct Preference Optimization (Online DPO) for Vision Language Models (VLMs), enabling alignment on multimodal datasets.\n\nReinforce Leave One Out (RLOO)\n\nRLOO now supports common VLMs. You can find a complete training example in the rloo_vlm.py script.\n\nHere\u2019s how to set up a RLOOTrainer :\n\ntrainer = RLOOTrainer( model=model_name, args=training_args, reward_funcs=[think_format_reward, accuracy_reward], train_dataset=train_dataset, eval_dataset=eval_dataset, ) trainer.train()\n\nAnd to launch training directly from the example script:\n\nCUDA_VISIBLE_DEVICES=1,2 python3 examples/scripts/rloo_vlm.py --model_name_or_path Qwen/Qwen2.5-VL-3B-Instruct\n\nOnline Direct Preference Optimization (Online DPO)\n\nOnline DPO also supports VLMs. See the online_dpo_vlm.py script for a simple example.\n\nTo run the example script (vLLM integration will be discussed later):\n\nCUDA_VISIBLE_DEVICES=1,2 python3 examples/scripts/online_dpo_vlm.py --model_name_or_path Qwen/Qwen2.5-VL-3B-Instruct --use_vllm --vllm_mode server\n\nThese scripts are ready-to-run for VLM training; full parameter tuning is documented in TRL: Online DPO trainer RLOO Trainer.\n\nNative Supervised Fine-tuning Support\n\nPreviously, SFTTrainer was partially supporting vision language models. This was primarily due to many differences across VLM implementations in transformers API. With the standardization of the transformers API, we have shipped a full support for vision language models. You can simply initialize SFTTrainer with a VLM.\n\nfrom trl import SFTConfig, SFTTrainer from datasets import load_dataset trainer = SFTTrainer( model= \"Qwen/Qwen2.5-VL-3B-Instruct\" , args=SFTConfig(max_length= None ), train_dataset=load_dataset( \"trl-lib/llava-instruct-mix\" , split= \"train\" ), ) trainer.train()\n\nTo train a VLM, you need to provide a dataset with an additional images column containing the images to be processed. You can take a look at Dataset Formats \u2014 Vision Datasets for more information on how it should look like. A good example is LLaVA Instruct Mix.\n\nWe also have a sft_vlm.py script that works out of the box for transformers vision language models.\n\nvLLM Integration in TRL\n\nvLLM is integrated in TRL to support online alignment methods where you need to generate samples during training. Running the example scripts like the following enables vLLM:\n\nCUDA_VISIBLE_DEVICES=1,2 python3 examples/scripts/grpo_vlm.py --model_name_or_path Qwen/Qwen2.5-VL-3B-Instruct --use_vllm --vllm_mode colocate\n\nThere\u2019s mainly two modes: colocate and server . colocate runs vLLM in the same process as the training loop, sharing the same GPU between training and generation, creating a vLLM LLM instance inside the GRPOTrainer . Meanwhile server requires you to serve vLLM separately in a different process where you can hit the server. You can start this server with the command:\n\ntrl vllm-serve --model Qwen/Qwen2.5-VL-3B-Instruct --tensor-parallel-size 1\n\nThen you can run the script as follows.\n\nCUDA_VISIBLE_DEVICES=1,2 python3 examples/scripts/grpo_vlm.py --model_name_or_path Qwen/Qwen2.5-VL-3B-Instruct --use_vllm --vllm_mode server\n\nOne more tip: we have added support for using vLLM with transformers backend in TRL. You can enable it when running a script with colocate or when serving the model by passing the --vllm_model_impl transformers flag.\n\nYou can read more about vLLM integration in TRL here.\n\nUseful Resources\n\nBelow, you can find a compilation of resources to explore the alignment of VLMs in detail. Enjoy!",
    "link": "https://huggingface.co/blog/trl-vlm-alignment",
    "Summary": "Vision Language Model Alignment in TRL \u26a1\ufe0fPublished August 7, 2025 Update on GitHubVision Language Models (VLMs) are getting stronger, but aligning them to human preferences still matters.\nApplying this property, we can calculate angle AOB as 360 - angle OAP - angle OBP - angle P. Since angle OAP and angle OBP are both 90 degrees, and angle P is given as 50 degrees, we can substitute these values into the equation to get angle AOB = 360 - 90 - 90 - 50 = 130 degrees.\nSpecifically, the angle subtended by an arc at the center of the circle (angle AOB) is twice the angle subtended by the same arc at any point on the circumference (angle APO).Given that angle APO = 25\u00b0, we can calculate angle AOB as follows: Since angle AOB is twice angle APO, we have:[ Angle AOB = 2 * Angle APO ][ Angle AOB = 2 * 25\u00b0 ][ Angle AOB = 50\u00b0 ]However, this calculation seems incorrect based on the provided choices.\nSpecifically, the angle subtended by an arc at the center of the circle (angle AOB) is twice the angle subtended by the same arc at any point on the circumference (angle APO).Given that angle APO = 25\u00b0, we can calculate angle AOB as follows: Since angle AOB is twice the angle APO, we have:[ Angle AOB = 2 * Angle APO ][ Angle AOB = 2 * 25\u00b0 ][ Angle AOB = 50\u00b0 ]However, this calculation seems incorrect based on the provided choices.\nNow, angle APO is given as 25\u00b0, so angle AOA would be 65\u00b0 (since it is supplementary to angle APO).",
    "Keywords": [
      "trl",
      "models",
      "vision",
      "alignment",
      "language",
      "optimization",
      "angle",
      "measure",
      "inscribed",
      "apo",
      "aob",
      "model",
      "arc"
    ]
  },
  {
    "Title": "Welcome GPT OSS, the new open-source model family from OpenAI!",
    "Authors": [],
    "Publish Date": null,
    "Text": "Welcome GPT OSS, the new open-source model family from OpenAI!\n\nPublished August 5, 2025 Update on GitHub\n\nGPT OSS is a hugely anticipated open-weights release by OpenAI, designed for powerful reasoning, agentic tasks, and versatile developer use cases. It comprises two models: a big one with 117B parameters ( gpt-oss-120b ), and a smaller one with 21B parameters ( gpt-oss-20b ). Both are mixture-of-experts (MoEs) and use a 4-bit quantization scheme (MXFP4), enabling fast inference (thanks to fewer active parameters, see details below) while keeping resource usage low. The large model fits on a single H100 GPU, while the small one runs within 16GB of memory and is perfect for consumer hardware and on-device applications.\n\nTo make it even better and more impactful for the community, the models are licensed under the Apache 2.0 license, along with a minimal usage policy:\n\nWe aim for our tools to be used safely, responsibly, and democratically, while maximizing your control over how you use them. By using gpt-oss, you agree to comply with all applicable law.\n\nAccording to OpenAI, this release is a meaningful step in their commitment to the open-source ecosystem, in line with their stated mission to make the benefits of AI broadly accessible. Many use cases rely on private and/or local deployments, and we at Hugging Face are super excited to welcome OpenAI to the community. We believe these will be long-lived, inspiring and impactful models.\n\nContents\n\nOverview of Capabilities and Architecture\n\n21B and 117B total parameters, with 3.6B and 5.1B active parameters, respectively.\n\n4-bit quantization scheme using mxfp4 format. Only applied on the MoE weights. As stated, the 120B fits in a single 80 GB GPU and the 20B fits in a single 16GB GPU.\n\nReasoning, text-only models; with chain-of-thought and adjustable reasoning effort levels.\n\nInstruction following and tool use support.\n\nInference implementations using transformers, vLLM, llama.cpp, and ollama.\n\nResponses API is recommended for inference.\n\nLicense: Apache 2.0, with a small complementary use policy.\n\nArchitecture\n\nToken-choice MoE with SwiGLU activations.\n\nWhen calculating the MoE weights, a softmax is taken over selected experts (softmax-after-topk).\n\nEach attention layer uses RoPE with 128K context.\n\nAlternate attention layers: full-context, and sliding 128-token window.\n\nAttention layers use a learned attention sink per-head, where the denominator of the softmax has an additional additive value.\n\nIt uses the same tokenizer as GPT-4o and other OpenAI API models. Some new tokens have been incorporated to enable compatibility with the Responses API.\n\n\n\nBenchmark results from OpenAI GPT OSS models, compared with o3 and o4-mini (Source: OpenAI).\n\nAPI access through Inference Providers\n\nOpenAI GPT OSS models are accessible through Hugging Face\u2019s Inference Providers service, allowing you to send requests to any supported provider using the same JavaScript or Python code. This is the same infrastructure that powers OpenAI\u2019s official demo on gpt-oss.com, and you can use it for your own projects.\n\nBelow is an example that uses Python and the super-fast Cerebras provider. For more info and additional snippets, check the inference providers section in the model cards and the dedicated guide we crafted for these models.\n\nimport os from openai import OpenAI client = OpenAI( base_url= \"https://router.huggingface.co/v1\" , api_key=os.environ[ \"HF_TOKEN\" ], ) completion = client.chat.completions.create( model= \"openai/gpt-oss-120b:cerebras\" , messages=[ { \"role\" : \"user\" , \"content\" : \"How many rs are in the word 'strawberry'?\" , } ], ) print (completion.choices[ 0 ].message)\n\nInference Providers also implements an OpenAI-compatible Responses API, the most advanced OpenAI interface for chat models, designed for more flexible and intuitive interactions.\n\nBelow is an example using the Responses API with the Fireworks AI provider. For more details, check out the open-source responses.js project.\n\nimport os from openai import OpenAI client = OpenAI( base_url= \"https://router.huggingface.co/v1\" , api_key=os.getenv( \"HF_TOKEN\" ), ) response = client.responses.create( model= \"openai/gpt-oss-20b:fireworks-ai\" , input = \"How many rs are in the word 'strawberry'?\" , ) print (response)\n\nLocal Inference\n\nUsing Transformers\n\nYou need to install the latest transformers release (v4.55.1 or later), as well as accelerate and kernels . We also recommend installing triton 3.4 or better, as it unblocks support for mxfp4 quantization on CUDA hardware:\n\npip install --upgrade transformers kernels accelerate \"triton>=3.4\"\n\nThe model weights are quantized in mxfp4 format, which was originally available on GPUs of the Hopper or Blackwell families, but now works on previous CUDA architectures (including Ada, Ampere, and Tesla). Installing triton 3.4, together with the kernels library, makes it possible to download optimized mxfp4 kernels on first use, achieving large memory savings. With these components in place, you can run the 20B model on GPUs with 16 GB of RAM. This includes many consumer cards (3090, 4090, 5080) as well as Colab and Kaggle!\n\nIf the previous libraries are not installed (or you don\u2019t have a compatible GPU), loading the model will fall back to bfloat16 , unpacked from the quantized weights.\n\nThe following snippet shows simple inference with the 20B model. As explained, it runs on 16 GB GPUs when using mxfp4 , or ~48 GB in bfloat16 .\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer model_id = \"openai/gpt-oss-20b\" tokenizer = AutoTokenizer.from_pretrained(model_id) model = AutoModelForCausalLM.from_pretrained( model_id, device_map= \"auto\" , torch_dtype= \"auto\" , ) messages = [ { \"role\" : \"user\" , \"content\" : \"How many rs are in the word 'strawberry'?\" }, ] inputs = tokenizer.apply_chat_template( messages, add_generation_prompt= True , return_tensors= \"pt\" , return_dict= True , ).to(model.device) generated = model.generate(**inputs, max_new_tokens= 100 ) print (tokenizer.decode(generated[ 0 ][inputs[ \"input_ids\" ].shape[- 1 ]:]))\n\nFlash Attention 3\n\nThe models use attention sinks, a technique the vLLM team made compatible with Flash Attention 3. We have packaged and integrated their optimized kernel in kernels-community/vllm-flash-attn3 . At the time of writing, this super-fast kernel has been tested on Hopper cards with PyTorch 2.7 and 2.8. We expect increased coverage in the coming days. If you run the models on Hopper cards (for example, H100 or H200), you need to pip install --upgrade kernels and add the following line to your snippet:\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer model_id = \"openai/gpt-oss-20b\" tokenizer = AutoTokenizer.from_pretrained(model_id) model = AutoModelForCausalLM.from_pretrained( model_id, device_map=\"auto\", torch_dtype=\"auto\", + # Flash Attention with Sinks + attn_implementation=\"kernels-community/vllm-flash-attn3\", ) messages = [ {\"role\": \"user\", \"content\": \"How many rs are in the word 'strawberry'?\"}, ] inputs = tokenizer.apply_chat_template( messages, add_generation_prompt=True, return_tensors=\"pt\", return_dict=True, ).to(model.device) generated = model.generate(**inputs, max_new_tokens=100) print(tokenizer.decode(generated[0][inputs[\"input_ids\"].shape[-1]:]))\n\nThis snippet will download the optimized, pre-compiled kernel code from kernels-community , as explained in our previous blog post. The transformers team has built, packaged, and tested the code, so it\u2019s totally safe for you to use.\n\nOther optimizations\n\nWe recommend you use mxfp4 if your GPU supports it. If you can additionally use Flash Attention 3, then by all means do enable it!\n\nIf your GPU is not compatible with mxfp4 , then we recommend you use MegaBlocks MoE kernels for a nice speed bump. To do so, you just need to adjust your inference code like this:\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer model_id = \"openai/gpt-oss-20b\" tokenizer = AutoTokenizer.from_pretrained(model_id) model = AutoModelForCausalLM.from_pretrained( model_id, device_map=\"auto\", torch_dtype=\"auto\", + # Optimize MoE layers with downloadable` MegaBlocksMoeMLP + use_kernels=True, ) messages = [ {\"role\": \"user\", \"content\": \"How many rs are in the word 'strawberry'?\"}, ] inputs = tokenizer.apply_chat_template( messages, add_generation_prompt=True, tokenize=True, return_tensors=\"pt\", return_dict=True, ).to(model.device) generated = model.generate(**inputs, max_new_tokens=100) print(tokenizer.decode(generated[0][inputs[\"input_ids\"].shape[-1]:]))\n\nMegaBlocks optimized MoE kernels require the model to run on bfloat16 , so memory consumption will be higher than running on mxfp4 . We recommend you use mxfp4 if you can, otherwise opt in to MegaBlocks via use_kernels=True .\n\nAMD ROCm support\n\nOpenAI GPT OSS has been verified on AMD Instinct hardware, and we\u2019re happy to announce initial support for AMD\u2019s ROCm platform in our kernels library, setting the stage for upcoming optimized ROCm kernels in Transformers. MegaBlocks MoE kernel acceleration is already available for OpenAI GPT OSS on AMD Instinct (e.g., MI300-series), enabling better training and inference performance. You can test it with the same inference code shown above.\n\nAMD also prepared a Hugging Face Space for users to try the model on AMD hardware.\n\nSummary of Available Optimizations\n\nAt the time of writing, this table summarizes our recommendations based on GPU compatibility and our tests. We expect Flash Attention 3 (with sink attention) to become compatible with additional GPUs.\n\nmxfp4 Flash Attention 3 (w/ sink attention) MegaBlocks MoE kernels Hopper GPUs (H100, H200) \u2705 \u2705 \u274c CUDA GPUS with 16+ GB of RAM \u2705 \u274c \u274c Other CUDA GPUs \u274c \u274c \u2705 AMD Instinct (MI3XX) \u274c \u274c \u2705 How to enable triton 3.4 + kernels library Use vllm-flash-attn3 from kernels-community use_kernels\n\nEven though the 120B model fits on a single H100 GPU (using mxfp4 ), you can also run it easily on multiple GPUs using accelerate or torchrun . Transformers provides a default parallelization plan, and you can leverage optimized attention kernels as well. The following snippet can be run with torchrun --nproc_per_node=4 generate.py on a system with 4 GPUs:\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer from transformers.distributed import DistributedConfig import torch model_path = \"openai/gpt-oss-120b\" tokenizer = AutoTokenizer.from_pretrained(model_path, padding_side= \"left\" ) device_map = { \"tp_plan\" : \"auto\" , } model = AutoModelForCausalLM.from_pretrained( model_path, torch_dtype= \"auto\" , attn_implementation= \"kernels-community/vllm-flash-attn3\" , **device_map, ) messages = [ { \"role\" : \"user\" , \"content\" : \"Explain how expert parallelism works in large language models.\" } ] inputs = tokenizer.apply_chat_template( messages, add_generation_prompt= True , return_tensors= \"pt\" , return_dict= True , ).to(model.device) outputs = model.generate(**inputs, max_new_tokens= 1000 ) response = tokenizer.decode(outputs[ 0 ]) print ( \"Model response:\" , response.split( \"<|channel|>final<|message|>\" )[- 1 ].strip())\n\nThe OpenAI GPT OSS models have been trained extensively to leverage tool use as part of their reasoning efforts. The chat template we crafted for transformers provides a lot of flexibility, please check our dedicated section later in this post.\n\nLlama.cpp offers native MXFP4 support with Flash Attention, delivering optimal performance across various backends such as Metal, CUDA, and Vulkan, right from the day-0 release.\n\nTo install it, follow the guide in llama.cpp Github\u2019s repository.\n\n# MacOS brew install llama.cpp # Windows winget install llama.cpp\n\nThe recommended way is to use it via llama-server:\n\nllama-server -hf ggml-org/gpt-oss-120b-GGUF -c 0 -fa --jinja --reasoning-format none # Then, access http://localhost:8080\n\nWe support both the 120B and 20B models. For more detailed information, visit this PR or the GGUF model collection.\n\nvLLM\n\nAs mentioned, vLLM developed optimized Flash Attention 3 kernels that support sink attention, so you\u2019ll get best results on Hopper cards. Both the Chat Completion and the Responses APIs are supported. You can install and start a server with the following snippet, which assumes 2 H100 GPUs are used:\n\nvllm serve openai/gpt-oss-120b --tensor-parallel-size 2\n\nOr, use it in Python directly like:\n\nfrom vllm import LLM llm = LLM( \"openai/gpt-oss-120b\" , tensor_parallel_size= 2 ) output = llm.generate( \"San Francisco is a\" )\n\ntransformers serve\n\nYou can use transformers serve to experiment locally with the models, without any other dependencies. You can launch the server with just:\n\ntransformers serve\n\nTo which you can send requests using the Responses API.\n\n# responses API curl -X POST http://localhost:8000/v1/responses \\ -H \"Content-Type: application/json\" \\ -d '{\"input\": [{\"role\": \"system\", \"content\": \"hello\"}], \"temperature\": 1.0, \"stream\": true, \"model\": \"openai/gpt-oss-120b\"}'\n\nYou can also send requests using the standard Completions API:\n\n# completions API curl -X POST http://localhost:8000/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -d '{\"messages\": [{\"role\": \"system\", \"content\": \"hello\"}], \"temperature\": 1.0, \"max_tokens\": 1000, \"stream\": true, \"model\": \"openai/gpt-oss-120b\"}'\n\nGPT OSS models are fully integrated with trl . We have developed a couple of fine-tuning examples using SFTTrainer to get you started:\n\nA LoRA example in the OpenAI cookbook, which shows how the model can be fine-tuned to reason in multiple languages.\n\nA basic fine-tuning script that you can adapt to your needs.\n\nDeploy on Hugging Face Partners\n\nAzure\n\nHugging Face collaborates with Azure on their Azure AI Model Catalog to bring the most popular open-source models \u2014spanning text, vision, speech, and multimodal tasks\u2014 directly into customers environments for secured deployments to managed online endpoints, leveraging Azure\u2019s enterprise-grade infrastructure, autoscaling, and monitoring.\n\nThe GPT OSS models are now available on the Azure AI Model Catalog (GPT OSS 20B, GPT OSS 120B), ready to be deployed to an online endpoints for real time inference.\n\nDell\n\nThe Dell Enterprise Hub is a secure online portal that simplifies training and deploying the latest open AI models on-premise using Dell platforms. Developed in collaboration with Dell, it offers optimized containers, native support for Dell hardware, and enterprise-grade security features.\n\nThe GPT OSS models are now available on Dell Enterprise Hub, ready to be deployed on-prem using Dell platforms.\n\nEvaluating the Model\n\nGPT OSS models are reasoning models: they therefore require a very large generation size (maximum number of new tokens) for evaluations, as their generation will first contain reasoning, then the actual answer. Using too small a generation size risks interrupting the prediction in the middle of reasoning, which will cause false negatives. The reasoning trace should then be removed from the model answer before computing metrics, to avoid parsing errors, especially with math or instruct evaluations.\n\nHere\u2019s an example on how to evaluate the models with lighteval (you need to install from source).\n\ngit clone https://github.com/huggingface/lighteval pip install -e .[dev] # make sure you have the correct transformers version installed! lighteval accelerate \\ \"model_name=openai/gpt-oss-20b,max_length=16384,skip_special_tokens=False,generation_parameters={temperature:1,top_p:1,top_k:40,min_p:0,max_new_tokens:16384}\" \\ \"extended|ifeval|0|0,lighteval|aime25|0|0\" \\ --save-details --output-dir \"openai_scores\" \\ --remove-reasoning-tags --reasoning-tags=\"[('<|channel|>analysis<|message|>','<|end|><|start|>assistant<|channel|>final<|message|>')]\"\n\nFor the 20B model, this should give you 69.5 (+/-1.9) for IFEval (strict prompt), and 63.3 (+/-8.9) for AIME25 (in pass@1), scores within expected range for a reasoning model of this size.\n\nIf you want to do your custom evaluation script, note that to filter out the reasoning tags properly, you will need to use skip_special_tokens=False in the tokenizer, in order to get the full trace in the model output (to filter reasoning using the same string pairs as in the example above) - you can discover why below.\n\nChats and Chat Templates\n\nOpenAI GPT OSS uses the concept of \u201cchannels\u201d in its outputs. Most of the time, you will see an \u201canalysis\u201d channel that contains things that are not intended to be sent to the end-user, like chains of thought, and a \u201cfinal\u201d channel containing messages that are actually intended to be displayed to the user.\n\nAssuming no tools are being used, the structure of the model output looks like this:\n\n<|start|>assistant<|channel|>analysis<|message|>CHAIN_OF_THOUGHT<|end|><|start|>assistant<|channel|>final<|message|>ACTUAL_MESSAGE\n\nMost of the time, you should ignore everything except the text after <|channel|>final<|message|>. Only this text should be appended to the chat as the assistant message, or displayed to the user. There are two exceptions to this rule, though: You may need to include analysis messages in the history during training or if the model is calling external tools.\n\nWhen training: If you\u2019re formatting examples for training, you generally want to include the chain of thought in the final message. The right place to do this is in the thinking key.\n\nchat = [ { \"role\" : \"user\" , \"content\" : \"Hi there!\" }, { \"role\" : \"assistant\" , \"content\" : \"Hello!\" }, { \"role\" : \"user\" , \"content\" : \"Can you think about this one?\" }, { \"role\" : \"assistant\" , \"thinking\" : \"Thinking real hard...\" , \"content\" : \"Okay!\" } ] inputs = tokenizer.apply_chat_template(chat, add_generation_prompt= False )\n\nYou can feel free to include thinking keys in previous turns, or when you\u2019re doing inference rather than training, but they will generally be ignored. The chat template will only ever include the most recent chain of thought, and only in training (when add_generation_prompt=False and the final turn is an assistant turn).\n\nThe reason why we do it this way is subtle: The OpenAI gpt-oss models were trained on multi-turn data where all but the final chain of thought was dropped. This means that when you want to fine-tune an OpenAI gpt-oss model, you should do the same.\n\nLet the chat template drop all chains of thought except the final one\n\nMask the labels on all turns except the final assistant turn, or else you will be training it on the previous turns without chains of thought, which will teach it to emit responses without CoTs. This means that you cannot train on an entire multi-turn conversation as a single sample; instead, you must break it into one sample per assistant turn with only the final assistant turn unmasked each time, so that the model can learn from each turn while still correctly only seeing a chain of thought on the final message each time.\n\nSystem and Developer Messages\n\nOpenAI GPT OSS is unusual because it distinguishes between a \u201csystem\u201d message and a \u201cdeveloper\u201d message at the start of the chat, but most other models only use \u201csystem\u201d. In GPT OSS, the system message follows a strict format and contains information like the current date, the model identity and the level of reasoning effort to use, and the \u201cdeveloper\u201d message is more freeform, which makes it (very confusingly) similar to the \u201csystem\u201d messages of most other models.\n\nTo make GPT OSS easier to use with the standard API, the chat template will treat a message with \u201csystem\u201d or \u201cdeveloper\u201d role as the developer message. If you want to modify the actual system message, you can pass the specific arguments model_identity or reasoning_effort to the chat template:\n\nchat = [ { \"role\" : \"system\" , \"content\" : \"This will actually become a developer message!\" } ] tokenizer.apply_chat_template( chat, model_identity= \"You are OpenAI GPT OSS.\" , reasoning_effort= \"high\" )\n\nTool Use With transformers\n\nGPT OSS supports two kinds of tools: The \u201cbuiltin\u201d tools browser and python, and custom tools supplied by the user. To enable builtin tools, pass their names in a list to the builtin_tools argument of the chat template, as shown below. To pass custom tools, you can pass them either as JSON schema or as Python functions with type hints and docstrings using the tools argument. See the chat template tools documentation for more details, or you can just modify the example below:\n\ndef get_current_weather ( location: str ): \"\"\" Returns the current weather status at a given location as a string. Args: location: The location to get the weather for. \"\"\" return \"Terrestrial.\" chat = [ { \"role\" : \"user\" , \"content\" : \"What's the weather in Paris right now?\" } ] inputs = tokenizer.apply_chat_template( chat, tools=[weather_tool], builtin_tools=[ \"browser\" , \"python\" ], add_generation_prompt= True , return_tensors= \"pt\" )\n\nIf the model chooses to call a tool (indicated by a message ending in <|call|> ), then you should add the tool call to the chat, call the tool, then add the tool result to the chat and generate again:\n\ntool_call_message = { \"role\" : \"assistant\" , \"tool_calls\" : [ { \"type\" : \"function\" , \"function\" : { \"name\" : \"get_current_temperature\" , \"arguments\" : { \"location\" : \"Paris, France\" } } } ] } chat.append(tool_call_message) tool_output = get_current_weather( \"Paris, France\" ) tool_result_message = { \"role\" : \"tool\" , \"content\" : tool_output } chat.append(tool_result_message)\n\nAcknowledgements\n\nThis is an important release for the community and it took a momentous effort across teams and companies to comprehensively support the new models in the ecosystem.\n\nThe authors of this blog post were selected among the ones who contributed content to the post itself, and does not represent dedication to the project. In addition to the author list, others contributed significant content reviews, including Merve and Sergio. Thank you!\n\nThe integration and enablement work involved dozens of people. In no particular order, we'd like to highlight Cyril, Lysandre, Arthur, Marc, Mohammed, Nouamane, Harry, Benjamin, Matt from the open source team. From the TRL team, Ed, Lewis, and Quentin were all involved. We'd also like to thank Cl\u00e9mentine from Evaluations, and David and Daniel from the Kernels team. On the commercial partnerships side we got significant contributions from Simon, Alvaro, Jeff, Akos, Alvaro, and Ivar. The Hub and Product teams contributed Inference Providers support, llama.cpp support, and many other improvements, all thanks to Simon, C\u00e9lina, Pierric, Lucain, Xuan-Son, Chunte, and Julien. Magda and Anna were involved from the legal team.\n\nHugging Face's role is to enable the community to use these models effectively. We are indebted to companies such as vLLM for advancing the field, and cherish our continued collaboration with inference providers to provide ever simpler ways to build on top of them.\n\nAnd of course, we deeply appreciate OpenAI's decision to release these models for the community at large. Here's to many more!",
    "link": "https://huggingface.co/blog/welcome-openai-gpt-oss",
    "Summary": "Welcome GPT OSS, the new open-source model family from OpenAI!\nBenchmark results from OpenAI GPT OSS models, compared with o3 and o4-mini (Source: OpenAI).\nimport os from openai import OpenAI client = OpenAI( base_url= \"https://router.huggingface.co/v1\" , api_key=os.getenv( \"HF_TOKEN\" ), ) response = client.responses.create( model= \"openai/gpt-oss-20b:fireworks-ai\" , input = \"How many rs are in the word 'strawberry'?\"\nMegaBlocks MoE kernel acceleration is already available for OpenAI GPT OSS on AMD Instinct (e.g., MI300-series), enabling better training and inference performance.\nThe GPT OSS models are now available on the Azure AI Model Catalog (GPT OSS 20B, GPT OSS 120B), ready to be deployed to an online endpoints for real time inference.",
    "Keywords": [
      "oss",
      "chat",
      "opensource",
      "models",
      "role",
      "family",
      "content",
      "welcome",
      "attention",
      "openai",
      "gpt",
      "using",
      "model"
    ]
  },
  {
    "Title": "Implementing MCP Servers in Python: An AI Shopping Assistant with Gradio",
    "Authors": [],
    "Publish Date": null,
    "Text": "Implementing MCP Servers in Python: An AI Shopping Assistant with Gradio\n\nPublished July 31, 2025 Update on GitHub\n\nPython Developers, want to give your LLM superpowers? Gradio is the fastest way to do it! With Gradio's Model Context Protocol (MCP) integration, your LLM can plug directly into the thousands of AI models and Spaces hosted on the Hugging Face Hub . By pairing the general reasoning capabilities of LLMs with the specialized abilities of models found on Hugging Face, your LLM can go beyond simply answering text questions to actually solving problems in your daily life.\n\nFor Python developers, Gradio makes implementing powerful MCP servers a breeze, offering features like:\n\nAutomatic conversion of python functions into LLM tools: Each API endpoint in your Gradio app is automatically converted into an MCP tool with a corresponding name, description, and input schema. The docstring of your function is used to generate the description of the tool and its parameters.\n\nEach API endpoint in your Gradio app is automatically converted into an MCP tool with a corresponding name, description, and input schema. The docstring of your function is used to generate the description of the tool and its parameters. Real-time progress notifications: Gradio streams progress notifications to your MCP client, allowing you to monitor the status in real-time without having to implement this feature yourself.\n\nGradio streams progress notifications to your MCP client, allowing you to monitor the status in real-time without having to implement this feature yourself. Automatic file uploads, including support for public URLs and handling of various file types.\n\nImagine this: you hate shopping because it takes too much time, and you dread trying on clothes yourself. What if an LLM could handle this for you? In this post, we'll create an LLM-powered AI assistant that can browse online clothing stores, find specific garments, and then use a virtual try-on model to show you how those clothes would look on you. See the demo below:\n\nThe Goal: Your Personal AI Stylist\n\nTo bring our AI shopping assistant to life, we'll combine three key components:\n\nIDM-VTON Diffusion Model: This AI model is responsible for the virtual try-on functionality. It can edit existing photos to make it appear as if a person is wearing a different garment. We'll be using the Hugging Face Space for IDM-VTON, accessible here. Gradio: Gradio is an open-source Python library that makes it easy to build AI-powered web applications and, crucially for our project, to create MCP servers. Gradio will act as the bridge, allowing our LLM to call the IDM-VTON model and other tools. Visual Studio Code's AI Chat Feature: We'll use VS Code's built-in AI chat, which supports adding arbitrary MCP servers, to interact with our AI shopping assistant. This will provide a user-friendly interface for issuing commands and viewing the virtual try-on results.\n\nBuilding the Gradio MCP Server\n\nThe core of our AI shopping assistant is the Gradio MCP server. This server will expose one main tool:\n\nvton_generation : This function will take a human model image and a garment image as input and use the IDM-VTON model to generate a new image of the person wearing the garment.\n\nHere's the Python code for our Gradio MCP server:\n\nfrom gradio_client import Client, handle_file import gradio as gr import re client = Client( \"freddyaboulton/IDM-VTON\" , hf_token= \"<Your-token>\" ) def vton_generation ( human_model_img: str , garment: str ): \"\"\"Use the IDM-VTON model to generate a new image of a person wearing a garment.\"\"\" \"\"\" Args: human_model_img: The human model that is modelling the garment. garment: The garment to wear. \"\"\" output = client.predict( dict ={ \"background\" : handle_file(human_model_img), \"layers\" :[], \"composite\" : None }, garm_img=handle_file(garment), garment_des= \"\" , is_checked= True , is_checked_crop= False , denoise_steps= 30 , seed= 42 , api_name= \"/tryon\" ) return output[ 0 ] vton_mcp = gr.Interface( vton_generation, inputs=[ gr.Image( type = \"filepath\" , label= \"Human Model Image URL\" ), gr.Image( type = \"filepath\" , label= \"Garment Image URL or File\" ) ], outputs=gr.Image( type = \"filepath\" , label= \"Generated Image\" ) ) if __name__ == \"__main__\" : vton_mcp.launch(mcp_server= True )\n\nBy setting mcp_server=True in the launch() method, Gradio automatically converts our Python functions into MCP tools that LLMs can understand and use. The docstrings of our functions are used to generate descriptions of the tools and their parameters.\n\nThe original IDM-VTON space was implemented with Gradio 4.x which precedes the automatic MCP functionality. So in this demo, we'll be building a Gradio interface that queries the original space via the Gradio API client.\n\nFinally, run this script with python.\n\nConfiguring VS Code\n\nTo connect our Gradio MCP server to VS Code's AI chat, we'll need to edit the mcp.json file. This configuration tells the AI chat where to find our MCP server and how to interact with it.\n\nYou can find this file by typing MCP in the command panel and selecting MCP: Open User Configuration . Once you open it, make sure the following servers are present:\n\n{ \"servers\" : { \"vton\" : { \"url\" : \"http://127.0.0.1:7860/gradio_api/mcp/\" } , \"playwright\" : { \"command\" : \"npx\" , \"args\" : [ \"-y\" , \"@playwright/mcp@latest\" ] } } }\n\nThe playwright MCP server will let our AI assistant browse the web.\n\nMake sure the URL of the vton server matches the url printed to the console in the previous section. To run the playwright MCP server, you need to have node installed.\n\nPutting It All Together\n\nNow we can start interacting with our AI shopping assistant. Open a new chat in VS Code, and you can ask the assistant something like \"Browse the Uniqlo website for blue t-shirts, and show me what I would look like in three of them, using my photo at [your-image-url].\"\n\nSee the above video for an example!\n\nConclusion\n\nThe combination of Gradio, MCP, and powerful AI models like IDM-VTON opens up exciting possibilities for creating intelligent and helpful AI assistants. By following the steps outlined in this blog post, you can build your own assistant to solve the problems you care most about!",
    "link": "https://huggingface.co/blog/gradio-vton-mcp",
    "Summary": "Implementing MCP Servers in Python: An AI Shopping Assistant with GradioPublished July 31, 2025 Update on GitHubPython Developers, want to give your LLM superpowers?\nSee the demo below:The Goal: Your Personal AI StylistTo bring our AI shopping assistant to life, we'll combine three key components:IDM-VTON Diffusion Model: This AI model is responsible for the virtual try-on functionality.\nGradio: Gradio is an open-source Python library that makes it easy to build AI-powered web applications and, crucially for our project, to create MCP servers.\nVisual Studio Code's AI Chat Feature: We'll use VS Code's built-in AI chat, which supports adding arbitrary MCP servers, to interact with our AI shopping assistant.\nBuilding the Gradio MCP ServerThe core of our AI shopping assistant is the Gradio MCP server.",
    "Keywords": [
      "servers",
      "gradio",
      "shopping",
      "image",
      "garment",
      "ai",
      "implementing",
      "assistant",
      "server",
      "model",
      "mcp",
      "python"
    ]
  },
  {
    "Title": "Introducing Trackio: A Lightweight Experiment Tracking Library from Hugging Face",
    "Authors": [],
    "Publish Date": null,
    "Text": "Introducing Trackio: A Lightweight Experiment Tracking Library from Hugging Face\n\nPublished July 29, 2025 Update on GitHub\n\ntrackio\n\nwandb\n\nBackground\n\nTL;DR: Trackio is a new, open-source, and free experiment tracking Python library that provides a local dashboard and seamless integration with Hugging Face Spaces for easy sharing and collaboration. Sinceis a drop-in replacement for, you can get started with the syntax you already know!\n\nIf you have trained your own machine learning model, you know how important it is to be able to track metrics, parameters, and hyperparameters during training and visualize them afterwards to better understand your training run.\n\nMost machine learning researchers use specific experiment tracking libraries to do this. However, these libraries can be paid, require complex setup, or lack the flexibility needed for rapid experimentation and sharing.\n\nWhy We Switched to Trackio\n\nAt Hugging Face, our science team has started using Trackio for our research projects, and we've found several key advantages over other tracking solutions:\n\nEasy Sharing and Embedding: Trackio makes it incredibly simple to share training progress with colleagues or embed plots directly in blog posts and documentation using iframes. This is especially valuable when you want to showcase specific training curves or metrics without requiring others to set up accounts or navigate complex dashboards.\n\nStandardization and Transparency: Metrics like GPU energy usage are important to track and share with the community so we can have a better idea of the energy demands and environmental impacts of model training. Using Trackio, which directly gets information from the nvidia-smi command, makes it easy to quantify and compare energy usage and to add it to model cards.\n\nData Accessibility: Unlike some tracking tools that lock your data behind proprietary APIs, Trackio makes it straightforward to extract and analyze the data being recorded. This is crucial for researchers who need to perform custom analysis or integrate training metrics into their research workflows.\n\nFlexibility for Experimentation: Trackio's lightweight design allows us to easily experiment with new tracking features during training runs. For instance, we can decide when to move tensors from GPU to CPU when logging tensors while training, which significantly improves training throughput when you need to track model/intermediate states without impacting performance.\n\nUsing Trackio\n\nOk then, so what is Trackio and how do you use it? Trackio is an open-source Python library that lets you track any metrics and visualize them using a local Gradio dashboard. You can also sync this dashboard to Hugging Face Spaces, which means you can then share the dashboard with other users simply by sharing a URL. Since Spaces can be private or public, this means you can share a dashboard publicly or just within members of your Hugging Face organization.\n\nInstalling\n\nYou can install trackio using pip:\n\npip install trackio\n\nOr, if you prefer using uv :\n\nuv pip install trackio\n\nUsage\n\ntrackio is designed to be a drop-in replacement for experiment tracking libraries like wandb . The API is compatible with wandb.init , wandb.log , and wandb.finish , so you can simply import trackio as wandb in your code.\n\n- import wandb + import trackio as wandb\n\nHere is an example:\n\nimport trackio import random import time runs = 3 epochs = 8 def simulate_multiple_runs (): for run in range (runs): trackio.init(project= \"fake-training\" , config={ \"epochs\" : epochs, \"learning_rate\" : 0.001 , \"batch_size\" : 64 }) for epoch in range (epochs): train_loss = random.uniform( 0.2 , 1.0 ) train_acc = random.uniform( 0.6 , 0.95 ) val_loss = train_loss - random.uniform( 0.01 , 0.1 ) val_acc = train_acc + random.uniform( 0.01 , 0.05 ) trackio.log({ \"epoch\" : epoch, \"train_loss\" : train_loss, \"train_accuracy\" : train_acc, \"val_loss\" : val_loss, \"val_accuracy\" : val_acc }) time.sleep( 0.2 ) trackio.finish() simulate_multiple_runs()\n\nVisualizing Results\n\nAfter logging your experiments, you can launch the dashboard to visualize your results. Run the following command in your terminal:\n\ntrackio show\n\nOr, launch it from Python:\n\nimport trackio trackio.show()\n\nYou can also specify a project name:\n\ntrackio show --project \"my project\"\n\nOr in Python:\n\ntrackio.show(project= \"my project\" )\n\nSharing with \ud83e\udd17 Spaces\n\nTo sync your local dashboard to Hugging Face Spaces, simply pass a space_id to init :\n\ntrackio.init(project= \"fake-training\" , space_id= \"org_name/space_name\" )\n\nIf you are hosting your dashboard on Spaces, you can simply share the URL or embed it anywhere using an iframe:\n\n< iframe src = \"https://org_name-space_name.hf.space/?project=fake-training&metrics=train_loss,train_accuracy&sidebar=hidden\" width = 600 height = 600 frameBorder = \"0\" > </ iframe >\n\nSince Spaces can be private or public, this means you can share a dashboard publicly or just within members of your Hugging Face organization \u2014 all for free!\n\nWhen you sync your Trackio dashboard to Hugging Face Spaces, the data is logged to an ephemeral Sqlite database on Spaces. Because this database is reset if your Space restarts, Trackio also converts the Sqlite database to a Parquet dataset and backs it up to a Hugging Face Dataset every 5 minutes. This means you can visualize your logged metrics in a Hugging Face dataset at any time easily:\n\nTip: you can set the name of this dataset by passing in a dataset_id to trackio.init() .\n\nIntegrated with \ud83e\udd17 Transformers and \ud83e\udd17 Accelerate\n\nTrackio integrates natively with Hugging Face libraries like transformers and accelerate , so you can log metrics with minimal setup.\n\nWith transformers.Trainer :\n\nimport numpy as np from datasets import Dataset from transformers import Trainer, AutoModelForCausalLM, TrainingArguments data = np.random.randint( 0 , 1000 , ( 8192 , 64 )).tolist() dataset = Dataset.from_dict({ \"input_ids\" : data, \"labels\" : data}) trainer = Trainer( model=AutoModelForCausalLM.from_pretrained( \"Qwen/Qwen3-0.6B\" ), args=TrainingArguments(run_name= \"fake-training\" , report_to= \"trackio\" ), train_dataset=dataset, ) trainer.train()\n\nWith accelerate :\n\nfrom accelerate import Accelerator accelerator = Accelerator(log_with= \"trackio\" ) accelerator.init_trackers( \"fake-training\" ) for step, batch in enumerate (dataloader): ... accelerator.log({ \"training_loss\" : loss}, step=step) accelerator.end_training()\n\nNo extra setup needed\u2014just plug it in and start tracking.\n\nDesign Principles\n\nAPI compatible with popular experiment tracking libraries, making migration both to and from Trackio seamless.\n\nLocal-first: logs and dashboards run and persist locally by default, with the option to host on Hugging Face Spaces.\n\nLightweight and extensible: the core codebase is under 1,000 lines of Python, making it easy to understand and modify.\n\nFree and open-source: all features, including hosting on Hugging Face, are free.\n\nBuilt on top of \ud83e\udd17 Datasets and Spaces for robust data handling and visualization.\n\nNext Steps\n\nTrackio is intentionally lightweight and is currently in beta. Some features found in other tracking tools, such as artifact management, or complex visualizations, are not available yet. If you'd like to have these features, please create issues here: https://github.com/gradio-app/trackio/issues\n\nGiven Trackio's lightweight and open-source nature, we'd love to work with the machine learning community to design an experiment tracking product that works for all of us!",
    "link": "https://huggingface.co/blog/trackio",
    "Summary": "Introducing Trackio: A Lightweight Experiment Tracking Library from Hugging FacePublished July 29, 2025 Update on GitHubtrackiowandbBackgroundTL;DR: Trackio is a new, open-source, and free experiment tracking Python library that provides a local dashboard and seamless integration with Hugging Face Spaces for easy sharing and collaboration.\nYou can also sync this dashboard to Hugging Face Spaces, which means you can then share the dashboard with other users simply by sharing a URL.\nThe API is compatible with wandb.init , wandb.log , and wandb.finish , so you can simply import trackio as wandb in your code.\nWhen you sync your Trackio dashboard to Hugging Face Spaces, the data is logged to an ephemeral Sqlite database on Spaces.\nLocal-first: logs and dashboards run and persist locally by default, with the option to host on Hugging Face Spaces.",
    "Keywords": [
      "import",
      "tracking",
      "training",
      "dashboard",
      "library",
      "face",
      "hugging",
      "experiment",
      "trackio",
      "metrics",
      "lightweight",
      "using",
      "spaces",
      "introducing"
    ]
  },
  {
    "Title": "Parquet Content-Defined Chunking",
    "Authors": [],
    "Publish Date": null,
    "Text": "Parquet Content-Defined Chunking\n\nPublished July 25, 2025 Update on GitHub\n\nReduce Parquet file upload and download times on Hugging Face Hub by leveraging the new Xet storage layer and Apache Arrow\u2019s Parquet Content-Defined Chunking (CDC) feature enabling more efficient and scalable data workflows.\n\nTL;DR: Parquet Content-Defined Chunking (CDC) is now available in PyArrow and Pandas, enabling efficient deduplication of Parquet files on content-addressable storage systems like Hugging Face's Xet storage layer. CDC dramatically reduces data transfer and storage costs by uploading or downloading only the changed data chunks. Enable CDC by passing the use_content_defined_chunking argument:\n\nimport pandas as pd import pyarrow.parquet as pq df.to_parquet( \"hf://datasets/{user}/{repo}/path.parquet\" , use_content_defined_chunking= True ) pq.write_table(table, \"hf://datasets/{user}/{repo}/path.parquet\" , use_content_defined_chunking= True )\n\nTable of Contents\n\nIntroduction\n\nApache Parquet is a columnar storage format that is widely used in the data engineering community.\n\nAs of today, Hugging Face hosts nearly 21 PB of datasets, with Parquet files alone accounting for over 4 PB of that storage. Optimizing Parquet storage is therefore a high priority. Hugging Face has introduced a new storage layer called Xet that leverages content-defined chunking to efficiently deduplicate chunks of data reducing storage costs and improving download/upload speeds.\n\nWhile Xet is format agnostic, Parquet's layout and column-chunk (data page) based compression can produce entirely different byte-level representations for data with minor changes, leading to suboptimal deduplication performance. To address this, the Parquet files should be written in a way that minimizes the byte-level differences between similar data, which is where content-defined chunking (CDC) comes into play.\n\nLet's explore the performance benefits of the new Parquet CDC feature used alongside Hugging Face's Xet storage layer.\n\nData Preparation\n\nFor demonstration purposes, we will use a manageable sized subset of OpenOrca dataset.\n\nimport numpy as np import pyarrow as pa import pyarrow.compute as pc import pyarrow.parquet as pq from huggingface_hub import hf_hub_download def shuffle_table ( table, seed= 40 ): rng = np.random.default_rng(seed) indices = rng.permutation( len (table)) return table.take(indices) path = hf_hub_download( repo_id= \"Open-Orca/OpenOrca\" , filename= \"3_5M-GPT3_5-Augmented.parquet\" , repo_type= \"dataset\" ) orca = pq.read_table(path, schema=pa.schema([ pa.field( \"id\" , pa.string()), pa.field( \"system_prompt\" , pa.string()), pa.field( \"question\" , pa.large_string()), pa.field( \"response\" , pa.large_string()), ])) orca = orca.add_column( orca.schema.get_field_index( \"question\" ), \"question_length\" , pc.utf8_length(orca[ \"question\" ]) ) orca = orca.add_column( orca.schema.get_field_index( \"response\" ), \"response_length\" , pc.utf8_length(orca[ \"response\" ]) ) orca = shuffle_table(orca) table = orca[: 100_000 ] table[: 3 ].to_pandas()\n\nid system_prompt question_length question response_length response 0 cot.64099 You are an AI assistant that helps people find... 241 Consider the question. What is the euphrates l... 1663 The question is asking what the Euphrates Rive... 1 flan.1206442 You are an AI assistant. You will be given a t... 230 Single/multi-select question: Is it possible t... 751 It is not possible to conclude that the cowboy... 2 t0.1170225 You are an AI assistant. User will you give yo... 1484 Q:I'm taking a test and have to guess the righ... 128 The passage mainly tells us what things are im...\n\nUpload the table as a Parquet file to Hugging Face Hub\n\nSince pyarrow>=21.0.0 we can use Hugging Face URIs in the pyarrow functions to directly read and write parquet (and other file formats) files to the Hub using the hf:// URI scheme.\n\npq.write_table(table, \"hf://datasets/kszucs/pq/orca.parquet\" ) New Data Upload: 100 %|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 96.1 MB / 96.1 MB, 48.0 kB/s Total Bytes: 96.1 M Total Transfer: 96.1 M\n\nWe can see that the table has been uploaded entirely (total bytes == total transfer) as new data because it is not known to the Xet storage layer yet. Now read it back as a pyarrow table:\n\ndownloaded_table = pq.read_table( \"hf://datasets/kszucs/pq/orca.parquet\" ) assert downloaded_table.equals(table)\n\nNote that all pyarrow functions that accept a file path also accept a Hugging Face URI, like pyarrow datasets, CSV functions, incremental Parquet writer or reading only the parquet metadata:\n\npq.read_metadata( \"hf://datasets/kszucs/pq/orca.parquet\" )\n\n<pyarrow._parquet.FileMetaData object at 0x16ebfa980> created_by: parquet-cpp-arrow version 21.0.0-SNAPSHOT num_columns: 6 num_rows: 100000 num_row_groups: 1 format_version: 2.6 serialized_size: 4143\n\nDifferent Use Cases for Parquet Deduplication\n\nTo demonstrate the effectiveness of the content-defined chunking feature, we will try out how it performs in case of:\n\nRe-uploading exact copies of the table Adding/removing columns from the table Changing column types in the table Appending new rows and concatenating tables Inserting / deleting rows in the table Change row-group size of the table Using Varying File-Level Splits\n\n1. Re-uploading Exact Copies of the Table\n\nWhile this use case sounds trivial, traditional file systems do not deduplicate files resulting in full re-upload and re-download of the data. In contrast, a system utilizing content-defined chunking can recognize that the file content is identical and avoid unnecessary data transfer.\n\npq.write_table(table, \"hf://datasets/kszucs/pq/orca-copy.parquet\" ) New Data Upload: | | 0.00 B / 0.00 B, 0.00 B/s Total Bytes: 96.1 M Total Transfer: 0.00\n\nWe can see that no new data has been uploaded, and the operation was instantaneous. Now let's see what happens if we upload the same file again but to a different repository:\n\npq.write_table(table, \"hf://datasets/kszucs/pq-copy/orca-copy-again.parquet\" ) New Data Upload: | | 0.00 B / 0.00 B, 0.00 B/s Total Bytes: 96.1 M Total Transfer: 0.00\n\nThe upload was instantaneous again since deduplication works across repositories as well. This is a key feature of the Xet storage layer, allowing efficient data sharing and collaboration. You can read more about the details and scaling challenges in the From Chunks to Blocks: Accelerating Uploads and Downloads on the Hub blog post.\n\n2. Adding and Removing Columns from the Table\n\nFirst write out the original and changed tables to local parquet files to see their sizes:\n\ntable_with_new_columns = table.add_column( table.schema.get_field_index( \"response\" ), \"response_short\" , pc.utf8_slice_codeunits(table[ \"response\" ], 0 , 10 ) ) table_with_removed_columns = table.drop([ \"response\" ]) pq.write_table(table, \"/tmp/original.parquet\" ) pq.write_table(table_with_new_columns, \"/tmp/with-new-columns.parquet\" ) pq.write_table(table_with_removed_columns, \"/tmp/with-removed-columns.parquet\" )\n\n!ls -lah /tmp/*.parquet\n\n-rw-r--r-- 1 kszucs wheel 92M Jul 22 14:47 /tmp/original.parquet -rw-r--r-- 1 kszucs wheel 92M Jul 22 14:47 /tmp/with-new-columns.parquet -rw-r--r-- 1 kszucs wheel 67M Jul 22 14:47 /tmp/with-removed-columns.parquet\n\nNow upload them to Hugging Face to see how much data is actually transferred:\n\npq.write_table(table_with_new_columns, \"hf://datasets/kszucs/pq/orca-added-columns.parquet\" ) New Data Upload: 100 %|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 575kB / 575kB, 288kB/s Total Bytes: 96.6 M Total Transfer: 575k\n\nWe can see that only the new columns and the new parquet metadata placed in the file's footer were uploaded, while the original data was not transferred again. This is a huge benefit of the Xet storage layer, as it allows us to efficiently add new columns without transferring the entire dataset again.\n\nSame applies to removing columns, as we can see below:\n\npq.write_table(table_with_removed_columns, \"hf://datasets/kszucs/pq/orca-removed-columns.parquet\" ) New Data Upload: 100 %|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 37.7 kB / 37.7 kB, 27.0 kB/s Total Bytes: 70.6 M Total Transfer: 37.7 k\n\nTo have a better understanding of what has been uploaded, we can visualize the differences between the two parquet files using the deduplication estimation tool:\n\nfrom de import visualize visualize(table, table_with_new_columns, title= \"With New Columns\" , prefix= \"orca\" )\n\nWith New Columns\n\nCompression Vanilla Parquet None Dedup Stats 157.4 MB / 313.8 MB = 50% Snappy Dedup Stats 96.7 MB / 192.7 MB = 50%\n\nAdding two new columns mean that we have unseen data pages which must be transferred (highlighted in red), but the rest of the data remains unchanged (highlighted in green), so it is not transferred again. Note the small red area in the footer metadata which almost always changes as we modify the parquet file. The dedup stats show <deduped size> / <total size> = <dedup ratio> where smaller ratios mean higher deduplication performance.\n\nAlso visualize the difference after removing a column:\n\nvisualize(table, table_with_removed_columns, title= \"With Removed Columns\" , prefix= \"orca\" )\n\nWith Removed Columns\n\nCompression Vanilla Parquet None Dedup Stats 156.6 MB / 269.4 MB = 58% Snappy Dedup Stats 96.1 MB / 166.7 MB = 57%\n\nSince we are removing entire columns we can only see changes in the footer metadata, all the other columns remain unchanged and already existing in the storage layer, so they are not transferred again.\n\n3. Changing Column Types in the Table\n\nAnother common use case is changing the column types in the table e.g. to reduce the storage size or to optimize the data for specific queries. Let's change the question_length column from int64 data type to int32 and see how much data is transferred:\n\ntable_without_text = table_with_new_columns.drop([ \"question\" , \"response\" ]) table_with_casted_column = table_without_text.set_column( table_without_text.schema.get_field_index( \"question_length\" ), \"question_length\" , table_without_text[ \"question_length\" ].cast( \"int32\" ) )\n\npq.write_table(table_with_casted_column, \"hf://datasets/kszucs/pq/orca-casted-column.parquet\" ) New Data Upload: 100 %|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 181kB / 181kB, 113kB/s Total Bytes: 1.80 M Total Transfer: 181k\n\nAgain, we can see that only the new column and the updated parquet metadata were uploaded. Now visualize the deduplication heatmap:\n\nvisualize(table_without_text, table_with_casted_column, title= \"With Casted Column\" , prefix= \"orca\" )\n\nWith Casted Column\n\nCompression Vanilla Parquet None Dedup Stats 2.8 MB / 5.3 MB = 52% Snappy Dedup Stats 1.9 MB / 3.6 MB = 53%\n\nThe first red region indicates the new column that was added, while the second red region indicates the updated metadata in the footer. The rest of the data remains unchanged and is not transferred again.\n\n4. Appending New Rows and Concatenating Tables\n\nWe are going to append new rows by concatenating another slice of the original dataset to the table.\n\ntable = orca[: 100_000 ] next_10k_rows = orca[ 100_000 : 110_000 ] table_with_appended_rows = pa.concat_tables([table, next_10k_rows]) assert len (table_with_appended_rows) == 110_000\n\nNow check that only the new rows are being uploaded since the original data is already known to the Xet storage layer:\n\npq.write_table(table_with_appended_rows, \"hf://datasets/kszucs/pq/orca-appended-rows.parquet\" ) New Data Upload: 100 %|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10.3 MB / 10.3 MB, 1.36 MB/s Total Bytes: 106M Total Transfer: 10.3 M\n\nvisualize(table, table_with_appended_rows, title= \"With Appended Rows\" , prefix= \"orca\" )\n\nWith Appended Rows\n\nCompression Vanilla Parquet None Dedup Stats 173.1 MB / 328.8 MB = 52% Snappy Dedup Stats 106.5 MB / 201.8 MB = 52%\n\nSince each column gets new data, we can see multiple red regions. This is due to the actual parquet file specification where whole columns are laid out after each other (within each row group).\n\n5. Inserting / Deleting Rows in the Table\n\nHere comes the difficult part as insertions and deletions are shifting the existing rows which lead to different columns chunks or data pages in the parquet nomenclature. Since each data page is compressed separately, even a single row insertion or deletion can lead to a completely different byte-level representation starting from the edited row(s) to the end of the parquet file.\n\nThis parquet specific problem cannot be solved by the Xet storage layer alone, the parquet file itself needs to be written in a way that minimizes the data page differences even if there are inserted or deleted rows.\n\nLet's try to use the existing mechanism and see how it performs.\n\ntable = orca[: 100_000 ] table_with_deleted_rows = pa.concat_tables([ orca[: 15_000 ], orca[ 18_000 : 60_000 ], orca[ 61_000 : 100_000 ] ]) table_with_inserted_rows = pa.concat_tables([ orca[: 10_000 ], orca[ 100_000 : 101_000 ], orca[ 10_000 : 50_000 ], orca[ 101_000 : 103_000 ], orca[ 50_000 : 100_000 ], ]) assert len (table) == 100_000 assert len (table_with_deleted_rows) == 96_000 assert len (table_with_inserted_rows) == 103_000\n\npq.write_table(table_with_inserted_rows, \"hf://datasets/kszucs/pq/orca-inserted-rows.parquet\" ) New Data Upload: 100 %|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 89.8 MB / 89.8 MB, 42.7 kB/s Total Bytes: 99.1 M Total Transfer: 89.8 M\n\npq.write_table(table_with_deleted_rows, \"hf://datasets/kszucs/pq/orca-deleted-rows.parquet\" ) New Data Upload: 100 %|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 78.2 MB / 78.2 MB, 46.5 kB/s Total Bytes: 92.2 M Total Transfer: 78.2 M\n\nAlso visualize both cases to see the differences:\n\nvisualize(table, table_with_deleted_rows, title= \"Deleted Rows\" , prefix= \"orca\" ) visualize(table, table_with_inserted_rows, title= \"Inserted Rows\" , prefix= \"orca\" )\n\nDeleted Rows\n\nCompression Vanilla Parquet None Dedup Stats 185.3 MB / 306.8 MB = 60% Snappy Dedup Stats 174.4 MB / 188.3 MB = 92%\n\nInserted Rows\n\nCompression Vanilla Parquet None Dedup Stats 190.1 MB / 318.0 MB = 59% Snappy Dedup Stats 186.2 MB / 195.2 MB = 95%\n\nWe can see that the deduplication performance has dropped significantly (higher ratio), and the deduplication heatmaps show that the compressed parquet files are quite different from each other. This is due to the fact that the inserted and deleted rows have shifted the existing rows, leading to different data pages starting from the edited row(s) to the end of the parquet file.\n\nWe can solve this problem by writing parquet files with a new pyarrow feature called content-defined chunking (CDC). This feature ensures that the columns are consistently getting chunked into data pages based on their content, similarly how the Xet storage layer deduplicates data but applied to the logical values of the columns before any serialization or compression happens.\n\nThe feature can be enabled by passing use_content_defined_chunking=True to the write_parquet function:\n\nimport pyarrow.parquet as pq pq.write_table(table, \"hf://user/repo/filename.parquet\" , use_content_defined_chunking= True )\n\nPandas also supports the new feature:\n\ndf.to_parquet( \"hf://user/repo/filename.parquet\" , use_content_defined_chunking= True )\n\nLet's visualize the deduplication difference before and after using the Parquet CDC feature:\n\nvisualize(table, table_with_deleted_rows, title= \"With Deleted Rows\" , prefix= \"orca\" , with_cdc= True ) visualize(table, table_with_inserted_rows, title= \"With Inserted Rows\" , prefix= \"orca\" , with_cdc= True )\n\nDeleted Rows\n\nCompression Vanilla Parquet CDC Parquet None Dedup Stats 185.3 MB / 306.8 MB = 60% 162.9 MB / 307.2 MB = 53% Snappy Dedup Stats 174.4 MB / 188.3 MB = 92% 104.3 MB / 188.8 MB = 55%\n\nInserted Rows\n\nCompression Vanilla Parquet CDC Parquet None Dedup Stats 190.1 MB / 318.0 MB = 59% 164.1 MB / 318.4 MB = 51% Snappy Dedup Stats 186.2 MB / 195.2 MB = 95% 102.8 MB / 195.7 MB = 52%\n\nLooks much better! Since the proof of the pudding is in the eating, let's actually upload the tables using the content-defined chunking parquet feature and see how much data is transferred.\n\nNote that we need to upload the original table first with content-defined chunking enabled:\n\npq.write_table(table, \"hf://datasets/kszucs/pq/orca-cdc.parquet\" , use_content_defined_chunking= True ) New Data Upload: 100 %|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 94.5 MB / 94.5 MB, 46.5 kB/s Total Bytes: 96.4 M Total Transfer: 94.5 M\n\npq.write_table( table_with_inserted_rows, \"hf://datasets/kszucs/pq/orca-inserted-rows-cdc.parquet\" , use_content_defined_chunking= True ) New Data Upload: 100 %|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6.00 MB / 6.00 MB, 1.00 MB/s Total Bytes: 99.3 M Total Transfer: 6.00 M\n\npq.write_table( table_with_deleted_rows, \"hf://datasets/kszucs/pq/orca-deleted-rows-cdc.parquet\" , use_content_defined_chunking= True ) New Data Upload: 100 %|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7.57 MB / 7.57 MB, 1.35 MB/s Total Bytes: 92.4 M Total Transfer: 7.57 M\n\nThe uploaded data is significantly smaller than before, showing much better deduplication performance as highlighted in the heatmaps above.\n\nImportant to note that the same performance benefits apply to downloads using the huggingface_hub.hf_hub_download() and datasets.load_dataset() functions.\n\n6. Using Different Row-group Sizes\n\nThere are cases depending on the reader/writer constraints where larger or smaller row-group sizes might be beneficial. The parquet writer implementations use fixed-sized row-groups by default, in the case of pyarrow the default is 1Mi rows. Dataset writers may change to reduce the row-group size in order to improve random access performance or to reduce the memory footprint of the reader application.\n\nChanging the row-group size will shift rows between row-groups, shifting values between data pages, so we have a similar problem as with inserting or deleting rows. Let's compare the deduplication performance between different row-group sizes using the parquet CDC feature:\n\nfrom de import visualize table = orca[ 2_000_000 : 3_000_000 ] visualize(table, (table, { \"row_group_size\" : 128 * 1024 }), title= \"Small Row Groups\" , with_cdc= True , prefix= \"orca\" ) visualize(table, (table, { \"row_group_size\" : 256 * 1024 }), title= \"Medium Row Groups\" , with_cdc= True , prefix= \"orca\" )\n\nSmall Row Groups\n\nCompression Vanilla Parquet CDC Parquet None Dedup Stats 1.6 GB / 3.1 GB = 52% 1.6 GB / 3.1 GB = 50% Snappy Dedup Stats 1.1 GB / 1.9 GB = 59% 995.0 MB / 1.9 GB = 51%\n\nMedium Row Groups\n\nCompression Vanilla Parquet CDC Parquet None Dedup Stats 1.6 GB / 3.1 GB = 51% 1.6 GB / 3.1 GB = 50% Snappy Dedup Stats 1.1 GB / 1.9 GB = 57% 976.5 MB / 1.9 GB = 50%\n\n7. Using Varying File-Level Splits\n\nDatasets are often split into multiple files to improve parallelism and random access. Parquet CDC combined with the Xet storage layer can efficiently deduplicate data across multiple files even if the data is split at different boundaries.\n\nLet's write out the dataset with three different file-level splitting then compare the deduplication performance:\n\nfrom pathlib import Path from de import estimate def write_dataset ( table, base_dir, num_shards, **kwargs ): \"\"\"Simple utility to write a pyarrow table to multiple Parquet files.\"\"\" base_dir = Path(base_dir) base_dir.mkdir(parents= True , exist_ok= True ) rows_per_file = len (table) / num_shards for i in range (num_shards): start = i * rows_per_file end = min ((i + 1 ) * rows_per_file, len (table)) shard = table. slice (start, end - start) path = base_dir / f\"part- {i} .parquet\" pq.write_table(shard, path, **kwargs) write_dataset(orca, \"orca5-cdc\" , num_shards= 5 , use_content_defined_chunking= True ) write_dataset(orca, \"orca10-cdc\" , num_shards= 10 , use_content_defined_chunking= True ) write_dataset(orca, \"orca20-cdc\" , num_shards= 20 , use_content_defined_chunking= True ) estimate( \"orca5-cdc/*.parquet\" , \"orca10-cdc/*.parquet\" , \"orca20-cdc/*.parquet\" )\n\nTotal size: 9.3 GB Chunk size: 3.2 GB\n\nEven though we uploaded the dataset with three different sharding configurations, the overall upload size would be barely larger than the original dataset size.\n\nUsing Parquet CDC feature with Pandas\n\nSo far we've used PyArrow, let\u2019s explore using the same CDC feature with Pandas by downloading, filtering then uploading the dataset with the content-defined chunking feature enabled:\n\nimport pandas as pd src = \"hf://datasets/teknium/OpenHermes-2.5/openhermes2_5.json\" df = pd.read_json(src)\n\ndst = \"hf://datasets/kszucs/pq/hermes-2.5-cdc.parquet\" df.to_parquet(dst, use_content_defined_chunking= True ) New Data Upload: 100 %|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 799MB / 799MB, 197kB/s Total Bytes: 799M Total Transfer: 799M\n\nshort_df = df[[ len (c) < 10 for c in df.conversations]] short_dst = \"hf://datasets/kszucs/pq/hermes-2.5-cdc-short.parquet\" short_df.to_parquet(short_dst, use_content_defined_chunking= True ) New Data Upload: 100 %|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 21.9 MB / 21.9 MB, 45.4 kB/s Total Bytes: 801M Total Transfer: 21.9 M\n\nimport pyarrow as pa from de import visualize visualize( pa.Table.from_pandas(df), pa.Table.from_pandas(short_df), title= \"Hermes 2.5 Short Conversations\" , with_cdc= True , prefix= \"hermes\" )\n\nHermes 2.5 Short Conversations\n\nCompression Vanilla Parquet CDC Parquet None Dedup Stats 1.9 GB / 3.2 GB = 58% 1.6 GB / 3.2 GB = 51% Snappy Dedup Stats 1.5 GB / 1.6 GB = 94% 821.1 MB / 1.6 GB = 51%\n\nSince Parquet CDC is applied at the parquet data page level (column chunk level), the deduplication performance depends on the filter's selectivity, or rather the distribution of the changes across the dataset. If most of the data pages are affected, then the deduplication ratio will drop significantly.\n\nReferences\n\nMore details about the feature can be found at:\n\nConclusion\n\nWe explored the performance benefits of the new Parquet content-defined chunking feature used alongside Hugging Face's Xet storage layer. We demonstrated how it can efficiently deduplicate data in various scenarios making parquet operations faster and more storage-efficient. Comparing to traditional cloud storage solutions, the Xet storage layer with Parquet CDC can significantly reduce data transfer times and costs.\n\nMigrate your Hugging Face repositories from Git LFS to Xet to benefit from this here: https://huggingface.co/join/xet",
    "link": "https://huggingface.co/blog/parquet-cdc",
    "Summary": "Parquet Content-Defined ChunkingPublished July 25, 2025 Update on GitHubReduce Parquet file upload and download times on Hugging Face Hub by leveraging the new Xet storage layer and Apache Arrow\u2019s Parquet Content-Defined Chunking (CDC) feature enabling more efficient and scalable data workflows.\nTL;DR: Parquet Content-Defined Chunking (CDC) is now available in PyArrow and Pandas, enabling efficient deduplication of Parquet files on content-addressable storage systems like Hugging Face's Xet storage layer.\nTo address this, the Parquet files should be written in a way that minimizes the byte-level differences between similar data, which is where content-defined chunking (CDC) comes into play.\nWe can solve this problem by writing parquet files with a new pyarrow feature called content-defined chunking (CDC).\nReferencesMore details about the feature can be found at:ConclusionWe explored the performance benefits of the new Parquet content-defined chunking feature used alongside Hugging Face's Xet storage layer.",
    "Keywords": [
      "data",
      "parquet",
      "orca",
      "dedup",
      "mb",
      "table",
      "gb",
      "storage",
      "total",
      "contentdefined",
      "chunking",
      "stats"
    ]
  },
  {
    "Title": "Say hello to `hf`: a faster, friendlier Hugging Face CLI \u2728",
    "Authors": [],
    "Publish Date": null,
    "Text": "Say hello to `hf`: a faster, friendlier Hugging Face CLI \u2728\n\nPublished July 25, 2025 Update on GitHub\n\nhuggingface-cli\n\nhf\n\nWe are glad to announce a long-awaited quality-of-life improvement: the Hugging Face CLI has been officially renamed fromto\n\nSo... why this change?\n\nTyping huggingface-cli constantly gets old fast. More importantly, the CLI\u2019s command structure became messy as new features were added over time (upload, download, cache management, repo management, etc.). Renaming the CLI is a chance to reorganize commands into a clearer, more consistent format.\n\nWe decided not to reinvent the wheel and instead follow a well-known CLI pattern: hf <resource> <action> . This predictable grammar makes the Hugging Face CLI more ergonomic and discoverable, while also setting the stage for upcoming features.\n\nGetting started\n\nTo start playing with the new CLI, you\u2019ll need to install the latest huggingface_hub version:\n\npip install -U huggingface_hub\n\nand reload your terminal session. To test the install completed successfully, run hf version :\n\n\u279c hf version huggingface_hub version: 0.34.0\n\nNext, let\u2019s explore the new syntax with hf --help :\n\n\u279c hf --help usage: hf <command> [<args>] positional arguments: {auth,cache,download,jobs,repo,repo-files,upload,upload-large-folder,env,version,lfs-enable-largefiles,lfs-multipart-upload} hf command helpers auth Manage authentication (login, logout, etc.). cache Manage local cache directory. download Download files from the Hub jobs Run and manage Jobs on the Hub. repo Manage repos on the Hub. repo-files Manage files in a repo on the Hub. upload Upload a file or a folder to the Hub. Recommended for single-commit uploads. upload-large-folder Upload a large folder to the Hub. Recommended for resumable uploads. env Print information about the environment. version Print information about the hf version. options: -h, --help show this help message and exit\n\nAs we can see, commands are grouped by \"resource\" ( hf auth , hf cache , hf repo , etc.). We also surface hf upload and hf download at the root level since they\u2019re expected to be the most-used commands.\n\nTo dive deeper into any command group, simply append --help :\n\n\u279c hf auth --help usage: hf <command> [<args>] auth [-h] {login,logout,whoami,switch,list} ... positional arguments: {login,logout,whoami,switch,list} Authentication subcommands login Log in using a token from huggingface.co/settings/tokens logout Log out whoami Find out which huggingface.co account you are logged in as. switch Switch between access tokens list List all stored access tokens options: -h, --help show this help message and exit\n\n\ud83d\udd00 Migration\n\nIf you are used to huggingface-cli , most commands will look familiar. The biggest change affects authentication:\n\nhuggingface-cli login hf auth login\n\nhuggingface-cli whoami hf auth whoami\n\nhuggingface-cli logout hf auth logout\n\nAll auth commands have been grouped together with the existing hf auth switch (to switch between different local profiles) and hf auth list (to list local profiles).\n\nThe legacy huggingface-cli remains active and fully-functional. We\u2019re keeping it around to ease the transition. If you use any command from the legacy CLI, you\u2019ll see a warning that points you to the new CLI equivalent:\n\n\u279c huggingface-cli whoami \u26a0\ufe0f Warning: 'huggingface-cli whoami' is deprecated. Use 'hf auth whoami' instead. Wauplin orgs: huggingface,competitions,hf-internal-testing,templates,HF-test-lab,Gradio-Themes,autoevaluate,HuggingFaceM4,HuggingFaceH4,open-source-metrics,sd-concepts-library,hf-doc-build,hf-accelerate,HFSmolCluster,open-llm-leaderboard,pbdeeplinks,discord-community,llhf,sllhf,mt-metrics,DDUF,hf-inference,changelog,tiny-agents\n\nOne more thing... \ud83d\udca5 hf jobs\n\nWe couldn\u2019t resist shipping our first dedicated command: hf jobs.\n\nHugging Face Jobs is a new service that lets you run any script or Docker image on Hugging Face Infrastructure using the hardware flavor of your choice. Billing is \"pay-as-you-go\", meaning you pay only for the seconds you use. Here\u2019s how to launch your first command:\n\nhf jobs run --flavor=a10g-small ubuntu nvidia-smi\n\nThe CLI is heavily inspired by Docker\u2019s familiar commands:\n\n\u279c hf jobs -- help usage: hf < command > [<args>] jobs [-h] {inspect,logs,ps,run,cancel,uv} ... positional arguments: {inspect,logs,ps,run,cancel,uv} huggingface.co jobs related commands inspect Display detailed information on one or more Jobs logs Fetch the logs of a Job ps List Jobs run Run a Job cancel Cancel a Job uv Run UV scripts (Python with inline dependencies) on HF infrastructure options: -h, -- help show this help message and exit\n\nLearn more about Jobs by reading the guide.",
    "link": "https://huggingface.co/blog/hf-cli",
    "Summary": "Say hello to `hf`: a faster, friendlier Hugging Face CLI \u2728Published July 25, 2025 Update on GitHubhuggingface-clihfWe are glad to announce a long-awaited quality-of-life improvement: the Hugging Face CLI has been officially renamed fromtoSo... why this change?\nTo test the install completed successfully, run hf version :\u279c hf version huggingface_hub version: 0.34.0Next, let\u2019s explore the new syntax with hf --help :\u279c hf --help usage: hf <command> [<args>] positional arguments: {auth,cache,download,jobs,repo,repo-files,upload,upload-large-folder,env,version,lfs-enable-largefiles,lfs-multipart-upload} hf command helpers auth Manage authentication (login, logout, etc.).\noptions: -h, --help show this help message and exitAs we can see, commands are grouped by \"resource\" ( hf auth , hf cache , hf repo , etc.).\nThe biggest change affects authentication:huggingface-cli login hf auth loginhuggingface-cli whoami hf auth whoamihuggingface-cli logout hf auth logoutAll auth commands have been grouped together with the existing hf auth switch (to switch between different local profiles) and hf auth list (to list local profiles).\nWauplin orgs: huggingface,competitions,hf-internal-testing,templates,HF-test-lab,Gradio-Themes,autoevaluate,HuggingFaceM4,HuggingFaceH4,open-source-metrics,sd-concepts-library,hf-doc-build,hf-accelerate,HFSmolCluster,open-llm-leaderboard,pbdeeplinks,discord-community,llhf,sllhf,mt-metrics,DDUF,hf-inference,changelog,tiny-agentsOne more thing... \ud83d\udca5 hf jobsWe couldn\u2019t resist shipping our first dedicated command: hf jobs.",
    "Keywords": [
      "hello",
      "command",
      "help",
      "run",
      "hub",
      "whoami",
      "friendlier",
      "hugging",
      "face",
      "faster",
      "auth",
      "commands",
      "hf",
      "jobs",
      "cli",
      "say"
    ]
  },
  {
    "Title": "TimeScope: How Long Can Your Video Large Multimodal Model Go?",
    "Authors": [],
    "Publish Date": null,
    "Text": "TimeScope: How Long Can Your Video Large Multimodal Model Go?\n\nPublished July 23, 2025 Update on GitHub\n\nTimeScope is an open-source benchmark designed to measure how well vision-language models understand long videos. By adding short \u201cneedle\u201d clips into videos ranging from 1 minute to 8 hours, it evaluates three skills:\n\nlocalized retrieval,\n\ninformation synthesis,\n\nfine-grained temporal perception. Timescope reveals that many state-of-the-art models still struggle with true temporal comprehension.\n\nTable of Contents\n\nRecent advances in multimodal AI have produced models claiming to understand hour-long videos. This trend mirrors progress in long-context language models, which excel at reasoning over lengthy text. Following this, vision-language systems now advertise context windows that can handle thousands of frames. But these claims require a closer look: do these models truly demonstrate understanding of the sequence of events? Are they limited to surface-level retrieval \\ recognition? It's crucial to ask if their capabilities are being overstated.\n\nText benchmarks such as HELM and RULER have exposed the fragility of long-context claims, showing that models often struggle when tasks demand more than simple retrieval, like reasoning or aggregation at long context lengths. In the video domain, however, we're still playing catch-up. The most common test, Video Needle in a Haystack (VideoNIAH), injects static images as \"needles\" into videos, effectively measuring visual search rather than true temporal dynamics. As a result, even top-tier models advertising massive frame capacities are rarely trained beyond ~256 frames and see sharp drops on benchmarks like Video-MME when pushed further.\n\nThis measurement gap leaves us wondering: What does it really mean for a model to \"understand\" long videos? To address this, we're excited to introduce TimeScope, a new open-source benchmark hosted on Hugging Face. TimeScope probes the limits of long-video capabilities by inserting several short (~5-10 second) video clips\u2014our \"needles\"\u2014into base videos ranging from 1 minute to 8 hours. With three distinct task types, it evaluates not just retrieval but synthesis, localization, and fine-grained motion analysis, providing a more holistic view of temporal comprehension.\n\nWhy TimeScope? Motivating a Better Benchmark for Video\n\nThe promise of long-video AI is transformative \u2014 enabling agents to summarize hours of footage, detect subtle anomalies, and answer complex questions about extended narratives. Integrated into robotics, these models could analyze prolonged operations, adapt in real time, and push autonomous decision-making. Just as powerful is the vision of a personal assistant that understands daily life and offers continuous, actionable feedback.\n\nIn practice, this leads to overstated capabilities. Models might claim to process 10,000+ frames, but training data often caps at 256 frames per clip, leading to degraded performance on longer inputs. We've seen this in evaluations where increasing frame sampling rates tanks accuracy on tasks requiring temporal insight.\n\nTimeScope flips the script by emphasizing three pillars of long-video understanding:\n\nLocalized Retrieval: Can the model spot and answer questions about a specific short segment within a vast video? Information Synthesis: Can it gather and order details from multiple points across the timeline? Fine-Grained Temporal Perception: Can it analyze motion and events in needles that demand dense, multi-frame sampling?\n\nBenchmark Design\n\nTimeScope\u2019s key idea is using short video clips as \u201cneedles,\u201d and instead of just spotting the needle, it pushes models to deeply understand the whole video. We start with a long base video (e.g., a documentary, lecture, or ambient footage) and insert one or more hand-curated short video needles (5-10 seconds each) at random positions. These needles contain the key information needed to solve the task, forcing models to process the entire input without shortcuts like sparse sampling.\n\nFigure 1: Overview of TimeScope's needle insertion process. A long base video (1 min to 8 hours) serves as the haystack, into which we splice short video needles (~5-10 seconds). Tasks require detecting, synthesizing, or analyzing content from these needles, embedded at varying depths.\n\nWe evaluate across three needle types, each targeting a different aspect of long-video comprehension:\n\n1. Localized Retrieval\n\nThis tests basic retrieval and understanding of a localized event. Questions are put so that sampling a relevant frame from the needle should suffice\u2014like asking about a shorter part in a longer video.\n\nExample:\n\nWhat mode of transportation is shown in the video?\n\n2. Information Synthesis\n\nHere, we embed multiple text-based needles (e.g., 2-4 short clips displaying \"secret words\" via on-screen text) at different points in the video. The model must identify all words and report them in chronological order, simulating tasks like extracting timestamps or key facts from dispersed scenes. This requires scanning the full timeline and understanding relative positioning.\n\n3. Fine-Grained Temporal Perception\n\nFor questions focusing on motion or sequences within a short clip, single-frame sampling won't cut it\u2014the model needs to perceive dynamics across frames. This probes whether long-context handling preserves temporal fidelity.\n\nExample:\n\nHow many times did the man swing his axe? (a) one (b) two (c) three (d) four (e) five (f) six\n\nWith different video lengths and varying needle placements, TimeScope measures how much video a model can really handle\u2014and shows that performance drops as the video gets longer.\n\nEvaluations & Leaderboard\n\nTo kick things off, we ran TimeScope on a suite of leading vision-language models, from open-source favorites to the juggernauts like Gemini 2.5-Pro. The results underscore the benchmark\u2019s value: even models that claim to handle long videos well still struggle with real long-video tasks. These findings reveal clear patterns\u2014performance cliffs around certain durations, strengths in static retrieval versus weaknesses in motion analysis\u2014and pave the way for targeted improvements in model training. For detailed results and visualizations, check out our Hugging Face Space embedded above.\n\nWhat did we learn?\n\nModel size isn\u2019t everything. Qwen 2.5-VL 3B and 7B, as well as InternVL 2.5 models at 2B, 4B, and 8B parameters, exhibit nearly indistinguishable long-video curves to their smaller counterparts. All of them plateau at roughly the same context length, showing that simply scaling parameters does not automatically grant a longer temporal horizon.\n\nGemini 2.5-Pro is in a league of its own. It is the only model that maintains strong accuracy on videos longer than one hour.\n\nTrade-offs across tasks matter. Qwen 2.5-VL shines in the Information-Synthesis (OCR) task\u2014identifying and ordering dispersed text snippets\u2014yet it falls behind on Fine-Grained Temporal Perception, where precise motion counting is required.\n\nConclusion \u2013 Let\u2019s Raise the Bar for Long-Video AI\n\nTimeScope demonstrates that \u201chour-long video understanding\u201d is still more slogan than reality. By revealing where even state-of-the-art models stumble on temporal reasoning, information synthesis, and motion perception, the benchmark invites us to rethink how we train and evaluate multimodal systems.\n\nRun the Demo \u2013 Explore the public Space: https://huggingface.co/spaces/Apollo-LMMs/TimeScope Benchmark Locally \u2013 Evaluate any model with two quick commands: pip install git+https://github.com/EvolvingLMMs-Lab/lmms-eval.git python -m lmms_eval --model-path <your-model> --benchmark timescope Join the Leaderboard \u2013 Submit your scores and see how your model compares.\n\nWe hope this benchmark helps the community make steady, measurable progress toward models that better understand video over time.\n\nWe are open-sourcing all components of TimeScope:",
    "link": "https://huggingface.co/blog/timescope-video-lmm-benchmark",
    "Summary": "TimeScope: How Long Can Your Video Large Multimodal Model Go?\nBenchmark DesignTimeScope\u2019s key idea is using short video clips as \u201cneedles,\u201d and instead of just spotting the needle, it pushes models to deeply understand the whole video.\nWe start with a long base video (e.g., a documentary, lecture, or ambient footage) and insert one or more hand-curated short video needles (5-10 seconds each) at random positions.\nA long base video (1 min to 8 hours) serves as the haystack, into which we splice short video needles (~5-10 seconds).\n(a) one (b) two (c) three (d) four (e) five (f) sixWith different video lengths and varying needle placements, TimeScope measures how much video a model can really handle\u2014and shows that performance drops as the video gets longer.",
    "Keywords": [
      "needles",
      "models",
      "longvideo",
      "multimodal",
      "large",
      "videos",
      "video",
      "needle",
      "short",
      "temporal",
      "timescope",
      "model",
      "long"
    ]
  },
  {
    "Title": "Fast LoRA inference for Flux with Diffusers and PEFT",
    "Authors": [],
    "Publish Date": null,
    "Text": "Fast LoRA inference for Flux with Diffusers and PEFT\n\nPublished July 23, 2025 Update on GitHub\n\nLoRA adapters provide a great deal of customization for models of all shapes and sizes. When it comes to image generation, they can empower the models with different styles, different characters, and much more . Sometimes, they can also be leveraged to reduce inference latency . Hence, their importance is paramount, particularly when it comes to customizing and fine-tuning models.\n\nIn this post, we take the Flux.1-Dev model for text-to-image generation because of its widespread popularity and adoption, and how to optimize its inference speed when using LoRAs (~2.3x). It has over 30k adapters trained with it (as reported on the Hugging Face Hub platform). Therefore, its importance to the community is significant.\n\nNote that even though we demonstrate speedups with Flux, our belief is that our recipe is generic enough to be applied to other models as well.\n\nIf you cannot wait to get started with the code, please check out the accompanying code repository.\n\nTable of contents\n\nHurdles in optimizing LoRA inference\n\nWhen serving LoRAs, it is common to hotswap (swap in and swap out different LoRAs) them. A LoRA changes the base model architecture. Additionally, LoRAs can be different from one another \u2013 each one of them could have varying ranks and different layers they target for adaptation. To account for these dynamic properties of LoRAs, we must take necessary steps to ensure the optimizations we apply are robust.\n\nFor example, we can apply torch.compile on a model loaded with a particular LoRA to obtain speedups on inference latency. However, the moment we swap out the LoRA with a different one (with a potentially different configuration), we will run into recompilation issues, causing slowdowns in inference.\n\nOne can also fuse the LoRA parameters into the base model parameters, run compilation, and unfuse the LoRA parameters when loading new ones. However, this approach will again encounter the problem of recompilation whenever inference is run, due to potential architecture-level changes.\n\nOur optimization recipe takes into account the above-mentioned situations to be as realistic as possible. Below are the key components of our optimization recipe:\n\nFlash Attention 3 (FA3)\n\ntorch.compile\n\nFP8 quantization from TorchAO\n\nHotswapping-ready\n\nNote that amongst the above-mentioned, FP8 quantization is lossy but often provides the most formidable speed-memory trade-off. Even though we tested the recipe primarily using NVIDIA GPUs, it should work on AMD GPUs, too.\n\nOptimization recipe\n\nIn our previous blog posts (post 1 and post 2), we have already discussed the benefits of using the first three components of our optimization recipe. Applying them one by one is just a few lines of code:\n\nfrom diffusers import DiffusionPipeline, TorchAoConfig from diffusers.quantizers import PipelineQuantizationConfig from utils.fa3_processor import FlashFluxAttnProcessor3_0 import torch pipe = DiffusionPipeline.from_pretrained( \"black-forest-labs/FLUX.1-dev\" , torch_dtype=torch.bfloat16, quantization_config=PipelineQuantizationConfig( quant_mapping={ \"transformer\" : TorchAoConfig( \"float8dq_e4m3_row\" )} ) ).to( \"cuda\" ) pipe.transformer.set_attn_processor(FlashFluxAttnProcessor3_0()) pipe.transformer. compile (fullgraph= True , mode= \"max-autotune\" ) pipe_kwargs = { \"prompt\" : \"A cat holding a sign that says hello world\" , \"height\" : 1024 , \"width\" : 1024 , \"guidance_scale\" : 3.5 , \"num_inference_steps\" : 28 , \"max_sequence_length\" : 512 , } image = pipe(**pipe_kwargs).images[ 0 ]\n\nThe FA3 processor comes from here.\n\nThe problems start surfacing when we try to swap in and swap out LoRAs into a compiled diffusion transformer ( pipe.transformer ) without triggering recompilation.\n\nNormally, loading and unloading LoRAs will require recompilation, which defeats any speed advantage gained from compilation. Thankfully, there is a way to avoid the need for recompilation. By passing hotswap=True , diffusers will leave the model architecture unchanged and only exchange the weights of the LoRA adapter itself, which does not necessitate recompilation.\n\npipe.enable_lora_hotswap(target_rank=max_rank) pipe.load_lora_weights(<lora-adapter-name1>) pipe.transformer. compile (mode= \"max-autotune\" , fullgraph= True ) image = pipe(**pipe_kwargs).images[ 0 ] pipe.load_lora_weights(<lora-adapter-name2>, hotswap= True ) image = pipe(**pipe_kwargs).images[ 0 ]\n\n(As a reminder, the first call to pipe will be slow as torch.compile is a just-in-time compiler. However, the subsequent calls should be significantly faster.)\n\nThis generally allows for swapping LoRAs without recompilation, but there are limitations:\n\nWe need to provide the maximum rank among all LoRA adapters ahead of time. Thus, if we have one adapter with rank 16 and another with 32, we need to pass max_rank=32 .\n\n. LoRA adapters that are hotswapped in can only target the same layers, or a subset of layers, that the first LoRA targets.\n\nTargeting the text encoder is not supported yet.\n\nFor more information on hotswapping in Diffusers and its limitations, visit the hotswapping section of the documentation.\n\nThe benefits of this workflow become evident when we look at the inference latency without using compilation with hotswapping.\n\nOption Time (s) \u2b07\ufe0f Speedup (vs baseline) \u2b06\ufe0f Notes baseline 7.8910 \u2013 Baseline optimized 3.5464 2.23\u00d7 Hotswapping + compilation without recompilation hiccups (FP8 on by default) no_fp8 4.3520 1.81\u00d7 Same as optimized, but with FP8 quantization disabled no_fa3 4.3020 1.84\u00d7 Disable FA3 (flash\u2011attention v3) baseline + compile 5.0920 1.55\u00d7 Compilation on, but suffers from intermittent recompilation stalls no_fa3_fp8 5.0850 1.55\u00d7 Disable FA3 and FP8 no_compile_fp8 7.5190 1.05\u00d7 Disable FP8 quantization and compilation no_compile 10.4340 0.76\u00d7 Disable compilation: the slowest setting\n\nKey takeaways:\n\nThe \u201cregular + compile\u201d option provides a decent speedup over the regular option, but it incurs recompilation issues, which increase the overall execution time. In our benchmarks, we don\u2019t present the compilation time.\n\nWhen recompilation problems are eliminated through hotswapping (also known as the \u201coptimized\u201d option), we achieve the highest speedup.\n\nIn the \u201coptimized\u201d option, FP8 quantization is enabled, which can lead to quality loss. Even without using FP8, we get a decent amount of speedup (\u201cno_fp8\u201d option).\n\nFor demonstration purposes, we use a pool of two LoRAs for hotswapping with compilation. For the full code, please refer to the accompanying code repository.\n\nThe optimization recipe we have discussed so far assumes access to a powerful GPU like H100. However, what can we do when we\u2019re limited to using consumer GPUs such as RTX 4090? Let\u2019s find out.\n\nOptimized LoRA inference on a consumer GPU\n\nFlux.1-Dev (without any LoRA), using the Bfloat16 data-type, takes ~33GB of memory to run. Depending on the size of the LoRA module, and without using any optimization, this memory footprint can increase even further. Many consumer GPUs like the RTX 4090 only have 24GB. Throughout the rest of this section, we will consider an RTX 4090 machine as our testbed.\n\nFirst, to enable end-to-end execution of Flux.1-Dev, we can apply CPU offloading wherein components that are not needed to execute the current computation are offloaded to the CPU to free more accelerator memory. Doing so allows us to run the entire pipeline in ~22GB in 35.403 seconds on an RTX 4090. Enabling compilation can reduce the latency down to 31.205 seconds (1.12x speedup). In terms of code, it\u2019s just a few lines:\n\npipe = DiffusionPipeline.from_pretrained( \"black-forest-labs/FLUX.1-dev\" , torch_dtype=torch.bfloat16, ) pipe.enable_model_cpu_offload() pipe.transformer.compile_repeated_blocks(fullgraph= True ) image = pipe(**pipe_kwargs).images[ 0 ]\n\nNotice that we didn\u2019t apply the FP8 quantization here because it\u2019s not supported with CPU offloading and compilation (supporting issue thread). Therefore, just applying FP8 quantization to the Flux Transformer isn\u2019t enough to mitigate the memory exhaustion problem, either. In this instance, we decided to remove it.\n\nTherefore, to take advantage of the FP8 quantization scheme, we need to find a way to do it without CPU offloading. For Flux.1-Dev, if we additionally apply quantization to the T5 text encoder, we should be able to load and run the complete pipeline in 24GB. Below is a comparison of the results with and without the T5 text encoder being quantized (NF4 quantization from bitsandbytes ).\n\nAs we can notice in the figure above, quantizing the T5 text encoder doesn\u2019t incur too much of a quality loss. Combining the quantized T5 text encoder and FP8-quantized Flux Transformer with torch.compile gives us somewhat reasonable results \u2013 9.668 seconds from 32.27 seconds (a massive ~3.3x speedup) without a noticeable quality drop.\n\nIt is possible to generate images with 24 GB of VRAM even without quantizing the T5 text encoder, but that would have made our generation pipeline slightly more complicated.\n\nWe now have a way to run the entire Flux.1-Dev pipeline with FP8 quantization on an RTX 4090. We can apply the previously established optimization recipe for optimizing LoRA inference on the same hardware. Since FA3 isn\u2019t supported on RTX 4090, we will stick to the following optimization recipe with T5 quantization newly added to the mix:\n\nFP8 quantization\n\ntorch.compile\n\nHotswapping-ready\n\nT5 quantization (with NF4)\n\nIn the table below, we show the inference latency numbers with different combinations of the above components applied.\n\nOption Key args flags Time (s) \u2b07\ufe0f Speedup (vs baseline) \u2b06\ufe0f baseline disable_fp8=False disable_compile=True quantize_t5=True offload=False 23.6060 \u2013 optimized disable_fp8=False disable_compile=False quantize_t5=True offload=False 11.5715 2.04\u00d7\n\nQuick notes:\n\nCompilation provides a massive 2x speedup over the baseline.\n\nThe other options yielded OOM errors even with offloading enabled.\n\nTechnical details of hotswapping\n\nTo enable hotswapping without triggering recompilation, two hurdles have to be overcome. First, the LoRA scaling factor has to be converted into torch tensors from floats, which is achieved fairly easily. Second, the shape of the LoRA weights needs to padded to the largest required shape. That way, the data in the weights can be replaced without the need to reassign the whole attribute. This is why the max_rank argument discussed above is crucial. As we pad the values with zeros, the results remain unchanged, although the computation is slowed down a bit depending on how large the padding is.\n\nSince no new LoRA attributes are added, this also requires that each LoRA after the first one can only target the same layers, or a subset of layers, that the first one targets. Thus, choose the order of loading wisely. If LoRAs target disjoint layers, there is the possibility to create a dummy LoRA that targets the union of all target layers.\n\nTo see the nitty-gritty of this implementation, visit the hotswap.py file in PEFT.\n\nConclusion\n\nThis post outlined an optimization recipe for fast LoRA inference with Flux, demonstrating significant speedups. Our approach combines Flash Attention 3, torch.compile , and FP8 quantization while ensuring hotswapping capabilities without recompilation issues. On high-end GPUs like the H100, this optimized setup provides a 2.23x speedup over the baseline.\n\nFor consumer GPUs, specifically the RTX 4090, we tackled memory limitations by introducing T5 text encoder quantization (NF4) and leveraging regional compilation. This comprehensive recipe achieved a substantial 2.04x speedup, making LoRA inference on Flux viable and performant even with limited VRAM. The key insight is that by carefully managing compilation and quantization, the benefits of LoRA can be fully realized across different hardware configurations.\n\nHopefully, the recipes from this post will inspire you to optimize your LoRA-based use cases, benefitting from speedy inference.\n\nResources\n\nBelow is a list of the important resources that we cited throughout this post:",
    "link": "https://huggingface.co/blog/lora-fast",
    "Summary": "Fast LoRA inference for Flux with Diffusers and PEFTPublished July 23, 2025 Update on GitHubLoRA adapters provide a great deal of customization for models of all shapes and sizes.\nOptimized LoRA inference on a consumer GPUFlux.1-Dev (without any LoRA), using the Bfloat16 data-type, takes ~33GB of memory to run.\nTherefore, just applying FP8 quantization to the Flux Transformer isn\u2019t enough to mitigate the memory exhaustion problem, either.\nConclusionThis post outlined an optimization recipe for fast LoRA inference with Flux, demonstrating significant speedups.\nThis comprehensive recipe achieved a substantial 2.04x speedup, making LoRA inference on Flux viable and performant even with limited VRAM.",
    "Keywords": [
      "inference",
      "different",
      "loras",
      "flux",
      "compilation",
      "fast",
      "peft",
      "diffusers",
      "quantization",
      "speedup",
      "recompilation",
      "recipe",
      "lora",
      "fp8"
    ]
  },
  {
    "Title": "Consilium: When Multiple LLMs Collaborate",
    "Authors": [],
    "Publish Date": null,
    "Text": "Consilium: When Multiple LLMs Collaborate\n\nPublished July 17, 2025 Update on GitHub\n\nPicture this: four AI experts sitting around a poker table, debating your toughest decisions in real-time. That's exactly what Consilium, the multi-LLM platform I built during the Gradio Agents & MCP Hackathon , does. It lets AI models discuss complex questions and reach consensus through structured debate.\n\nThe platform works both as a visual Gradio interface and as an MCP (Model Context Protocol) server that integrates directly with applications like Cline (Claude Desktop had issues as the timeout could not be adjusted). The core idea was always about LLMs reaching consensus through discussion; that's where the name Consilium came from. Later, other decision modes like majority voting and ranked choice were added to make the collaboration more sophisticated.\n\nFrom Concept to Architecture\n\nThis wasn't my original hackathon idea. I initially wanted to build a simple MCP server to talk to my projects in RevenueCat. But I reconsidered when I realized a multi-LLM platform where these models discuss questions and return well-reasoned answers would be far more compelling.\n\nThe timing turned out to be perfect. Shortly after the hackathon, Microsoft published their AI Diagnostic Orchestrator (MAI-DxO), which is essentially an AI doctor panel with different roles like \"Dr. Challenger Agent\" that iteratively diagnose patients. In their setup with OpenAI o3, they correctly solved 85.5% of medical diagnosis benchmark cases, while practicing physicians achieved only 20% accuracy. This validates exactly what Consilium demonstrates: multiple AI perspectives collaborating can dramatically outperform individual analysis.\n\nAfter settling on the concept, I needed something that worked as both an MCP server and an engaging Hugging Face space demo. Initially I considered using the standard Gradio Chat component, but I wanted my submission to stand out. The idea was to seat LLMs around a table in a boardroom with speech bubbles, which should capture the collaborative discussion while also making it visually engaging. As I did not manage to style a standard table nicely so it was actually recognized as a table, I went for a poker-style roundtable. This approach also let me submit to two hackathon tracks by building a custom Gradio component and MCP server.\n\nBuilding the Visual Foundation\n\nThe custom Gradio component became the heart of the submission; the poker-style roundtable where participants sit and display speech bubbles showing their responses, thinking status, and research activities immediately caught the eye of anyone visiting the space. The component development was remarkably smooth thanks to Gradio's excellent developer experience, though I did encounter one documentation gap around PyPI publishing that led to my first contribution to the Gradio project.\n\nroundtable = consilium_roundtable( label= \"AI Expert Roundtable\" , label_icon= \"https://huggingface.co/front/assets/huggingface_logo-noborder.svg\" , value=json.dumps({ \"participants\" : [], \"messages\" : [], \"currentSpeaker\" : None , \"thinking\" : [], \"showBubbles\" : [], \"avatarImages\" : avatar_images }) )\n\nThe visual design proved robust throughout the hackathon; after the initial implementation, only features like user-defined avatars and center table text were added, while the core interaction model remained unchanged.\n\nIf you are interested in creating your own custom Gradio component you should take a look at Custom Components in 5 minutes and yes the title does not lie; it literally only takes 5 minutes for the basic setup.\n\nSession State Management\n\nThe visual roundtable maintains state through a session-based dictionary system where each user gets isolated state storage via user_sessions[session_id] . The core state object tracks participants , messages , currentSpeaker , thinking , and showBubbles arrays that are updated through update_visual_state() callbacks. When models are thinking, speaking, or research is being executed, the engine pushes incremental state updates to the frontend by appending to the messages array and toggling speaker/thinking states, creating the real-time visual flow without complex state machines - just direct JSON state mutations synchronized between backend processing and frontend rendering.\n\nMaking LLMs Actually Discuss\n\nWhile implementing, I realized there was no real discussion happening between the LLMs because they lacked clear roles. They received the full context of ongoing discussions but didn't know how to engage meaningfully. I introduced distinct roles to create productive debate dynamics, which, after a few tweaks, ended up being like this:\n\nself.roles = { 'standard' : \"Provide expert analysis with clear reasoning and evidence.\" , 'expert_advocate' : \"You are a PASSIONATE EXPERT advocating for your specialized position. Present compelling evidence with conviction.\" , 'critical_analyst' : \"You are a RIGOROUS CRITIC. Identify flaws, risks, and weaknesses in arguments with analytical precision.\" , 'strategic_advisor' : \"You are a STRATEGIC ADVISOR. Focus on practical implementation, real-world constraints, and actionable insights.\" , 'research_specialist' : \"You are a RESEARCH EXPERT with deep domain knowledge. Provide authoritative analysis and evidence-based insights.\" , 'innovation_catalyst' : \"You are an INNOVATION EXPERT. Challenge conventional thinking and propose breakthrough approaches.\" }\n\nThis solved the discussion problem but raised a new question: how to determine consensus or identify the strongest argument? I implemented a lead analyst system where users select one LLM to synthesize the final result and evaluate whether consensus was reached.\n\nI also wanted users to control communication structure. Beyond the default full-context sharing, I added two alternative modes:\n\nRing : Each LLM only receives the previous participant's response\n\n: Each LLM only receives the previous participant's response Star: All messages flow through the lead analyst as a central coordinator\n\nFinally, discussions need endpoints. I implemented configurable rounds (1-5), with testing showing that more rounds increase the likelihood of reaching consensus (though at higher computational cost).\n\nLLM Selection and Research Integration\n\nThe current model selection includes Mistral Large, DeepSeek-R1, Meta-Llama-3.3-70B, and QwQ-32B. While notable models like Claude Sonnet and OpenAI's o3 are absent, this reflected hackathon credit availability and sponsor award considerations rather than technical limitations.\n\nself.models = { 'mistral' : { 'name' : 'Mistral Large' , 'api_key' : mistral_key, 'available' : bool (mistral_key) }, 'sambanova_deepseek' : { 'name' : 'DeepSeek-R1' , 'api_key' : sambanova_key, 'available' : bool (sambanova_key) } ... }\n\nFor models supporting function calling, I integrated a dedicated research agent that appears as another roundtable participant. Rather than giving models direct web access, this agent approach provides visual clarity about external resource availability and ensures consistent access across all function-calling models.\n\ndef handle_function_calls ( self, completion, original_prompt: str , calling_model: str ) -> str : \"\"\"UNIFIED function call handler with enhanced research capabilities\"\"\" message = completion.choices[ 0 ].message if not hasattr (message, 'tool_calls' ) or not message.tool_calls: return message.content for tool_call in message.tool_calls: function_name = tool_call.function.name arguments = json.loads(tool_call.function.arguments) result = self._execute_research_function(function_name, arguments, calling_model_name)\n\nThe research agent accesses five sources: Web Search, Wikipedia, arXiv, GitHub, and SEC EDGAR. I built these tools on an extensible base class architecture for future expansion while focusing on freely embeddable resources.\n\nclass BaseTool ( ABC ): \"\"\"Base class for all research tools\"\"\" def __init__ ( self, name: str , description: str ): self.name = name self.description = description self.last_request_time = 0 self.rate_limit_delay = 1.0 @abstractmethod def search ( self, query: str , **kwargs ) -> str : \"\"\"Main search method - implemented by subclasses\"\"\" pass def score_research_quality ( self, research_result: str , source: str = \"web\" ) -> Dict [ str , float ]: \"\"\"Score research based on recency, authority, specificity, relevance\"\"\" quality_score = { \"recency\" : self._check_recency(research_result), \"authority\" : self._check_authority(research_result, source), \"specificity\" : self._check_specificity(research_result), \"relevance\" : self._check_relevance(research_result) } return quality_score\n\nSince research operations can be time-intensive, the speech bubbles display progress indicators and time estimates to maintain user engagement during longer research tasks.\n\nDiscovering the Open Floor Protocol\n\nAfter the hackathon, Deborah Dahl introduced me to the Open Floor Protocol, which aligns perfectly with the roundtable approach. This protocol provides standardized JSON message formatting for cross-platform agent communication. Its key differentiator from other agent-to-agent protocols is that all agents maintain constant conversation awareness exactly like sitting at the same table. Another feature I have not seen with other protocols is that the floor manager can dynamically invite and remove agents from the floor and agents.\n\nThe protocol's interaction patterns map directly to Consilium's architecture:\n\nDelegation : Transferring control between agents\n\n: Transferring control between agents Channeling : Passing messages without modification\n\n: Passing messages without modification Mediation : Coordinating behind the scenes\n\n: Coordinating behind the scenes Orchestration: Multiple agents collaborating\n\nI'm currently integrating Open Floor Protocol support to allow users to add any OFP-compliant agents to their roundtable discussions. You can follow this development at: https://huggingface.co/spaces/azettl/consilium_ofp\n\nLessons Learned and Future Implications\n\nThe hackathon introduced me to multi-agent debate research I hadn't previously encountered, including foundational studies like Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate. The community experience was remarkable; all participants actively supported each other through Discord feedback and collaboration. Seeing my roundtable component integrated into another hackathon project was one of my highlights working on Consilium.\n\nI will continue to work on Consilium and with expanded model selection, Open Floor Protocol integration, and configurable agent roles, the platform could support virtually any multi-agent debate scenario imaginable.\n\nBuilding Consilium reinforced my conviction that AI's future lies not just in more powerful individual models but in systems enabling effective AI collaboration. As specialized smaller language models become more efficient and resource-friendly, I believe roundtables of task-specific SLMs with dedicated research agents may provide compelling alternatives to general-purpose large language models for many use cases.",
    "link": "https://huggingface.co/blog/consilium-multi-llm",
    "Summary": "Consilium: When Multiple LLMs CollaboratePublished July 17, 2025 Update on GitHubPicture this: four AI experts sitting around a poker table, debating your toughest decisions in real-time.\nThat's exactly what Consilium, the multi-LLM platform I built during the Gradio Agents & MCP Hackathon , does.\nIt lets AI models discuss complex questions and reach consensus through structured debate.\nThe core idea was always about LLMs reaching consensus through discussion; that's where the name Consilium came from.\nSession State ManagementThe visual roundtable maintains state through a session-based dictionary system where each user gets isolated state storage via user_sessions[session_id] .",
    "Keywords": [
      "hackathon",
      "models",
      "agents",
      "state",
      "gradio",
      "research",
      "multiple",
      "collaborate",
      "llms",
      "str",
      "ai",
      "consilium",
      "roundtable",
      "visual"
    ]
  },
  {
    "Title": "Arc Virtual Cell Challenge: A Primer",
    "Authors": [],
    "Publish Date": null,
    "Text": "Arc Virtual Cell Challenge: A Primer\n\nPublished July 18, 2025 Update on GitHub\n\nGoal\n\nTrain a model to predict the effect on a cell of silencing a gene using CRISPR.\n\nArc Institute recently unveiled the Virtual Cell Challenge . Participants are required to train a model capable of predicting the effect of silencing a gene in a (partially) unseen cell type, a task they term context generalization. For ML engineers with little to no biology background, the jargon and required context can seem quite daunting. To encourage participation, we recapitulate the challenge in a form better suited to engineers from other disciplines.\n\nDoing things in the world of atoms is expensive, laborious and error prone. What if we could test thousands of drug candidates without ever touching a petri dish? This is the goal of the virtual cell challenge \u2014 a model (most likely a neural network) that can simulate exactly what happens to a cell when we change some parameter. Given that tightening your feedback loop is often the best way to speed up progress, a model capable of doing this accurately would have significant impact.\n\nTo train this neural network, we will need data. For the challenge, Arc has curated a dataset of ~300k single-cell RNA sequencing profiles. It may be worthwhile to revisit the Central Dogma before continuing. This essay will build off of this to provide the ~minimum biology knowledge you'll need for the challenge.\n\nTraining data\n\nThe training set consists of a sparse matrix and some associated metadata. More specifically, we have 220k cells, and for each cell we have a transcriptome. This transcriptome is a sparse row vector, where each entry is the raw count of RNA molecules (transcripts) that the corresponding gene (our column) encodes for. Of the 220k cells, ~38k are unperturbed, meaning no gene has been silenced using CRISPR. These control cells are crucial as we will see shortly.\n\nTo understand the dataset more concretely, let's select a gene, TMSB4X (the most frequently silenced gene in the dataset) and compare the number of RNA molecules detected for a control cell and a perturbed cell.\n\n\n\n\n\nWe can see that the cell with TMSB4X silenced has a greatly reduced number of transcripts compared with the control cells.\n\nModelling the challenge\n\nThe astute among you may be wondering why you don't just measure the count of the RNA molecules before and after silencing the gene \u2014 why do we need the control cells at all? Unfortunately, reading the transcriptome destroys the cell, which is a problem reminiscent of the observer effect.\n\nThis inability to measure the cell state before and after introduces many issues, as we are forced to use a population of basal (a.k.a control, unperturbed) cells as a reference point. The control cells and perturbed cells are not entirely homogeneous even prior to the perturbation. This means that we have to now separate out our true signal, the perturbation, from noise induced by the heterogeneity.\n\nMore formally, we can model observed gene expression in perturbed cells as:\n\nX ^ p \u223c T ^ p ( D basal ) + H ( D basal ) + \u03b5 , \u03b5 \u223c P \u03b5 \\hat{X}_p \\sim \\hat{T}_p(\\mathcal{D}_{\\text{basal}}) + H(\\mathcal{D}_{\\text{basal}}) + \\varepsilon, \\quad \\varepsilon \\sim P_\\varepsilon X^p\u200b\u223cT^p\u200b(Dbasal\u200b)+H(Dbasal\u200b)+\u03b5,\u03b5\u223cP\u03b5\u200b\n\nwhere:\n\nX ^ p \\hat{X}_p X ^ p \u200b : The observed gene expression measurements in cells with perturbation p p p\n\n: The observed gene expression measurements in cells with perturbation D basal \\mathcal{D}_{\\text{basal}} D basal \u200b : The distribution of the unperturbed, baseline cell population.\n\n: The distribution of the unperturbed, baseline cell population. T ^ p ( D basal ) \\hat{T}_p(\\mathcal{D}_{\\text{basal}}) T ^ p \u200b ( D basal \u200b ) : True effect caused by perturbation p p p on the population.\n\n: True effect caused by perturbation on the population. H ( D basal ) H(\\mathcal{D}_{\\text{basal}}) H ( D basal \u200b ) : Biological heterogeneity of the baseline population.\n\n: Biological heterogeneity of the baseline population. \u03b5 \\varepsilon \u03b5 : Experiment-specific technical noise, assumed independent of the unperturbed cell state and D basal \\mathcal{D}_{\\text{basal}} D basal \u200b .\n\nSTATE: The baseline from Arc\n\nPrior to the Virtual Cell Challenge, Arc released STATE, their own attempt to solve the challenge using a pair of transformer based models. This serves as a strong baseline for participants to start with, so we will explore it in detail.\n\nSTATE consists of two models, the State Transition Model (ST) and the State Embedding Model (SE). SE is designed to produce rich semantic embeddings of cells in an effort to improve cross cell type generalization. ST is the \"cell simulator\", that takes in either a transcriptome of a control cell, or an embedding of a cell produced by SE, along with a one hot encoded vector representing the perturbation of interest, and outputs the perturbed transcriptome.\n\nState Transition Model (ST)\n\n\n\n\n\nThe State Transition Model is a relatively simple transformer with a Llama backbone that operates upon the following:\n\nA set of transcriptomes (or SE embeddings) for covariate matched basal cells. A set of one hot vectors representing our gene perturbation for each cell.\n\nUsing a covariate matched set of control cells with paired target cells should assist the model in discerning the actual effect of our intended perturbation. Both the control set tensor and the perturbation tensor are fed through independent encoders, which are simply 4 layer MLPs with GELU activations. If working directly in gene expression space (i.e producing a full transcriptome), they pass the output through a learned decoder.\n\nST is trained using Maximum Mean Discrepancy. Put simply, the model learns to minimize the difference between the two probability distributions.\n\nState Embedding Model (SE)\n\n\n\n\n\nThe State Embedding Model is a BERT-like autoencoder. To understand this more deeply, first we have to take a little detour for some more biological grounding.\n\nA little biological detour\n\n\n\n\n\nA gene consists of exons (protein coding sections) and introns (non-protein coding sections). DNA is first transcribed into pre-mRNA, as shown above. The cell then performs Alternative Splicing. This is basically \"pick and choose exons\", cut out all introns. You can think of the gene as an IKEA manual for making a table. One could also construct a 3 legged table, perhaps an odd bookshelf with some effort, by leaving out some parts. These different objects are analogous to protein isoforms, proteins coded for by the same gene.\n\nBack to the model\n\nWith this basic understanding, we can move on to how the SE model works. Remember, our core goal for SE is to create meaningful cell embeddings. To do this, we must first create meaningful gene embeddings.\n\nTo produce a single gene embedding, we first obtain the amino acid sequence (e.g SDKPDMAEI \\texttt{SDKPDMAEI} SDKPDMAEI... for TMSB4X) of all the different protein isoforms encoded for by the gene in question. We then feed these sequences to ESM2, a 15B parameter Protein Language Model from FAIR. ESM produces an embedding per amino acid, and we mean pool them together to obtain a \"transcript\" (a.k.a protein isoform) embedding.\n\nNow we have all of these protein isoform embeddings, we then just mean pool those to get the gene embedding. Next, we project these gene embeddings to our model dimension using a learned encoder as follows:\n\ng ~ j = SiLU ( LayerNorm ( g j W g + b g ) ) \\tilde{g}_j = \\text{SiLU}(\\text{LayerNorm}(g_j \\mathbf{W}_g + \\mathbf{b}_g)) g~\u200bj\u200b=SiLU(LayerNorm(gj\u200bWg\u200b+bg\u200b))\n\nWe've now obtained a gene embedding, but what we really want is a cell embedding. To do this, Arc represents each cell as the top 2048 genes ranked by log fold expression level.\n\nWe then construct a \"cell sentence\" from our 2048 gene embeddings as follows: c ~ ( i ) = [ z cls , g ~ 1 ( i ) , g ~ 2 ( i ) , \u2026 , g ~ L ( i ) , z ds ] \u2208 R ( L + 2 ) \u00d7 h \\tilde{\\mathbf{c}}^{(i)} = \\left[\\mathbf{z}_{\\text{cls}}, \\tilde{\\mathbf{g}}_1^{(i)}, \\tilde{\\mathbf{g}}_2^{(i)}, \\ldots, \\tilde{\\mathbf{g}}_L^{(i)}, \\mathbf{z}_{\\text{ds}}\\right] \\in \\mathbb{R}^{(L+2) \\times h} c~(i)=[zcls\u200b,g~\u200b1(i)\u200b,g~\u200b2(i)\u200b,\u2026,g~\u200bL(i)\u200b,zds\u200b]\u2208R(L+2)\u00d7h\n\nWe add a [CLS] \\texttt{[CLS]} [CLS] token and [DS] \\texttt{[DS]} [DS] token to our sentence. The [CLS] \\texttt{[CLS]} [CLS] token ends up being used as our \"cell embedding\" (very BERT-like) and the [DS] \\texttt{[DS]} [DS] token is used to \"disentangle dataset-specific effects\". Although the genes are sorted by log fold expression level, Arc further enforces the magnitude of each genes expression by incorporating the transcriptome in a fashion analogous to positional embeddings. Through an odd \"soft binning\" algorithm and 2 MLPs, they create some \"expression encodings\" which they then add to each gene embedding. This should modulate the magnitude of each gene embedding by how intensely it is expressed in the transcriptome.\n\nTo train the model, they mask 1280 genes per cell, and the model is tasked with predicting them. The 1280 genes are selected such that they have a wide range of expression intensities. For the graphically inclined, the below demonstrates the construction of the cell sentence.\n\n\n\n\n\nEvaluations\n\nUnderstanding how your submission will be evaluated is key to success. The 3 evaluation metrics chosen by Arc are Perturbation Discrimination, Differential Expression and Mean Average Error. Given that Mean Average Error is simple and exactly as it sounds, we will omit it from our analysis.\n\nPerturbation Discrimination\n\n\n\n\n\nPerturbation Discrimination intends to evaluate how well your model can uncover relative differences between perturbations. To do this, we compute the Manhattan distances for all the measured perturbed transcriptomes in the test set (the ground truth we are trying to predict, y t y_t yt\u200b and all other perturbed transcriptomes, y p n y_p^n ypn\u200b) to our predicted transcriptome y ^ t \\hat{y}_t y^\u200bt\u200b. We then rank where the ground truth lands with respect to all transcriptomes as follows:\n\nr t = \u2211 p \u2260 t 1 { d ( y ^ t , y p ) < d ( y ^ t , y t ) } r_t = \\sum_{p\n\neq t} \\mathbf{1}\\{d(\\hat{y}_t, y_p) < d(\\hat{y}_t, y_t)\\} rt\u200b=p\ue020=t\u2211\u200b1{d(y^\u200bt\u200b,yp\u200b)<d(y^\u200bt\u200b,yt\u200b)}\n\nAfter, we normalize by the total number of transcriptomes:\n\nPDisc t = r t T \\text{PDisc}_t = \\frac{r_t}{T} PDisct\u200b=Trt\u200b\u200b\n\nWhere 0 0 0 would be a perfect match. The overall score for your predictions is the mean of all $$\\text{PDisc}_t$$. This is then normalized to:\n\nPDiscNorm = 1 \u2212 2 PDisc \\text{PDiscNorm} = 1 - 2\\text{PDisc} PDiscNorm=1\u22122PDisc\n\nWe multiply by 2 as for a random prediction, ~half of the results would be closer and half would be further away.\n\nDifferential Expression\n\nDifferential Expression intends to evaluate what fraction of the truly affected genes did you correctly identify as significantly affected. Firstly, for each gene compute a p p p-value using a Wilcoxon rank-sum test with tie correction. We do this for both our predicted perturbation distribution and the ground truth perturbation distribution.\n\nNext, we apply the Benjamini-Hochberg procedure, basically some stats to modulate the p p p-values, as with 20 , 000 20,000 20,000 genes and a p p p-value threshold of 0.05 0.05 0.05, you'd expect 1 , 000 1,000 1,000 false positives. We denote our set of predicted differentially expressed genes G p , p r e d G_{p,pred} Gp,pred\u200b, and the ground truth set of differentially expressed genes G p , t r u e G_{p,true} Gp,true\u200b.\n\nIf the size of our set is less than the ground truth set size, take the intersection of the sets, and divide by the true number of differentially expressed genes as follows:\n\nD E p = G p , p r e d \u2229 G p , t r u e n p , t r u e DE_p = \\frac{G_{p,pred} \\cap G_{p,true}}{n_{p,true}} DEp\u200b=np,true\u200bGp,pred\u200b\u2229Gp,true\u200b\u200b\n\nIf the size of our set is greater than the ground truth set size, select the subset we predict are most differentially expressed (our \"most confident\" predictions, denoted G ~ p , p r e d \\tilde{G}_{p,pred} G~p,pred\u200b), take the intersection with the ground truth set, and then divide by the true number.\n\nD E p = G ~ p , p r e d \u2229 G p , t r u e n p , t r u e DE_p = \\frac{\\tilde{G}_{p,pred} \\cap G_{p,true}}{n_{p,true}} DEp\u200b=np,true\u200bG~p,pred\u200b\u2229Gp,true\u200b\u200b\n\nDo this for all predicted perturbations and take the mean to obtain the final score.\n\nConclusion\n\nIf this challenge has piqued your interest, how can one get started? Fortunately, Arc has provided a Colab notebook that walks through the entire process of training their STATE model. Furthermore, STATE will be hitting transformers very soon, so starting with their pretrained models will be as simple as:\n\nimport torch from transformers import StateEmbeddingModel model_name = \"arcinstitute/SE-600M\" model = StateEmbeddingModel.from_pretrained(model_name) input_ids = torch.randn(( 1 , 1 , 5120 ), dtype=torch.float32) mask = torch.ones(( 1 , 1 , 5120 ), dtype=torch. bool ) mask[:, :, 2560 :] = False outputs = model(input_ids, mask)\n\nBest of luck to all participants!\n\nThis post was originally published here.",
    "link": "https://huggingface.co/blog/virtual-cell-challenge",
    "Summary": "Arc Virtual Cell Challenge: A PrimerPublished July 18, 2025 Update on GitHubGoalTrain a model to predict the effect on a cell of silencing a gene using CRISPR.\nT ^ p ( D basal ) \\hat{T}_p(\\mathcal{D}_{\\text{basal}}) T ^ p \u200b ( D basal \u200b ) : True effect caused by perturbation p p p on the population.\nState Embedding Model (SE)The State Embedding Model is a BERT-like autoencoder.\nNext, we apply the Benjamini-Hochberg procedure, basically some stats to modulate the p p p-values, as with 20 , 000 20,000 20,000 genes and a p p p-value threshold of 0.05 0.05 0.05, you'd expect 1 , 000 1,000 1,000 false positives.\nD E p = G ~ p , p r e d \u2229 G p , t r u e n p , t r u e DE_p = \\frac{\\tilde{G}_{p,pred} \\cap G_{p,true}}{n_{p,true}} DEp\u200b=np,true\u200bG~p,pred\u200b\u2229Gp,true\u200b\u200bDo this for all predicted perturbations and take the mean to obtain the final score.",
    "Keywords": [
      "g",
      "cell",
      "set",
      "perturbation",
      "cells",
      "challenge",
      "p",
      "d",
      "gene",
      "embedding",
      "primer",
      "virtual",
      "model",
      "arc"
    ]
  },
  {
    "Title": "Back to The Future: Evaluating AI Agents on Predicting Future Events",
    "Authors": [],
    "Publish Date": null,
    "Text": "Back to The Future: Evaluating AI Agents on Predicting Future Events\n\nPublished July 17, 2025 Update on GitHub\n\nMost current AI benchmarks focus on answering questions about the past, either by testing models on existing knowledge (in a static manner, such as HLE or GPQA, or augmented, like BrowseComp or GAIA) or previously solved problems (like PaperBench, DABStep, or most coding evaluations). However, we believe that more valuable AI, and ultimately AGI, will be distinguished by its ability to use this past to forecast interesting aspects of the future, rather than merely reciting old facts.\n\nForecasting future events is a complex and holistic task: it requires sophisticated reasoning, synthesis, weighing probabilities and genuine understanding, rather than pattern matching against or searching existing information. Evaluating models on their ability to predict future outcomes, whether in science, economics, geopolitics, or technology tests the kind of intelligence that creates real-world value.\n\nBeyond its inherent importance, this forecasting-based approach also solves many methodological problems faced by current evaluations and benchmarks. Traditional benchmarks that measure accuracy on fixed test sets are inevitably affected by possible data contamination, and without access to the full reproducible training pipeline of a model, it's hard to trust the results. The most serious evaluation efforts now keep their test sets completely private, creating a frustrating arms race between evaluators and potential \"gaming the leaderboard\" mechanics (Singh et al., 2025).\n\nForecasting makes contamination impossible by design, as you can't train on data that doesn't yet exist! This creates a level playing field where success depends on reasoning capability rather than memorization.\n\nPerhaps most importantly, predictions about the future are inherently verifiable. We can wait and see who was right, creating an objective, time-stamped measure of model performance.\n\nWe therefore propose evaluating agents on their ability to predict future events (Ye et al., 2024; Karger et al., 2025). FutureBench draws from real-world prediction markets and emerging news to create interesting prediction tasks grounded in actual future outcomes. We collect events from platforms and live news coverages and manifold markets, filtering them to focus on emerging events worth predicting. Using an agent-based approach, we curate scenarios that require genuine reasoning rather than simple pattern matching. Think geopolitical developments, market movements, or technology adoption trends - events where informed analysis actually matters.\n\nCan Agents Predict Future Events?\n\nThis is the obvious question, and it's at the heart of what makes this benchmark interesting! We believe the answer cannot be a simple \u201cyes\u201d or a \u201cno\u201d, as it mostly depends on the actual questions; there are always important caveats to consider. Humans constantly use their ability to weigh current information to predict future events. Aren't most career moves, relationship choices, or even business strategies essentially bets on future outcomes?\n\nSome predictions involve irreducible uncertainty (Will it rain on December 17th, 2027 at noon?), but many don't. When a skilled analyst predicts a company's quarterly earnings or a policy expert forecasts election outcomes, they're using available information to make informed decisions. This is precisely what we're asking AI agents to do with FutureBench! The task isn't to get agents to fortune-tell, but rather to synthesize information and reason under stronger uncertainty than most other benchmarks.\n\nThe agent's prediction quality directly reflects its ability to search relevant information, synthesize complex data, and reason about cause-and-effect relationships. These are precisely the capabilities we want to measure in real-world applications.\n\nTools like DeepResearch are already used for market analysis and strategic planning. The quality of information collection strongly correlates with decision-making effectiveness. FutureBench is inspired by this evaluation process and tries to compute agents\u2019 quality with objective, verifiable outcomes.\n\nFutureBench\n\nBuilding a benchmark that tests real prediction capabilities requires a steady stream of meaningful questions. We've developed two complementary approaches that capture different types of future events:\n\n1. News-Generated Questions: Finding Tomorrow's Headlines Today\n\nOur first approach uses AI to mine current events for prediction opportunities. We deploy a smolagents-based agent to scrape a few major news websites, analyze front-page articles, and generate prediction questions about their likely outcomes. The agent reads through and identifies interesting articles and formulates specific, time-bound questions from their content, for example \"Will the Federal Reserve cut interest rates by at least 0.25% by July 1st, 2025?\"\n\nWe guide this process with carefully crafted prompts that specify what makes a good prediction question\u2014events that are meaningful, verifiable, and uncertain extraction time.\n\nTechnical Stack:\n\nModel : DeepSeek-V3 for reasoning and question generation\n\n: DeepSeek-V3 for reasoning and question generation Scraping : Firecrawl for reliable content extraction\n\n: Firecrawl for reliable content extraction Search: Tavily for additional context when needed\n\nThe agent typically generates 5 questions per scraping session, with a time horizon of a single week, meaning that we assume we\u2019ll know the answer to the question after seven days. This gives us a natural pipeline of fresh evaluation material tied to real-world events.\n\n2. Polymarket Integration: Leveraging Prediction Markets\n\nOur second source draws from Polymarket. These questions come from a prediction market platform where real participants make forecasts about future events. We currently ingest around 8 questions per week.\n\nHowever, the raw data needs filtering. We apply strong filtering to remove general questions regarding temperature and some questions regarding the stock and crypto markets, which would otherwise be too numerous for practical use in our benchmark. In addition to this, polymarket questions have less constraints regarding the final \u201crealization\u201d time, the actual outcome of the event could be available only next month or by the end of the year. These are still very relevant questions, but the data collection of the outcome is more sparse.\n\nExample Questions\n\nHere's an example of what comes out of our question generation pipeline:\n\nNews-Generated Polymarket \"Will the Federal Reserve cut interest rates by at least 0.25% by July 1st, 2025?\" \"Will monthly inflation increase by 0.2% in June?\" \"Will Ukraine and Russia hold peace negotiations by July 8th, 2025?\" \"Will Zohran Mamdani\u2019s RCV margin of victory be greater than 13% in the New York City Mayoral Democratic Primary?\"\n\nFuture Bench: Three Levels of Systematic Evaluation\n\nThe next question is, what does this type of benchmark allow us to measure? The framework operates on three distinct levels, allowing us to isolate exactly what we're measuring:\n\nLevel 1: Framework Comparison Keep the underlying LLMs and tools constant while varying frameworks. How does a LangChain-based agent compare to one built with CrewAI when both use GPT-4 and the same search tools? This isolates the impact of different agentic frameworks.\n\nLevel 2: Tool Performance Fix the LLM and framework while comparing different implementations. Which search tool (for example Tavily, Google, Bing) leads to better predictions than other search engines, holding everything else constant? This reveals which tools actually provide value. How much value do tools bring in general with respect to models without tools?\n\nLevel 3: Model Capabilities Hold the framework and tools constant while testing different LLMs. Given access to the same set of tools, does DeepSeek-V3 use them as effectively as GPT-4? This measures pure reasoning ability. This systematic approach lets us understand exactly where performance gains and losses occur in the agent pipeline.\n\nThe benchmark also serves as a robust test of instruction following. Agents must respect specific formatting requirements and generate actions that can be correctly parsed and executed. In practice, this often reveals where smaller language models struggle with complex multi-step reasoning.\n\n\ud83d\ude80 Try it yourself! Explore the live leaderboard: FutureBench Interactive Leaderboard\n\nPredicting The Future: Agents and Initial Results\n\nWe use SmolAgents as a baseline agent framework for all the questions. We also compute performance on the base models. For the prediction task itself, the agents get access to a focused toolkit:\n\nSearch : Tavily integration for finding recent information and expert analysis\n\n: Tavily integration for finding recent information and expert analysis Web Scraper: A simple web scraping tool for following up on specific sources and getting detailed context.\n\nThis intentionally lean setup forces agents to be strategic about information gathering while still providing the tools needed for informed predictions.\n\nInitial Results\n\nWe compare different models using smolagents as a baseline (you can find the leaderboard on our HF Space). We also run the standard language models without internet access to estimate a general prior. As expected, we see agentic models performing better than simple language models; stronger models show more stable prediction quality. Overall we also find interesting patterns in how different models try to approach a question:\n\nInteresting Action Patterns\n\nRunning this benchmark has revealed insights into how different models approach information gathering. One striking difference is with respect to scraping. GPT-4.1 appears to rely more on search results. Claude3.7 and 4 explore the web space in more detail and tend to use web scraping more frequently; this thorough approach also means collecting many more input tokens during the research process, thus increasing the cost.\n\nModels show interesting approaches to making predictions, for example, to answer the question \"Will annual inflation increase by 2.6 or more in June?\":\n\nThe DeepSeekV3 agent analyzed June 2025 inflation prospects by searching recent CPI data (finding current inflation at 2.4-2.8%), considered tariff impacts as upward pressure, and concluded inflation would exceed the 2.6% threshold.\n\nClaude3.7 analyzed June 2025 inflation through comprehensive research (11 searches vs DeepSeekV3's 3), systematically gathering May 2025 CPI data (2.4% year-over-year), identifying decelerating monthly trends (0.2%\u21920.1%), weighing tariff pressures against Fed restrictive policy, calculating precise 0.2% gap needed, and concluded recent deceleration made reaching 2.6% threshold unlikely, answering \"No.\"\n\nGPT4.1 analyzed June 2025 inflation through targeted searches for market consensus and forecasts, identified May 2025 CPI at 2.4% (below 2.5% expectations), noted weak 0.1% monthly increases, found no forecaster predictions of 2.6%+ for June, and concluded the jump from 2.4% to 2.6% was unlikely given recent below-expectation trends.\n\nInterestingly, Claude was the only model that tried to access the Bureau of Labor Statistics website to scrape it directly, which failed because it is a .gov website and we do not allow this type of action.\n\nThe models exhibit distinct reasoning patterns in their outputs. GPT's analysis focused on consensus forecasts as the key signal for future events rather than extrapolating from current data, while Claude's approach exhibited rigorous analytical structure with its systematic pro/con framework and quantitative gap analysis, and DeepSeekV3's output displayed explicit acknowledgment of data limitations and systematic methodology adjustments when initial approaches encountered constraints.\n\nThese behavioral differences reveal interesting patterns in how different models approach information gathering. The variations in web usage and token consumption suggest that models have distinct strategies for tackling prediction tasks, which FutureBench can help us measure and understand.\n\nLimitations and Future Directions\n\nOne challenge is that evaluation can be expensive due to the large number of input tokens. For example, Claude tends to visit web pages frequently, thus accumulating many input tokens. In a multi-turn loop, this can make the number of input tokens skyrocket very quickly. This increases the cost of any subsequent generation, even though most tokens are eventually cached.\n\nFutureBench is an evolving benchmark, as we discover new findings and better patterns, we\u2019ll keep incorporating them. We would love feedback from the community to understand how to better source questions, which experiments to run and which data is the most interesting to analyze.\n\nReferences\n\nSingh, S., Nan, Y., Wang, A., D'souza, D., Kapoor, S., Ustun, A., Koyejo, S., Deng, Y., Longpre, S., Smith, N., Ermi\u015f, B.H., Fadaee, M., & Hooker, S. (2025). The Leaderboard Illusion. ArXiv, abs/2504.20879.\n\nKarger, E., Bastani, H., Yueh-Han, C., Jacobs, Z., Halawi, D., Zhang, F., & Tetlock, P.E. (2025). ForecastBench: A Dynamic Benchmark of AI Forecasting Capabilities. ICLR.\n\nYe, C., Hu, Z., Deng, Y., Huang, Z., Ma, M.D., Zhu, Y., & Wang, W. (2024). MIRAI: Evaluating LLM Agents for Event Forecasting. ArXiv, abs/2407.01231.",
    "link": "https://huggingface.co/blog/futurebench",
    "Summary": "We therefore propose evaluating agents on their ability to predict future events (Ye et al., 2024; Karger et al., 2025).\nCan Agents Predict Future Events?\nHumans constantly use their ability to weigh current information to predict future events.\nThe agent's prediction quality directly reflects its ability to search relevant information, synthesize complex data, and reason about cause-and-effect relationships.\nThese behavioral differences reveal interesting patterns in how different models approach information gathering.",
    "Keywords": [
      "data",
      "agents",
      "models",
      "predicting",
      "information",
      "future",
      "2025",
      "ai",
      "prediction",
      "events",
      "evaluating",
      "questions",
      "approach"
    ]
  },
  {
    "Title": "Ettin Suite: SoTA Paired Encoders and Decoders",
    "Authors": [],
    "Publish Date": null,
    "Text": "Ettin Suite: SoTA Paired Encoders and Decoders\n\nPublished July 16, 2025 Update on GitHub\n\nWhat would happen if you took the ModernBERT recipe and applied it to a decoder-only model? Turns out, a state-of-the-art decoder language model that beats Llama 3.2 1B and SmolLM2!\n\nWe introduce a new open-data training recipe to reproduce the encoder-only ModernBERT model (and actually beat it!). We then apply the exact same recipe to decoder-only models. For the first time, we have two state-of-the-art models trained in the same setup but with two different training objectives: masked language modeling (MLM), and causal language modeling (CLM).\n\nThis blog post introduces Ettin, the first suite of SoTA paired encoder-only and decoder-only models (17M-1B params) trained with identical data (2T tokens), architecture, and training recipes. Ettin enables true apples-to-apples comparisons between architectures and delivers state-of-the-art performance for open-data models in both categories. We then further explore whether it is possible to get a competitive encoder starting from the decoder and vice-versa.\n\nIf you are interested in trying out the models, some boilerplates are available at the end of this blogpost!\n\nEncoders vs Decoders: The Architecture Divide\n\nThe LLM community has largely converged on decoder-only models like GPT, Llama, and Qwen. Their generative capabilities are impressive, but this focus is detracting attention from other categories, such as encoder-only models like BERT.\n\nHowever, encoder BERT-like models remain the workhorses of production systems for classification, retrieval, and embedding tasks. They're faster, more memory-efficient, and often more accurate for discriminative tasks. The key difference lies in their attention patterns:\n\nEncoder models use bidirectional attention, allowing each token to \"see\" all other tokens in the sequence (fully visible)\n\nuse bidirectional attention, allowing each token to \"see\" all other tokens in the sequence (fully visible) Decoder models use causal attention, where tokens can only \"see\" previous tokens to enable autoregressive generation\n\nWhile decoder models have seen rapid innovation, encoder model development had stagnated \u2013 until recently, with efforts like ModernBERT modernizing them. But which architecture is better? Previous comparisons between encoders and decoders used different datasets, architectures, and training recipes, so it was hard to tell.\n\nNamed after the two-headed Norse giant, Ettin provides a controlled comparison by training with both architectures on identical data, identical model shapes, and identical training recipes. They only differ in attention patterns and training objectives!\n\nTraining Recipe: Modern Techniques for Both Architectures\n\nWe build on the ModernBERT recipe, which borrowed modern techniques from decoder-only models and brought them to encoder training. This provides a strong base for training both architectures.\n\nSizes\n\nWe train six different sizes, ranging from 17M to 1B parameters. This allows us to test the effects of scale, and provides a wide variety of models for you to use! No matter if you need a blazing fast on-device model or a powerful but slower model, we got you covered!\n\nThree-Phase Training Process\n\nWe use a comprehensive three-phase training approach to maximize performance:\n\nPhase 1 - Pre-training (1.7T tokens): We start with a diverse mixture of high-quality data sources, training on shorter contexts (1024 tokens) to establish strong foundational knowledge.\n\nPhase 2 - Context Extension (250B tokens): We increase context length to 8K tokens using higher-quality filtered data, allowing models to understand longer documents and more complex relationships.\n\nPhase 3 - Decay (100B tokens): We finish with premium data sources including scientific papers, textbooks, and curated content while gradually reducing the learning rate.\n\nModern Architecture Components\n\nOur encoder models gain all the benefits of ModernBERT's speed, allowing them to be significantly faster than the previous generations of encoders.\n\nData Sources and Quality\n\nUnlike ModernBERT, all our training data is public and reproducible:\n\nYou can continue to train these models on new data or propose a new recipe to further improve results!\n\nEncoder Results: Beating ModernBERT\n\nOur encoder models outperform ModernBERT across all tasks and model sizes, while using completely open training data. Since we provide a large range of sizes, you can now use ModernBERT-style models in smaller sizes (great for on-device or for fast-inference), or power up with a 1B-sized encoder that crushes the competition.\n\nDecoder Results: Beating Llama 3.2 and SmolLM2\n\nApplying the same recipe to decoder models yields equally impressive results, with our models outperforming or matching established baselines such as Llama 3.2 and SmolLM2:\n\nThe gains are particularly strong on knowledge-intensive tasks like SciQ, reflecting the benefits of our high-quality training data mixture. These results demonstrate that our training recipe creates genuinely strong models in both architectural paradigms.\n\nFair Fight: Encoders vs Decoders on Even Ground\n\nFor the first time, we can fairly compare encoder and decoder architectures trained with identical data and recipes. The results reveal fundamental architectural advantages that persist even when all other factors are controlled:\n\nArchitecture-Specific Advantages Persist\n\nThe results show clear patterns:\n\nEncoders dominate classification and retrieval: On MNLI classification, even a 150M encoder (89.2) outperforms a 400M decoder (88.2). For retrieval tasks, the gap is smaller but still noticeable - especially when decoders are not trained with MNTP.\n\nDecoders excel at generation: On generative tasks, decoders maintain consistent advantages, with the performance gap actually widening at larger model sizes.\n\nSize doesn't always matter: A 400M encoder beats a 1B decoder on classification tasks, while a 400M decoder beats a 1B encoder on generation tasks.\n\nCross-Objective Training Falls Short\n\nDue to the lack of new encoder models, works like LLM2Vec have proposed to continue pre-training decoders with MLM. We can now test the effectiveness of this strategy!\n\nWe switched the objective and continued to train our models with the opposite objective for 50B additional tokens. This is what we found:\n\nEncoder-from-decoder : Still generally trails native encoders on classification/retrieval\n\n: Still generally trails native encoders on classification/retrieval Decoder-from-encoder: Are significantly worse than native decoders, especially at larger scales. This may be because the encoders were trained with MLM instead of MNTP (masked next token prediction) as proposed by LLM2Vec (and used in our encoder from decoder recipe).\n\nThis suggests the architecture choice matters fundamentally, not just the training objective.\n\nBeyond Performance: Understanding Model Behavior\n\nWith identical training data, we can study how different objectives affect learning. For example, analyzing gender bias using the WinoGender benchmark reveals:\n\nEncoder models prefer gender-neutral pronouns more often (60%+ neutral vs 30%+ for decoders)\n\nprefer gender-neutral pronouns more often (60%+ neutral vs 30%+ for decoders) Both architectures show male bias, but decoders slightly more so\n\nshow male bias, but decoders slightly more so Cross-objective training affects bias patterns in measurable ways\n\nThis opens doors for systematic studies of how training objectives influence model behavior beyond just accuracy metrics.\n\nUsage Examples\n\nYou can use these models with just a few lines of code!\n\nEncoders\n\nfrom transformers import AutoTokenizer, AutoModel tokenizer = AutoTokenizer.from_pretrained( \"jhu-clsp/ettin-encoder-150m\" ) model = AutoModel.from_pretrained( \"jhu-clsp/ettin-encoder-150m\" ) def predict_masked_token ( text ): inputs = tokenizer(text, return_tensors= \"pt\" ) with torch.no_grad(): outputs = model(**inputs) mask_indices = torch.where(inputs[ \"input_ids\" ] == tokenizer.mask_token_id) predictions = outputs.logits[mask_indices] top_tokens = torch.topk(predictions, 5 , dim=- 1 ) return [tokenizer.decode(token) for token in top_tokens.indices[ 0 ]] masked_text = \"The capital of France is [MASK].\" predictions = predict_masked_token(masked_text) print ( f\"Predictions: {predictions} \" )\n\nFor classification and retrieval tasks, use encoder models: You may want to use a fine-tuned version for these tasks as well.\n\nDecoders\n\nFor text generation tasks, use decoder models:\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM tokenizer = AutoTokenizer.from_pretrained( \"jhu-clsp/ettin-decoder-150m\" ) model = AutoModelForCausalLM.from_pretrained( \"jhu-clsp/ettin-decoder-150m\" ) prompt = \"The future of artificial intelligence is\" inputs = tokenizer(prompt, return_tensors= \"pt\" ) outputs = model.generate(inputs.input_ids, max_length= 50 , temperature= 0.7 ) generated_text = tokenizer.decode(outputs[ 0 ], skip_special_tokens= True )\n\nFine-tuning Examples\n\nEncoders\n\nClick to see how to finetune this into a dense embedding model using Sentence Transformers import argparse from datasets import load_dataset from sentence_transformers import ( SentenceTransformer, SentenceTransformerTrainer, SentenceTransformerTrainingArguments, ) from sentence_transformers.evaluation import TripletEvaluator from sentence_transformers.losses import CachedMultipleNegativesRankingLoss from sentence_transformers.training_args import BatchSamplers def main (): parser = argparse.ArgumentParser() parser.add_argument( \"--lr\" , type = float , default= 8e-5 ) parser.add_argument( \"--model_name\" , type = str , default= \"jhu-clsp/ettin-encoder-150m\" ) args = parser.parse_args() lr = args.lr model_name = args.model_name model_shortname = model_name.split( \"/\" )[- 1 ] model = SentenceTransformer(model_name) dataset = load_dataset( \"sentence-transformers/msmarco-co-condenser-margin-mse-sym-mnrl-mean-v1\" , \"triplet-hard\" , split= \"train\" , ) dataset_dict = dataset.train_test_split(test_size= 1_000 , seed= 12 ) train_dataset = dataset_dict[ \"train\" ].select( range ( 1_250_000 )) eval_dataset = dataset_dict[ \"test\" ] loss = CachedMultipleNegativesRankingLoss(model, mini_batch_size= 16 ) run_name = f\" {model_shortname} -DPR- {lr} \" args = SentenceTransformerTrainingArguments( output_dir= f\"output/ {model_shortname} / {run_name} \" , num_train_epochs= 1 , per_device_train_batch_size= 512 , per_device_eval_batch_size= 512 , warmup_ratio= 0.05 , fp16= False , bf16= True , batch_sampler=BatchSamplers.NO_DUPLICATES, learning_rate=lr, save_strategy= \"steps\" , save_steps= 500 , save_total_limit= 2 , logging_steps= 500 , run_name=run_name, ) dev_evaluator = TripletEvaluator( anchors=eval_dataset[ \"query\" ], positives=eval_dataset[ \"positive\" ], negatives=eval_dataset[ \"negative\" ], name= \"msmarco-co-condenser-dev\" , ) dev_evaluator(model) trainer = SentenceTransformerTrainer( model=model, args=args, train_dataset=train_dataset, eval_dataset=eval_dataset, loss=loss, evaluator=dev_evaluator, ) trainer.train() dev_evaluator(model) model.save_pretrained( f\"output/ {model_shortname} / {run_name} /final\" ) model.push_to_hub(run_name, private= False ) if __name__ == \"__main__\" : main()\n\nClick to see how to finetune this into a multi-vector embedding model with PyLate from datasets import load_dataset from pylate import losses, models, utils from sentence_transformers import ( SentenceTransformerTrainer, SentenceTransformerTrainingArguments, ) def main (): train = load_dataset( path= \"lightonai/ms-marco-en-bge\" , name= \"train\" , ) queries = load_dataset( path= \"lightonai/ms-marco-en-bge\" , name= \"queries\" , ) documents = load_dataset( path= \"lightonai/ms-marco-en-bge\" , name= \"documents\" , ) train.set_transform( utils.KDProcessing(queries=queries, documents=documents).transform, ) num_train_epochs = 1 lr = 8e-5 batch_size = 16 accum_steps = 1 model_name = \"jhu-clsp/ettin-encoder-150m\" model_shortname = model_name.split( \"/\" )[- 1 ] run_name = f\" {model_shortname} -colbert-KD- {lr} \" output_dir = f\"output/ {model_shortname} / {run_name} \" model = models.ColBERT(model_name_or_path=model_name) args = SentenceTransformerTrainingArguments( output_dir=output_dir, num_train_epochs=num_train_epochs, per_device_train_batch_size=batch_size, fp16= False , bf16= True , run_name=run_name, logging_steps= 10 , learning_rate=lr, gradient_accumulation_steps=accum_steps, warmup_ratio= 0.05 , ) train_loss = losses.Distillation(model=model) trainer = SentenceTransformerTrainer( model=model, args=args, train_dataset=train, loss=train_loss, data_collator=utils.ColBERTCollator(tokenize_fn=model.tokenize), ) trainer.train() model.save_pretrained( f\" {output_dir} /final\" ) if __name__ == \"__main__\" : main()\n\nClick to see how to finetune this into a sparse retrieval model using Sentence Transformers import logging from datasets import load_dataset from sentence_transformers import ( SparseEncoder, SparseEncoderModelCardData, SparseEncoderTrainer, SparseEncoderTrainingArguments, ) from sentence_transformers.sparse_encoder.evaluation import SparseNanoBEIREvaluator from sentence_transformers.sparse_encoder.losses import SparseMultipleNegativesRankingLoss, SpladeLoss from sentence_transformers.training_args import BatchSamplers logging.basicConfig( format = \"%(asctime)s - %(message)s\" , datefmt= \"%Y-%m-%d %H:%M:%S\" , level=logging.INFO) model = SparseEncoder( \"jhu-clsp/ettin-encoder-150m\" , model_card_data=SparseEncoderModelCardData( language= \"en\" , license= \"apache-2.0\" , ) ) full_dataset = load_dataset( \"sentence-transformers/natural-questions\" , split= \"train\" ).select( range ( 100_000 )) dataset_dict = full_dataset.train_test_split(test_size= 1_000 , seed= 12 ) train_dataset = dataset_dict[ \"train\" ] eval_dataset = dataset_dict[ \"test\" ] loss = SpladeLoss( model=model, loss=SparseMultipleNegativesRankingLoss(model=model), query_regularizer_weight= 5e-5 , document_regularizer_weight= 3e-5 , ) run_name = \"splade-distilbert-base-uncased-nq\" args = SparseEncoderTrainingArguments( output_dir= f\"models/ {run_name} \" , num_train_epochs= 1 , per_device_train_batch_size= 16 , per_device_eval_batch_size= 16 , learning_rate= 2e-5 , warmup_ratio= 0.1 , fp16= True , bf16= False , batch_sampler=BatchSamplers.NO_DUPLICATES, eval_strategy= \"steps\" , eval_steps= 1000 , save_strategy= \"steps\" , save_steps= 1000 , save_total_limit= 2 , logging_steps= 200 , run_name=run_name, ) dev_evaluator = SparseNanoBEIREvaluator(dataset_names=[ \"msmarco\" , \"nfcorpus\" , \"nq\" ], batch_size= 16 ) trainer = SparseEncoderTrainer( model=model, args=args, train_dataset=train_dataset, eval_dataset=eval_dataset, loss=loss, evaluator=dev_evaluator, ) trainer.train() dev_evaluator(model) model.save_pretrained( f\"models/ {run_name} /final\" ) model.push_to_hub(run_name)\n\nClick to see how to finetune this into a reranker model using Sentence Transformers import logging import traceback import torch from datasets import load_dataset from sentence_transformers import SentenceTransformer from sentence_transformers.cross_encoder import ( CrossEncoder, CrossEncoderModelCardData, CrossEncoderTrainer, CrossEncoderTrainingArguments, ) from sentence_transformers.cross_encoder.evaluation import ( CrossEncoderNanoBEIREvaluator, CrossEncoderRerankingEvaluator, ) from sentence_transformers.cross_encoder.losses import BinaryCrossEntropyLoss from sentence_transformers.evaluation import SequentialEvaluator from sentence_transformers.util import mine_hard_negatives logging.basicConfig( format = \"%(asctime)s - %(message)s\" , datefmt= \"%Y-%m-%d %H:%M:%S\" , level=logging.INFO) def main (): model_name = \"jhu-clsp/ettin-encoder-150m\" train_batch_size = 64 num_epochs = 1 num_hard_negatives = 5 model = CrossEncoder( model_name, model_card_data=CrossEncoderModelCardData( language= \"en\" , license= \"apache-2.0\" , ), ) print ( \"Model max length:\" , model.max_length) print ( \"Model num labels:\" , model.num_labels) logging.info( \"Read the gooaq training dataset\" ) full_dataset = load_dataset( \"sentence-transformers/gooaq\" , split= \"train\" ).select( range ( 100_000 )) dataset_dict = full_dataset.train_test_split(test_size= 1_000 , seed= 12 ) train_dataset = dataset_dict[ \"train\" ] eval_dataset = dataset_dict[ \"test\" ] logging.info(train_dataset) logging.info(eval_dataset) embedding_model = SentenceTransformer( \"sentence-transformers/static-retrieval-mrl-en-v1\" , device= \"cpu\" ) hard_train_dataset = mine_hard_negatives( train_dataset, embedding_model, num_negatives=num_hard_negatives, margin= 0 , range_min= 0 , range_max= 100 , sampling_strategy= \"top\" , batch_size= 4096 , output_format= \"labeled-pair\" , use_faiss= True , ) logging.info(hard_train_dataset) loss = BinaryCrossEntropyLoss(model=model, pos_weight=torch.tensor(num_hard_negatives)) nano_beir_evaluator = CrossEncoderNanoBEIREvaluator( dataset_names=[ \"msmarco\" , \"nfcorpus\" , \"nq\" ], batch_size=train_batch_size, ) hard_eval_dataset = mine_hard_negatives( eval_dataset, embedding_model, corpus=full_dataset[ \"answer\" ], num_negatives= 30 , batch_size= 4096 , include_positives= True , output_format= \"n-tuple\" , use_faiss= True , ) logging.info(hard_eval_dataset) reranking_evaluator = CrossEncoderRerankingEvaluator( samples=[ { \"query\" : sample[ \"question\" ], \"positive\" : [sample[ \"answer\" ]], \"documents\" : [sample[column_name] for column_name in hard_eval_dataset.column_names[ 2 :]], } for sample in hard_eval_dataset ], batch_size=train_batch_size, name= \"gooaq-dev\" , always_rerank_positives= False , ) evaluator = SequentialEvaluator([reranking_evaluator, nano_beir_evaluator]) evaluator(model) short_model_name = model_name if \"/\" not in model_name else model_name.split( \"/\" )[- 1 ] run_name = f\"reranker- {short_model_name} -gooaq-bce\" args = CrossEncoderTrainingArguments( output_dir= f\"models/ {run_name} \" , num_train_epochs=num_epochs, per_device_train_batch_size=train_batch_size, per_device_eval_batch_size=train_batch_size, learning_rate= 2e-5 , warmup_ratio= 0.1 , fp16= False , bf16= True , dataloader_num_workers= 4 , load_best_model_at_end= True , metric_for_best_model= \"eval_gooaq-dev_ndcg@10\" , eval_strategy= \"steps\" , eval_steps= 1000 , save_strategy= \"steps\" , save_steps= 1000 , save_total_limit= 2 , logging_steps= 200 , logging_first_step= True , run_name=run_name, seed= 12 , ) trainer = CrossEncoderTrainer( model=model, args=args, train_dataset=hard_train_dataset, loss=loss, evaluator=evaluator, ) trainer.train() evaluator(model) final_output_dir = f\"models/ {run_name} /final\" model.save_pretrained(final_output_dir) try : model.push_to_hub(run_name) except Exception: logging.error( f\"Error uploading model to the Hugging Face Hub:\n\n{traceback.format_exc()} To upload it manually, you can run \" f\"`huggingface-cli login`, followed by loading the model using `model = CrossEncoder( {final_output_dir!r} )` \" f\"and saving it using `model.push_to_hub(' {run_name} ')`.\" ) if __name__ == \"__main__\" : main()\n\nDecoders\n\nClick to expand decoder training code Full training python trl/scripts/sft.py \\ --model_name_or_path jhu-clsp/ettin-decoder-17m \\ --dataset_name trl-lib/Capybara \\ --learning_rate 2.0e-5 \\ --num_train_epochs 1 \\ --packing \\ --per_device_train_batch_size 2 \\ --gradient_accumulation_steps 8 \\ --gradient_checkpointing \\ --eos_token '<|im_end|>' \\ --eval_strategy steps \\ --eval_steps 100 \\ --output_dir ettin-decoder-17m \\ --push_to_hub LoRA python trl/scripts/sft.py \\ --model_name_or_path jhu-clsp/ettin-decoder-17m \\ --dataset_name trl-lib/Capybara \\ --learning_rate 2.0e-4 \\ --num_train_epochs 1 \\ --packing \\ --per_device_train_batch_size 2 \\ --gradient_accumulation_steps 8 \\ --gradient_checkpointing \\ --eos_token '<|im_end|>' \\ --eval_strategy steps \\ --eval_steps 100 \\ --use_peft \\ --lora_r 32 \\ --lora_alpha 16 \\ --output_dir ettin-decoder-17m \\ --push_to_hub with sft.py : import argparse from datasets import load_dataset from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer from transformers.models.auto.modeling_auto import MODEL_FOR_IMAGE_TEXT_TO_TEXT_MAPPING_NAMES from trl import ( ModelConfig, ScriptArguments, SFTConfig, SFTTrainer, TrlParser, clone_chat_template, get_kbit_device_map, get_peft_config, get_quantization_config, ) def main ( script_args, training_args, model_args ): quantization_config = get_quantization_config(model_args) model_kwargs = dict ( revision=model_args.model_revision, trust_remote_code=model_args.trust_remote_code, attn_implementation=model_args.attn_implementation, torch_dtype=model_args.torch_dtype, use_cache= False if training_args.gradient_checkpointing else True , device_map=get_kbit_device_map() if quantization_config is not None else None , quantization_config=quantization_config, ) config = AutoConfig.from_pretrained(model_args.model_name_or_path) valid_image_text_architectures = MODEL_FOR_IMAGE_TEXT_TO_TEXT_MAPPING_NAMES.values() if config.architectures and any (arch in valid_image_text_architectures for arch in config.architectures): from transformers import AutoModelForImageTextToText model_kwargs.pop( \"use_cache\" , None ) model = AutoModelForImageTextToText.from_pretrained(model_args.model_name_or_path, **model_kwargs) else : model = AutoModelForCausalLM.from_pretrained(model_args.model_name_or_path, **model_kwargs) tokenizer = AutoTokenizer.from_pretrained( model_args.model_name_or_path, trust_remote_code=model_args.trust_remote_code, use_fast= True ) if tokenizer.chat_template is None : model, tokenizer = clone_chat_template(model, tokenizer, \"Qwen/Qwen3-0.6B\" ) dataset = load_dataset(script_args.dataset_name, name=script_args.dataset_config) trainer = SFTTrainer( model=model, args=training_args, train_dataset=dataset[script_args.dataset_train_split], eval_dataset=dataset[script_args.dataset_test_split] if training_args.eval_strategy != \"no\" else None , processing_class=tokenizer, peft_config=get_peft_config(model_args), ) trainer.train() trainer.save_model(training_args.output_dir) if training_args.push_to_hub: trainer.push_to_hub(dataset_name=script_args.dataset_name) def make_parser ( subparsers: argparse._SubParsersAction = None ): dataclass_types = (ScriptArguments, SFTConfig, ModelConfig) if subparsers is not None : parser = subparsers.add_parser( \"sft\" , help = \"Run the SFT training script\" , dataclass_types=dataclass_types) else : parser = TrlParser(dataclass_types) return parser if __name__ == \"__main__\" : parser = make_parser() script_args, training_args, model_args, _ = parser.parse_args_and_config(return_remaining_strings= True ) main(script_args, training_args, model_args)\n\nModel Family and Links\n\nThe complete Ettin suite includes models at six different scales (for both encoders and decoders):\n\nStandard Models:\n\nResearch Resources:",
    "link": "https://huggingface.co/blog/ettin",
    "Summary": "Ettin Suite: SoTA Paired Encoders and DecodersPublished July 16, 2025 Update on GitHubWhat would happen if you took the ModernBERT recipe and applied it to a decoder-only model?\nThis blog post introduces Ettin, the first suite of SoTA paired encoder-only and decoder-only models (17M-1B params) trained with identical data (2T tokens), architecture, and training recipes.\nModern Architecture ComponentsOur encoder models gain all the benefits of ModernBERT's speed, allowing them to be significantly faster than the previous generations of encoders.\nCross-Objective Training Falls ShortDue to the lack of new encoder models, works like LLM2Vec have proposed to continue pre-training decoders with MLM.\npredictions = predict_masked_token(masked_text) print ( f\"Predictions: {predictions} \" )For classification and retrieval tasks, use encoder models: You may want to use a fine-tuned version for these tasks as well.",
    "Keywords": [
      "suite",
      "import",
      "decoders",
      "models",
      "training",
      "decoder",
      "ettin",
      "encoder",
      "sota",
      "paired",
      "train",
      "run_name",
      "tokens",
      "encoders",
      "true",
      "model"
    ]
  },
  {
    "Title": "Five Big Improvements to Gradio MCP Servers",
    "Authors": [],
    "Publish Date": null,
    "Text": "Five Big Improvements to Gradio MCP Servers\n\nPublished July 17, 2025 Update on GitHub\n\nGradio is an open-source Python package for creating AI-powered web applications. Gradio is compliant with the MCP server protocol and powers thousands of MCP servers hosted on Hugging Face Spaces . The Gradio team ison Gradio and Spaces being the best way to build and host AI-powered MCP servers.\n\nTo that end, here are some of the big improvements we've added to Gradio MCP servers as of version 5.38.0.\n\nSeamless Local File Support\n\nIf you've tried to use a remote Gradio MCP server that takes a file as input (image, video, audio), you've probably encountered this error:\n\nThis happens because the Gradio server is hosted on a different machine, meaning any input files must be accessible via a public URL so they can be downloaded remotely.\n\nWhile many ways exist to host files online, they all add a manual step to your workflow. In the age of LLM agents, shouldn't we expect them to handle this for you?\n\nGradio now includes a \"File Upload\" MCP server that agents can use to upload files directly to your Gradio application. If any tools in your Gradio MCP server require file inputs, the connection documentation will now show you how to start the \"File Upload\" MCP server:\n\nLearn more about using this server (and important security considerations) in the Gradio Guides.\n\nReal-time Progress Notifications\n\nDepending on the AI task, getting results can take some time. Now, Gradio streams progress notifications to your MCP client, allowing you to monitor the status in real-time!\n\nAs an MCP developer, it's highly recommended to implement your MCP tools to emit these progress statuses. Our guide shows you how.\n\nTransform OpenAPI Specs to MCP in One Line\n\nIf you want to integrate an existing backend API into an LLM, you have to manually map API endpoints to MCP tools. This can be a time-consuming and error prone chore. With this release, Gradio can automate the entire process for you! With a single line of code, you can integrate your business backend into any MCP-compatible LLM.\n\nOpenAPI is a widely adopted standard for describing RESTful APIs in a machine-readable format, typically as a JSON file. Gradio now features the gr.load_openapi function, which creates a Gradio application directly from an OpenAPI schema. You can then launch the app with mcp_server=True to automatically create an MCP server for your API!\n\nimport gradio as gr demo = gr.load_openapi( openapi_spec= \"https://petstore3.swagger.io/api/v3/openapi.json\" , base_url= \"https://petstore3.swagger.io/api/v3\" , paths=[ \"/pet.*\" ], methods=[ \"get\" , \"post\" ], ) demo.launch(mcp_server= True )\n\nFind more details in the Gradio Guides.\n\nImprovements to Authentication\n\nA common pattern in MCP server development is to use authentication headers to call services on behalf of your users. As an MCP server developer, you want to clearly communicate to your users which credentials they need to provide for proper server usage.\n\nTo make this possible, you can now type your MCP server arguments as gr.Header . Gradio will automatically extract that header from the incoming request (if it exists) and pass it to your function. The benefit of using gr.Header is that the MCP connection docs will automatically display the headers you need to supply when connecting to the server!\n\nIn the example below, the X-API-Token header is extracted from the incoming request and passed in as the x_api_token argument to make_api_request_on_behalf_of_user .\n\nimport gradio as gr def make_api_request_on_behalf_of_user ( prompt: str , x_api_token: gr.Header ): \"\"\"Make a request to everyone's favorite API. Args: prompt: The prompt to send to the API. Returns: The response from the API. Raises: AssertionError: If the API token is not valid. \"\"\" return \"Hello from the API\" if not x_api_token else \"Hello from the API with token!\" demo = gr.Interface( make_api_request_on_behalf_of_user, [ gr.Textbox(label= \"Prompt\" ), ], gr.Textbox(label= \"Response\" ), ) demo.launch(mcp_server= True )\n\nYou can read more about this in the Gradio Guides.\n\nModifying Tool Descriptions\n\nGradio automatically generates tool descriptions from your function names and docstrings. Now you can customize the tool description even further with the api_description parameter. In this example, the tool description will read \"Apply a sepia filter to any image.\"\n\nimport gradio as gr import numpy as np def sepia ( input_img ): \"\"\" Args: input_img (np.array): The input image to apply the sepia filter to. Returns: The sepia filtered image. \"\"\" sepia_filter = np.array([ [ 0.393 , 0.769 , 0.189 ], [ 0.349 , 0.686 , 0.168 ], [ 0.272 , 0.534 , 0.131 ] ]) sepia_img = input_img.dot(sepia_filter.T) sepia_img /= sepia_img. max () return sepia_img gr.Interface(sepia, \"image\" , \"image\" , api_description= \"Apply a sepia filter to any image.\" )\\ .launch(mcp_server= True )\n\nRead more in the guide.\n\nConclusion\n\nWant us to add a new MCP-related feature to Gradio? Let us know in the comments of the blog or on GitHub. Also if you've built a cool MCP server or Gradio app let us know in the comments and we'll amplify it!",
    "link": "https://huggingface.co/blog/gradio-mcp-updates",
    "Summary": "Five Big Improvements to Gradio MCP ServersPublished July 17, 2025 Update on GitHubGradio is an open-source Python package for creating AI-powered web applications.\nGradio is compliant with the MCP server protocol and powers thousands of MCP servers hosted on Hugging Face Spaces .\nTo that end, here are some of the big improvements we've added to Gradio MCP servers as of version 5.38.0.\nGradio now includes a \"File Upload\" MCP server that agents can use to upload files directly to your Gradio application.\nsepia_filter = np.array([ [ 0.393 , 0.769 , 0.189 ], [ 0.349 , 0.686 , 0.168 ], [ 0.272 , 0.534 , 0.131 ] ]) sepia_img = input_img.dot(sepia_filter.T) sepia_img /= sepia_img.",
    "Keywords": [
      "servers",
      "gradio",
      "sepia",
      "image",
      "prompt",
      "big",
      "improvements",
      "api",
      "file",
      "server",
      "tool",
      "mcp",
      "sepia_img"
    ]
  },
  {
    "Title": "Migrating the Hub from Git LFS to Xet",
    "Authors": [],
    "Publish Date": null,
    "Text": "Migrating the Hub from Git LFS to Xet\n\nPublished July 15, 2025 Update on GitHub\n\nIn January of this year, Hugging Face's Xet Team deployed a new storage backend, and shortly after shifted ~6% of Hub downloads through the infrastructure . This represented a significant milestone, but it was just the beginning. In 6 months, 500,000 repositories holding 20 PB joined the move to Xet as the Hub outgrows Git LFS and transitions to a storage system that scales with the workloads of AI builders.\n\nToday, more than 1 million people on the Hub are using Xet. In May, it became the default on the Hub for new users and organizations. With only a few dozen GitHub issues, forum threads, and Discord messages, this is perhaps the quietest migration of this magnitude.\n\nHow? For one, the team came prepared with years of experience building and supporting the content addressed store (CAS) and Rust client that provide the system's foundation. Without these pieces, Git LFS may still be the future on the Hub. However, the unsung heroes of this migration are:\n\nAn integral piece of infrastructure known internally as the Git LFS Bridge Background content migrations that run around the clock\n\nTogether, these components have allowed us to aggressively migrate PBs in the span of days without worrying about the impact to the Hub or the community. They're giving us the peace of mind to move even faster in the coming weeks and months (skip to the end \ud83d\udc47 to see what's coming).\n\nBridges and backward compatibility\n\nIn the early days of planning the migration to Xet, we made a few key design decisions:\n\nThere would be no \"hard cut-over\" from Git LFS to Xet\n\nA Xet-enabled repository should be able to contain both Xet and LFS files\n\nRepository migrations from LFS to Xet don't require \"locks\"; that is, they can run in the background without disrupting downloads or uploads\n\nDriven by our commitment to the community, these seemingly straightforward decisions had significant implications. Most importantly, we did not believe users and teams should have to immediately alter their workflow or download a new client to interact with Xet-enabled repositories.\n\nIf you have a Xet-aware client (e.g., hf-xet , the Xet integration with huggingface_hub ), uploads and downloads pass through the entire Xet stack. The client either breaks up files into chunks using content defined chunking while uploading, or requests file reconstruction information when downloading. On upload, chunks are passed to CAS and stored in S3. During downloads, CAS provides the chunk ranges the client needs to request from S3 to reconstruct the file locally.\n\nFor older versions of huggingface_hub or huggingface.js, which do not support chunk-based file transfers, you can still download and upload to Xet repos, but these bytes take a different route. When a Xet-backed file is requested from the Hub along the resolve endpoint, the Git LFS Bridge constructs and returns a single presigned URL, mimicking the LFS protocol. The Bridge then does the work of reconstructing the file from the content held in S3 and returns it to the requester.\n\nGreatly simplified view of the Git LFS Bridge - in reality this path includes a few more API calls and components like the CDN fronting the Bridge, DynamoDB for file metadata, and S3 itself.\n\nTo see this in action, right click on the image above and open it in a new tab. The URL redirects from https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/migrating-the-hub-to-xet/bridge.png to one that begins with https://cas-bridge.xethub.hf.co/xet-bridge-us/... . You can also use curl -vL on the same URL to see the redirects in your terminal.\n\nMeanwhile, when a non-Xet-aware client uploads a file, it is sent first to LFS storage then migrated to Xet. This \u201cbackground migration process,\u201d only briefly mentioned in our docs, powers both the migrations to Xet and upload backward compatibility. It is behind the migration of well over a dozen PBs of models and datasets and is keeping 500,000 repos in sync with Xet storage all without missing a beat.\n\nEvery time a file needs to be migrated from LFS to Xet, a webhook is triggered, pushing the event to a distributed queue where it is processed by an orchestrator. The orchestrator:\n\nEnables Xet on the repo if the event calls for it\n\nFetches a listing of LFS revisions for every LFS file in the repo\n\nBatches the files into jobs based on size or number of files; either 1000 files or 500MB, whichever comes first\n\nPlaces the jobs on another queue for migration worker pods\n\nThese migration workers then pick up the jobs and each pod:\n\nDownloads the LFS files listed in the batch\n\nUploads the LFS files to the Xet content addressed store using xet-core\n\nMigration flow triggered by a webhook event; starting at the orchestrator for brevity.\n\nScaling migrations\n\nIn April, we tested this system's limits by reaching out to bartowski and asking if they wanted to test out Xet. With nearly 500 TB across 2,000 repos, bartowski's migration uncovered a few weak links:\n\nTemporary shard files for global dedupe were first written to /tmp and then moved into the shard cache. On our worker pods, however, /tmp and the Xet cache sat on different mount points. The move failed and the shard files were never removed. Eventually the disk filled, triggering a wave of No space left on device errors.\n\nand then moved into the shard cache. On our worker pods, however, and the Xet cache sat on different mount points. The move failed and the shard files were never removed. Eventually the disk filled, triggering a wave of errors. After supporting the launch of Llama 4, we'd scaled CAS for bursty downloads, but the migration workers flipped the script as hundreds of multi-gigabyte uploads pushed CAS beyond its resources\n\nOn paper, the migration workers were capable of significantly more throughput than what was reported; profiling the pods revealed network and EBS I/O bottlenecks\n\nFixing this three-headed monster meant touching every layer - patching xet-core, resizing CAS, and beefing up the worker node specs. Fortunately, bartowski was game to work with us while every repo made its way to Xet. These same lessons powered the moves of the biggest storage users on the Hub like RichardErkhov (1.7PB and 25,000 repos) and mradermacher (6.1PB and 42,000 repos \ud83e\udd2f).\n\nCAS throughput, meanwhile, has grown by an order of magnitude between the first and latest large-scale migrations:\n\nBartowski migration: CAS sustained ~35 Gb/s, with ~5 Gb/s coming from regular Hub traffic.\n\nCAS sustained ~35 Gb/s, with ~5 Gb/s coming from regular Hub traffic. mradermacher and RichardErkhov migrations: CAS peaked around ~300 Gb/s, while still serving ~40 Gb/s of everyday load.\n\nCAS throughput; each spike corresponds to a significant migration with the baseline throughput steadily increasing to just shy of 100 Gb/s as of July 2025\n\nZero friction, faster transfers\n\nWhen we began replacing LFS, we had two goals in mind:\n\nDo no harm Drive the most impact as fast as possible\n\nDesigning with our initial constraints and these goals allowed us to:\n\nIntroduce and harden hf-xet before including it in huggingface_hub as a required dependency\n\nbefore including it in as a required dependency Support the community uploading to and downloading from Xet-enabled repos through whatever means they use today while our infrastructure handles the rest\n\nLearn invaluable lessons - from scale to how our client operated on distributed file systems - from incrementally migrating the Hub to Xet\n\nInstead of waiting for all upload paths to become Xet-aware, forcing a hard cut-over, or pushing the community to adopt a specific workflow, we could begin migrating the Hub to Xet immediately with minimal user impact. In short, let teams keep their workflows and organically transition to Xet with infrastructure supporting the long-term goal of a unified storage system.\n\nXet for everyone\n\nIn January and February, we onboarded power users to provide feedback and pressure-test the infrastructure. To get community feedback, we launched a waitlist to preview Xet-enabled repositories. Soon after, Xet became the default for new users on the Hub.\n\nWe now support some of the largest creators on the Hub (Meta Llama, Google, OpenAI, and Qwen) while the community keeps working uninterrupted.\n\nWhat's next?\n\nStarting this month, we're bringing Xet to everyone. Watch for an email providing access to Xet and once you have it, update to the latest huggingface_hub ( pip install -U huggingface_hub ) to unlock faster transfers right away. This will also mean:\n\nAll of your existing repositories will migrate from LFS to Xet\n\nAll newly created repos will be Xet-enabled by default\n\nIf you upload or download from the Hub using your browser or use Git, that's fine. Chunk-based support for both is coming soon. In the meantime use whichever workflow you already have; no restrictions.\n\nNext up: open-sourcing the Xet protocol and the entire infrastructure stack. The future of storing and moving bytes that scale to AI workloads is on the Hub, and we're aiming to bring it to everyone.\n\nIf you have any questions, drop us a line in the comments \ud83d\udc47, open a discussion on the Xet team page.",
    "link": "https://huggingface.co/blog/migrating-the-hub-to-xet",
    "Summary": "Migrating the Hub from Git LFS to XetPublished July 15, 2025 Update on GitHubIn January of this year, Hugging Face's Xet Team deployed a new storage backend, and shortly after shifted ~6% of Hub downloads through the infrastructure .\nIn 6 months, 500,000 repositories holding 20 PB joined the move to Xet as the Hub outgrows Git LFS and transitions to a storage system that scales with the workloads of AI builders.\nWithout these pieces, Git LFS may still be the future on the Hub.\nWhen a Xet-backed file is requested from the Hub along the resolve endpoint, the Git LFS Bridge constructs and returns a single presigned URL, mimicking the LFS protocol.\nMeanwhile, when a non-Xet-aware client uploads a file, it is sent first to LFS storage then migrated to Xet.",
    "Keywords": [
      "files",
      "hub",
      "file",
      "repos",
      "xet",
      "migration",
      "cas",
      "lfs",
      "git",
      "migrating",
      "gbs"
    ]
  },
  {
    "Title": "Asynchronous Robot Inference: Decoupling Action Prediction and Execution",
    "Authors": [],
    "Publish Date": null,
    "Text": "Asynchronous Robot Inference: Decoupling Action Prediction and Execution\n\nPublished July 10, 2025 Update on GitHub\n\nTable of Contents\n\nGetting started\n\nRobotic policies are increasingly bulky, and predict chunks of future actions rather than a single next action. This results in the robot being idle while awaiting new actions to perform, introducing noticeable lags at execution, and lacking responsiveness. Asynchronous inference tightens the control loop, removing lags at runtime and resulting in more adaptive control by decoupling action prediction from action execution. In this blog post, we cover the basics behind async inference, and how it can be used to improve the performance of robotic policies in the real-world.\n\nGet started with async inference by following our tutorial.\n\nSequential inference (first) versus async inference (second). Allowing for replanning and a tighter control loop, async inference results in (1) attempts at recovery, and (2) a ~2x speedup in task completion. Sequential inference keeps acting out the current action chunk even after failure to grasp the object, while async inference can replan and act the new action chunk. Both setups use the same policy!\n\nAsync inference: a deep dive\n\nWith async inference, we decouple action execution from action prediction. This is particularly relevant considering the tendency of currently popular models like [ACT], [OpenVLA], [PI0], and [SmolVLA] to be outputting chunks of actions a t : t + H a_{t:t+H} at:t+H\u200b rather than single actions a t a_t at\u200b given an observation o t o_t ot\u200b. Convince yourself of this by running all these models using LeRobot.\n\nUsing chunks sequentially results in (1) lags at runtime, impacting task execution time and (2) lack of responsiveness, due to acting widely open-loop. Asynchronous inference mitigates both these limitations by decoupling action prediction from action execution. We introduced asynchronous inference in SmolVLA, and found it to result in a ~2x speed-up in task completion time with comparable task success rate.\n\nIn particular, we design a 2-component system where policy inference and action execution are performed in two different processes, possibly on two different machines connected through the network:\n\nA PolicyServer , hosted on accelerated hardware and capable of running inference using more computational resources than the ones allocated on a real-world robot.\n\n, hosted on accelerated hardware and capable of running inference using more computational resources than the ones allocated on a real-world robot. A RobotClient enqueues the received actions and executes them while the next chunk is being computed.\n\nCommunication between PolicyServer and RobotClient relies on gRPC, which guarantees ~5\u00d7 faster performance than a comparable REST API. The result of all of this is a robot that never waits for inference.\n\nAsynchronous inference, highlighting: (1) The client sending the first observation for inference, receiving the first chunk shortly after; (2) The client sending another observation for processing while it has not yet exhausted the current chunk; (3) The client receiving an updated action chunk, which it aggregates with the remainders of the one it was previously executing.\n\n1. Why sequential inference falls short\n\nSuppose a policy \u03c0 \\pi \u03c0 maps the current observation o t o_t ot\u200b to a sequence of H H H future actions. Formally, \u03c0 : O \u21a6 A , A t = ( a t , a t + 1 , \u2026 a t + H ) = \u03c0 ( o t ) \\pi : \\mathcal{O} \\;\\mapsto\\; \\mathcal{A}, \\mathbf{A}_t = \\begin{pmatrix} a_{t}, & a_{t+1}, & \\dots & a_{t+H} \\end{pmatrix} = \\pi(o_t) \u03c0:O\u21a6A,At\u200b=(at\u200b,\u200bat+1\u200b,\u200b\u2026\u200bat+H\u200b\u200b)=\u03c0(ot\u200b).\n\nA traditional control loop would therefore consist of the following steps:\n\nCapture o t o_t o t \u200b . Run \u03c0 ( o t ) \\pi(o_t) \u03c0 ( o t \u200b ) to obtain A t = \u03c0 ( o t ) \\mathbf{A}_t = \\pi(o_t) A t \u200b = \u03c0 ( o t \u200b ) . Enqueue A t \\mathbf{A_t} A t \u200b and start acting popping actions from the queue. If the queue is empty, wait for A t + H \\mathbf{A}_{t+H} A t + H \u200b , otherwise repeat step 3.\n\nDuring step 2 the robot is idle. The latency grows with the model size (and models tend to be increasingly bulky over time), and can quickly dominate interaction time (which is typically around 1/ fps ), as shown in the video below (coming from our Discord community \ud83e\udd17):\n\nThis directly results in (1) reduced performance in terms of task completion time---the robot needs to be waiting for the next action chunk to be computed---and (2) reduced responsiveness, due to (2.1) acting widely open-loop while actions are available and (2.2) complete idleness while waiting for the next action chunk.\n\n(Left)Sequential inference with highlighted idle periods. (Right)Time to select an action showing spikes when inference is triggered due to local queue exhaustion (inference latency is around ~100ms---~3 frames at 30fps---using an ACT model on a 2021 MacBook Pro).\n\n2. Asynchronous inference, in a nutshell\n\nOur system removes the idle period by overlapping computation and execution:\n\nRobotClient streams the latest observation to PolicyServer . While the server performs inference, the client executes the current queue of actions. New actions arrive, are merged into the queue, and the loop continues.\n\nThe key idea is that the robot already knows what to do for the next few timesteps, so it can keep moving while fresh actions are being computed on the server.\n\nAsynchronous inference overlaps in time the execution of the current action chunk with the computation of the next one, by decoupling these two processes, possibly running them on entirely distinct machines connected through the network.\n\nThis results in a tighter control loop, and a robot that never waits for inference. In turn, this results in ~2x speedup in task completion time with comparable task success rate, and more adaptive control coming from a tighter loop (see video below).\n\n3. System Architecture\n\nComponent Role Technology RobotClient Runs on-board, streams observations, maintains an action queue, executes actions Python, gRPC PolicyServer Hosts the policy, performs batched inference, sends action chunks back Python, gRPC, possibly accelerated hardware (GPU/TPU)\n\nBecause gRPC is HTTP/2-based and uses protocol buffers, it achieves low-latency binary messaging and bidirectional streams out of the box, which in turn helps us maintain a tighter control loop and sub-100ms round-trip latency (on our local network, and hosting SmolVLA on a NVIDIA RTX 4090).\n\nThe RobotClient runs on-board, and streams observations to the PolicyServer through gRPC. The PolicyServer prepares the observations received for inference, and sends back to the RobotClient an action chunk.\n\nRobot Client\n\nFrom the client's perspective, observations are streamed to the server according to the local queue status. Incoming chunks are aggregated on overlapping portions with the currently available action queue.\n\nThe RobotClient maintains a local action queue and follows a simple yet effective strategy: send a new observation when the queue length drops below a configurable threshold (\\(g\\) in the SmolVLA paper, chunk_size_threshold in the code). This threshold value, expressed as a fraction of the maximum chunk size, acts as a trigger condition that balances computational load with responsiveness.\n\nThe client streams observations to the server, according to the local queue status.\n\nFrom the client's perspective, the process unfolds as follows:\n\nQueue monitoring: The client continuously monitors its action queue length against a chunk size threshold parameter. When the queue drops below this threshold, it signals that a new observation should be sent for processing. Observation streaming: Once the threshold condition is met, the client captures the current observation and streams it to the PolicyServer via gRPC. Crucially, observations are streamed rather than being sent via a unary RPC because they typically exceed the maximum message size of 4MB (multiple camera captures at high resolution result in this). Action chunk aggregation: When a new action chunk arrives from the server, the client merges it with any remaining actions in the current queue over the overlapping portion. This is where custom aggregators come into play, handling overlapping sections between the current and incoming chunks differently. As of now, we support flexibly aggregation between the chunks via the specification of a custom aggregate_fn(chunk1: torch.Tensor, chunk2: torch.Tensor) -> torch.Tensor function, which is called for each overlapping timestep and can be user-provided. The overlapping portions (shown in light blue in the diagram) require careful handling. We can design different aggregation strategies: Replace : Simply replace overlapping actions with the newer predictions\n\n: Simply replace overlapping actions with the newer predictions Weighted blend: Combine overlapping actions using temporal weights (closer actions get higher weight)\n\nThis system is highly configurable, as the chunk size threshold can be tuned based on network latency, model inference time, and desired responsiveness. A lower threshold means more frequent updates (and higher computational cost), while a higher threshold reduces communication overhead at the expense of potential queue starvation. Lastly, we typically receive actions from PolicyServer in a thread, and perform them in another one. This keeps the client listening for incoming chunks in a separate thread, without blocking execution and always consuming the current chunk until a new one becomes fully available.\n\nPolicy Server\n\nUpon receiving observations from the RobotClient , the PolicyServer receives observations from the RobotClient , and performs the necessary observation cleaning to make received observations ready for inference. This process is illustrated in the image below:\n\nThe observation cleaning pipeline running on the server, highlighting the three main steps related to (1) Keys matching (2) Preprocessing and (3) Preparation for inference.\n\nOnce the observation has been prepared, it is compared with the last observation used for inference. This avoids collapsing into a loop whereby very similar observations are processed, thus triggering unnecessary inference and similar actions being executed (which in turn, result in very similar observations being processed again). We compare observations in terms of their joint-space similarity, which provides us an approximate and quick way of measuring changes in the robot. Clearly, this metric is not adaptive to dynamic changes in the environment (an object changing its position, or disturbances being applied), but we found it to be a good trade-off for the majority of the cases, and to be very effective in avoiding unnecessary inference and state collapse. Critically, the RobotClient retains control over whether a given observation must be processed, to avoid deadlocks. Observations sent by the client and tagged with must_go=True are processed regardless of the similarity metric.\n\nThe policy workflow, in which incoming observations are compared to the last one used for inference, and processed only if different enough, or `must_go`.\n\nLastly, to ensure the PolicyServer always processes the latest available observation, we block incoming observations until the previous one has been successfully processed. In this, we leverage queues on the PolicyServer to ensure incoming observations are not enqueued until the server is ready to process them (see below).\n\nThe client pings the server every 1/fps seconds, but observations are not enqueued for processing until the previous one has been successfully processed.\n\n4. Analyzing async inference\n\nFor all practical purposes, in async inference there are two time-scales that matter:\n\nEnvironment step environment_dt = 1 / fps \\texttt{environment\\_dt} = 1/\\texttt{fps} environment_dt = 1/ fps , depicting how fast the robot can perform an action.\n\n, depicting how fast the robot can perform an action. Inference latency inference_time \\texttt{inference\\_time} inference_time : forward-pass + network round-trip. We can assume the network round-trip to be negligible with respect to the policy inference time, though this might not be the case for every setup.\n\nImportantly, the ratio c = environment_dt inference_time c = \\frac{\\texttt{environment\\_dt}}{\\texttt{inference\\_time}} c=inference_timeenvironment_dt\u200b results in different behaviours:\n\nc \u226a 1 c \\ll 1 c \u226a 1 : environment evolves faster than inference. In this scenario, the queue empties quickly and we degenerate to sequential control.\n\n: environment evolves faster than inference. In this scenario, the queue empties quickly and we degenerate to sequential control. c \u2265 1 c \\ge 1 c \u2265 1 : server keeps up. The queue is always (nearly) full.\n\nCritically, c c c influences the number of available actions in the queue at any given time. To avoid the aforementioned sequential limit control, one can:\n\nUse more compute for the policy server, hosting the server on a GPU, reducing inference_time \\texttt{inference\\_time} inference_time as a consequence of allocating more computational resources. Sending observations to the server more often, send a new observation when the queue length k k k drops below a fraction g = k / H g = k/H g = k / H of its maximum size. g = 0 g=0 g = 0 reproduces sequential inference (empty queue, wait).\n\nreproduces sequential inference (empty queue, wait). g = 1 g=1 g = 1 sends an observation every timestep (max compute, minimal lag).\n\nExperiments (see plots below) show that g \u2248 0.7 g\\approx0.7 g\u22480.7 offers a good trade-off when observations sent are not filtered out (they are all must-go). We recommend setting g = 0.5 g=0.5 g=0.5 and following our documentation to tune this parameter to your needs.\n\nThe number of available actions in the queue at any given time, as a function of g. Larger values of g result in more frequent updates, and more computational cost. Values of g closer to 0 reproduce sequential inference (empty queue, wait). We found g~0.7 to be a good trade-off in our experiments.\n\n5. Using async in your setup\n\nAsync inference is a simple yet effective way to improve the performance of robotic policies. In our experiments using SmolVLA, async inference results in a ~2x speedup in task completion time with comparable task success rate, and more adaptive control coming from a tighter loop.\n\nTo run your policy using async inference, you just need to follow our tutorial with your own custom parameters (e.g., the policy path or the chunk size threshold). Async inference comes with support for policies supporting action chunking!\n\nConclusions\n\nWe have introduced async inference, a simple yet effective way to improve the performance of robotic policies. In our experiments using SmolVLA, async inference results in a ~2x speedup in task completion time with comparable task success rate, and more adaptive control coming from a tighter loop.",
    "link": "https://huggingface.co/blog/async-robot-inference",
    "Summary": "Asynchronous inference tightens the control loop, removing lags at runtime and resulting in more adaptive control by decoupling action prediction from action execution.\nSequential inference (first) versus async inference (second).\nSequential inference keeps acting out the current action chunk even after failure to grasp the object, while async inference can replan and act the new action chunk.\nAsync inference: a deep diveWith async inference, we decouple action execution from action prediction.\nAsynchronous inference mitigates both these limitations by decoupling action prediction from action execution.",
    "Keywords": [
      "inference",
      "g",
      "asynchronous",
      "queue",
      "control",
      "execution",
      "observations",
      "async",
      "prediction",
      "observation",
      "robot",
      "actions",
      "action",
      "chunk",
      "decoupling"
    ]
  },
  {
    "Title": "ScreenEnv: Deploy your full stack Desktop Agent",
    "Authors": [],
    "Publish Date": null,
    "Text": "ScreenEnv: Deploy your full stack Desktop Agent\n\nPublished July 10, 2025 Update on GitHub\n\nWhat is ScreenEnv?\n\n: ScreenEnv is a powerful Python library that lets you create isolated Ubuntu desktop environments in Docker containers for testing and deploying GUI Agents (aka Computer Use agents). With built-in support for the Model Context Protocol (MCP), it's never been easier to deploy desktop agents that can see, click, and interact with real applications.\n\nImagine you need to automate desktop tasks, test GUI applications, or build an AI agent that can interact with software. This used to require complex VM setups and brittle automation frameworks.\n\nScreenEnv changes this by providing a sandboxed desktop environment that runs in a Docker container. Think of it as a complete virtual desktop session that your code can fully control - not just clicking buttons and typing text, but managing the entire desktop experience including launching applications, organizing windows, handling files, executing terminal commands, and recording the entire session.\n\nWhy ScreenEnv?\n\n\ud83d\udda5\ufe0f Full Desktop Control : Complete mouse and keyboard automation, window management, application launching, file operations, terminal access, and screen recording\n\n: Complete mouse and keyboard automation, window management, application launching, file operations, terminal access, and screen recording \ud83e\udd16 Dual Integration Modes : Support both Model Context Protocol (MCP) for AI systems and direct Sandbox API - adapting to any agent or backend logic\n\n: Support both Model Context Protocol (MCP) for AI systems and direct Sandbox API - adapting to any agent or backend logic \ud83d\udc33 Docker Native: No complex VM setup - just Docker. The environment is isolated, reproducible, and easily deployed anywhere in less than 10 seconds. Support AMD64 and ARM64 architecture.\n\n\ud83c\udfaf One-Line Setup\n\nfrom screenenv import Sandbox sandbox = Sandbox()\n\nTwo Integration Approaches\n\nScreenEnv provides two complementary ways to integrate with your agents and backend systems, giving you flexibility to choose the approach that best fits your architecture:\n\nOption 1: Direct Sandbox API\n\nPerfect for custom agent frameworks, existing backends, or when you need fine-grained control:\n\nfrom screenenv import Sandbox sandbox = Sandbox(headless= False ) sandbox.launch( \"xfce4-terminal\" ) sandbox.write( \"echo 'Custom agent logic'\" ) screenshot = sandbox.screenshot() image = Image. open (BytesIO(screenshot_bytes)) ... sandbox.close()\n\nOption 2: MCP Server Integration\n\nIdeal for AI systems that support the Model Context Protocol:\n\nfrom screenenv import MCPRemoteServer from mcp import ClientSession from mcp.client.streamable_http import streamablehttp_client server = MCPRemoteServer(headless= False ) print ( f\"MCP Server URL: {server.server_url} \" ) async def mcp_session (): async with streamablehttp_client(server.server_url) as streams: async with ClientSession(*streams) as session: await session.initialize() print ( await session.list_tools()) response = await session.call_tool( \"screenshot\" , {}) image_bytes = base64.b64decode(response.content[ 0 ].data) image = Image. open (BytesIO(image_bytes)) server.close()\n\nThis dual approach means ScreenEnv adapts to your existing infrastructure rather than forcing you to change your agent architecture.\n\n\u2728 Create a Desktop Agent with screenenv and smolagents\n\nscreenenv natively supports smolagents , making it easy to build your own custom Desktop Agent for automation. Here\u2019s how to create your own AI-powered Desktop Agent in just a few steps:\n\n1. Choose Your Model\n\nPick the backend VLM you want to power your agent.\n\nimport os from smolagents import OpenAIServerModel model = OpenAIServerModel( model_id= \"gpt-4.1\" , api_key=os.getenv( \"OPENAI_API_KEY\" ), ) from smolagents import HfApiModel model = HfApiModel( model_id= \"Qwen/Qwen2.5-VL-7B-Instruct\" , token=os.getenv( \"HF_TOKEN\" ), provider= \"nebius\" , ) from smolagents import TransformersModel model = TransformersModel( model_id= \"Qwen/Qwen2.5-VL-7B-Instruct\" , device_map= \"auto\" , torch_dtype= \"auto\" , trust_remote_code= True , ) from smolagents import LiteLLMModel model = LiteLLMModel(model_id= \"anthropic/claude-sonnet-4-20250514\" )\n\n2. Define Your Custom Desktop Agent\n\nInherit from DesktopAgentBase and implement the _setup_desktop_tools method to build your own action space!\n\nfrom screenenv import DesktopAgentBase, Sandbox from smolagents import Model, Tool, tool from smolagents.monitoring import LogLevel from typing import List class CustomDesktopAgent ( DesktopAgentBase ): \"\"\"Agent for desktop automation\"\"\" def __init__ ( self, model: Model, data_dir: str , desktop: Sandbox, tools: List [Tool] | None = None , max_steps: int = 200 , verbosity_level: LogLevel = LogLevel.INFO, planning_interval: int | None = None , use_v1_prompt: bool = False , **kwargs, ): super ().__init__( model=model, data_dir=data_dir, desktop=desktop, tools=tools, max_steps=max_steps, verbosity_level=verbosity_level, planning_interval=planning_interval, use_v1_prompt=use_v1_prompt, **kwargs, ) def _setup_desktop_tools ( self ) -> None : \"\"\"Define your custom tools here.\"\"\" def click ( x: int , y: int ) -> str : \"\"\" Clicks at the specified coordinates. Args: x: The x-coordinate of the click y: The y-coordinate of the click \"\"\" self.desktop.left_click(x, y) return f\"Clicked at ( {x} , {y} )\" self.tools[ \"click\" ] = click def write ( text: str ) -> str : \"\"\" Types the specified text at the current cursor position. Args: text: The text to type \"\"\" self.desktop.write(text, delay_in_ms= 10 ) return f\"Typed text: ' {text} '\" self.tools[ \"write\" ] = write def press ( key: str ) -> str : \"\"\" Presses a keyboard key or combination of keys Args: key: The key to press (e.g. \"enter\", \"space\", \"backspace\", etc.) or a multiple keys string to press, for example \"ctrl+a\" or \"ctrl+shift+a\". \"\"\" self.desktop.press(key) return f\"Pressed key: {key} \" self.tools[ \"press\" ] = press def open ( file_or_url: str ) -> str : \"\"\" Directly opens a browser with the specified url or opens a file with the default application. Args: file_or_url: The URL or file to open \"\"\" self.desktop. open (file_or_url) self.logger.log( f\"Opening: {file_or_url} \" ) return f\"Opened: {file_or_url} \" def launch_app ( app_name: str ) -> str : \"\"\" Launches the specified application. Args: app_name: The name of the application to launch \"\"\" self.desktop.launch(app_name) return f\"Launched application: {app_name} \" self.tools[ \"launch_app\" ] = launch_app ...\n\n3. Run the Agent on a Desktop Task\n\nfrom screenenv import Sandbox sandbox = Sandbox(headless= False , resolution=( 1920 , 1080 )) agent = CustomDesktopAgent( model=model, data_dir= \"data\" , desktop=sandbox, ) task = \"Open LibreOffice, write a report of approximately 300 words on the topic \u2018AI Agent Workflow in 2025\u2019, and save the document.\" result = agent.run(task) print ( f\"\ud83d\udcc4 Result: {result} \" ) sandbox.close()\n\nIf you encounter acces denied docker error, you can try to run the agent with sudo -E python -m test.py or add your user to the docker group.\n\n\ud83d\udca1 For a comprehensive implementation, see this CustomDesktopAgent source on GitHub.\n\nGet Started Today\n\npip install screenenv git clone git@github.com:huggingface/screenenv.git cd screenenv python -m examples.desktop_agent\n\nScreenEnv aims to expand beyond Linux to support Android, macOS, and Windows, unlocking true cross-platform GUI automation. This will enable developers and researchers to build agents that generalize across environments with minimal setup.\n\nThese advancements pave the way for creating reproducible, sandboxed environments ideal for benchmarking and evaluation.\n\nRepository: https://github.com/huggingface/screenenv",
    "link": "https://huggingface.co/blog/screenenv",
    "Summary": "ScreenEnv: Deploy your full stack Desktop AgentPublished July 10, 2025 Update on GitHubWhat is ScreenEnv?\n\ud83c\udfaf One-Line Setupfrom screenenv import Sandbox sandbox = Sandbox()Two Integration ApproachesScreenEnv provides two complementary ways to integrate with your agents and backend systems, giving you flexibility to choose the approach that best fits your architecture:Option 1: Direct Sandbox APIPerfect for custom agent frameworks, existing backends, or when you need fine-grained control:from screenenv import Sandbox sandbox = Sandbox(headless= False ) sandbox.launch( \"xfce4-terminal\" ) sandbox.write( \"echo 'Custom agent logic'\" ) screenshot = sandbox.screenshot() image = Image.\n\u2728 Create a Desktop Agent with screenenv and smolagentsscreenenv natively supports smolagents , making it easy to build your own custom Desktop Agent for automation.\nHere\u2019s how to create your own AI-powered Desktop Agent in just a few steps:1.\nRun the Agent on a Desktop Taskfrom screenenv import Sandbox sandbox = Sandbox(headless= False , resolution=( 1920 , 1080 )) agent = CustomDesktopAgent( model=model, data_dir= \"data\" , desktop=sandbox, ) task = \"Open LibreOffice, write a report of approximately 300 words on the topic \u2018AI Agent Workflow in 2025\u2019, and save the document.\"",
    "Keywords": [
      "deploy",
      "import",
      "support",
      "sandbox",
      "model",
      "screenenv",
      "str",
      "def",
      "stack",
      "text",
      "desktop",
      "agent"
    ]
  },
  {
    "Title": "Building the Hugging Face MCP Server",
    "Authors": [],
    "Publish Date": null,
    "Text": "Building the Hugging Face MCP Server\n\nPublished July 10, 2025 Update on GitHub\n\nIntroduction\n\nDesign Choices\n\nRemote Servers Production Deployment\n\nConclusion TL;DR: The Hugging Face Official MCP Server offers unique customization options for AI Assistants accessing the Hub, along with access to thousands of AI applications through one simple URL. We used MCPs \"Streamable HTTP\" transport for deployment, and examine in detail the trade-offs that Server Developers have. We've learned many things about building a useful MCP server in the last month - we'll describe our journey here.\n\nIntroduction\n\nThe Model Context Protocol (MCP) is fulfilling its promise of being the standard to connect AI Assistants to the outside world.\n\nAt Hugging Face, providing access to the Hub via MCP is an obvious choice, and this article shares our experience developing the hf.co/mcp MCP Server.\n\nDesign Choices\n\nThe community uses the Hub for research, development, content creation and more. We wanted to let people customize the server for their own needs, as well as easily access thousands of AI applications available on Spaces. This meant making the MCP Server dynamic by adjusting users' tools on the fly.\n\nThe Hugging Face MCP Settings Page where Users can configure their tools.\n\nWe also wanted to make access simple by avoiding complicated downloads and configuration, so making it remotely accessible via a simple URL was a must.\n\nRemote Servers\n\nWhen building a remote MCP Server, the first decision is deciding how clients will connect to it. MCP offers several transport options, with different trade-offs. TL;DR: our open source code supports all variants, but for production we chose to go with the most modern one. This section goes through the different options in detail.\n\nSince its launch in November 2024, MCP has undergone rapid evolution with 3 protocol revisions in 9 months. This has seen the replacement of the SSE Transport with Streamable HTTP, as well as the introduction and rework of authorization.\n\nThese rapid changes mean support for different MCP Features and revisions in Client applications varies, providing extra challenges for our design choices.\n\nHere is a brief summary of the Transport Options offered by the Model Context Protocol and associated SDKs:\n\nTransport Notes STDIO Typically used when the MCP Server is running on the same computer as the Client. Able to access local resources such as files if needed. HTTP with SSE Used for remote connections over HTTP. Deprecated in the 2025-03-26 version of MCP but still in use. Streamable HTTP A more flexible remote HTTP transport that provides more options for deployment than the outgoing SSE version\n\nBoth STDIO and HTTP with SSE are fully bi-directional by default - meaning that Client and Server maintain an open connection and can send messages to each other at any time.\n\nSSE refers to \"Server Sent Events\" - a way for HTTP Servers to maintain an open connection and send events in response to a request.\n\nUnderstanding Streamable HTTP\n\nMCP Server Developers face a lot of choices when setting up the Streamable HTTP transport.\n\nThere are 3 main communication patterns to choose from:\n\nDirect Response - Simple Request/Response (like standard REST APIs). This is perfect for straightforward, stateless operations like simple searches.\n\n- Simple Request/Response (like standard REST APIs). This is perfect for straightforward, stateless operations like simple searches. Request Scoped Streams - Temporary SSE Streams associated with a single Request. This is useful for sending Progress Updates if the Tool Call takes a long time - such as Video Generation. Additionally the Server may need to request information from the user with an Elicitation, or conduct a Sampling request.\n\n- Temporary SSE Streams associated with a single Request. This is useful for sending Progress Updates if the Tool Call takes a long time - such as Video Generation. Additionally the Server may need to request information from the user with an Elicitation, or conduct a Sampling request. Server Push Streams - Long-lived SSE connection supporting server-initiated messages. This enables Resource, Tool and Prompt List change notifications or ad-hoc Sampling and Elicitations. These connections need extra management such as keep-alive and resumption mechanics on re-connection.\n\nWhen using Request Scoped Streams with the official SDKs, use the sendNotification() and sendRequest() methods provided in the RequestHandlerExtra parameter (TypeScript) or set the related_request_id (Python) to send messages to the correct stream.\n\nAn additional factor to consider is whether or not the MCP Server itself needs to maintain state for each connection. This is decided by the Server when the Client sends its Initialize request:\n\nStateless Stateful Session IDs Not needed Server responds with an mcp-session-id What it means Each request is independent Server maintains client context Scaling Simple horizontal scaling: any instance can handle any request Need session affinity or shared state mechanisms Resumption Not needed May replay messages for broken connections\n\nThe table below summarizes the MCP Features and their supported communication pattern:\n\nMCP Feature Server Push Request Scoped Direct Response Tools, Prompts, Resources Y Y Y Sampling/Elicitation Server Initiated at any time Related to a Client initiated request N Resource Subscriptions Y N N Tool/Prompt List Changes Y N N Tool Progress Notification - Y N\n\nWith Request Scoped streams, Sampling and Elicitation requests need a Stateful connection so that the mcp-session-id can be used for response association.\n\nThe Hugging Face MCP Server is Open Source - and supports STDIO, SSE and Streamable HTTP deployment in both Direct Response and Server Push mode. You can configure keep-alive and last activity timeouts when using Server Push Streams. There's also a built-in observability dashboard that you can use to understand how different Clients manage connections, and handle Tool List change notifications.\n\nThe following picture shows our MCP Server connection dashboard running in \"Server Push\" Streamable HTTP mode:\n\nThe Hugging Face MCP Server Connection Dashboard.\n\nProduction Deployment\n\nFor production, we decided to launch our MCP Server with Streamable HTTP in a Stateless, Direct Response configuration for the following reasons:\n\nStateless For anonymous users we supply a standard set of Tools for using the Hub along with an Image Generator. For authenticated users our state comprises their selected tools and chosen Gradio applications. We also make sure that users ZeroGPU quota is correctly applied for their account. This is managed using the supplied HF_TOKEN or OAuth credentials that we look up on request. None of our existing tools require us to maintain any other state between requests.\n\nYou can use OAuth login by adding ?login to the MCP Server url - e.g. https://huggingface.co/mcp?login . We may make this the default once the claude.ai remote integration supports the latest OAuth spec.\n\nDirect Response provides the lowest deployment resource overhead - and we don't currently have any Tools that require Sampling or Elicitation during execution.\n\nFuture Support At launch, the \"HTTP with SSE\" transport was still the remote default in a lot of MCP Clients. However, we didn't want to invest heavily in managing it due to its imminent deprecation. Fortunately, popular clients had already started making the switch (VSCode and Cursor), and within a week of launch claude.ai also added support. If you need to connect with SSE, feel free to deploy a copy of our Server on a FreeCPU Hugging Face Space.\n\nTool List Change Notifications\n\nIn the future, we would like to support real-time Tool List Changed notifications when users update their settings on the Hub. However, this raises a couple of practical issues:\n\nFirst, users tend to configure their favourite MCP Servers in their Client and leave them enabled. This means that the Client remains connected whilst the application is open. Sending notifications would mean maintaining as many open connections as there were currently active Clients - regardless of active usage - on the chance the user updates their tool configuration.\n\nSecond, most MCP Servers and Clients disconnect after a period of inactivity, resuming when necessary. This inevitably means that immediate push notifications would be missed - as the notification channel will have been closed. In practice, it is far simpler for the Client to refresh the connection and Tool List as needed.\n\nUnless you have reasonable control over the Client/Server pair, using Server Push Streams adds a lot of complexity to a public deployment, when lower-resource solutions for refreshing the Tool List exist.\n\nURL User Experience\n\nJust before launch, @julien-c submitted a PR to include friendly instructions for users visiting hf.co/mcp . This hugely improves the User Experience - the default response is otherwise an unfriendly bit of JSON.\n\nInitially, we found this generated an enormous amount of traffic. After a bit of investigation we found that when returning a web page rather than an HTTP 405 error, VSCode would poll the endpoint multiple times per second!\n\nThe fix suggested by @coyotte508 was to properly detect browsers and only return the page in that circumstance. Thanks also to the VSCode team who rapidly fixed it.\n\nAlthough not specifically stated - returning a page in this manner does seem acceptable within the MCP Specification.\n\nMCP Client Behaviour\n\nThe MCP Protocol sends several requests during initialization. A typical connection sequence is: Initialize , Notifications/Initialize , tools/list and then prompts/list .\n\nGiven that MCP Clients will connect and reconnect whilst open, and the fact that users make periodic calls, we find there is a ratio of around 100 MCP Control messages for each Tool Call.\n\nSome clients also send requests that don't make sense for our Stateless, Direct Response configuration - for example Pings, Cancellations or attempts to list Resources (which isn't a capability we currently advertise).\n\nThe first week of July 2025 saw an astonishing 164 different Clients accessing our Server. Interestingly, one of the most popular tools is mcp-remote . Approximately half of all Clients use it as a bridge to connect to our remote server.\n\nConclusion\n\nMCP is rapidly evolving, and we're excited about what has already been achieved across Chat Applications, IDEs, Agents and MCP Servers over the last few months.\n\nWe can already see how powerful integrating the Hugging Face Hub has been and support for Gradio Spaces now makes it possible for LLMs to be easily extended with the latest Machine Learning applications.\n\nHere are some great examples of things people have been doing with our MCP Server so far:\n\nWe hope that this post has provided insights to the decisions that need to be made building Remote MCP Servers, and encourage you to try some of the examples in your favourite MCP Client.\n\nTake a look at our Open Source MCP Server, and try some of the different transport options with your Client, or open an Issue or Pull Request to make improvements or suggest new functionality.\n\nLet us know your thoughts, feedback and questions on this discussion thread.",
    "link": "https://huggingface.co/blog/building-hf-mcp",
    "Summary": "Building the Hugging Face MCP ServerPublished July 10, 2025 Update on GitHubIntroductionDesign ChoicesRemote Servers Production DeploymentConclusion TL;DR: The Hugging Face Official MCP Server offers unique customization options for AI Assistants accessing the Hub, along with access to thousands of AI applications through one simple URL.\nWe've learned many things about building a useful MCP server in the last month - we'll describe our journey here.\nAt Hugging Face, providing access to the Hub via MCP is an obvious choice, and this article shares our experience developing the hf.co/mcp MCP Server.\nThe Hugging Face MCP Server is Open Source - and supports STDIO, SSE and Streamable HTTP deployment in both Direct Response and Server Push mode.\nThe following picture shows our MCP Server connection dashboard running in \"Server Push\" Streamable HTTP mode:The Hugging Face MCP Server Connection Dashboard.",
    "Keywords": [
      "building",
      "sse",
      "face",
      "hugging",
      "request",
      "http",
      "server",
      "tool",
      "users",
      "response",
      "clients",
      "mcp",
      "client"
    ]
  },
  {
    "Title": "Reachy Mini \u2013 The Open-Source Robot for Today's and Tomorrow's AI Builders",
    "Authors": [],
    "Publish Date": null,
    "Text": "Reachy Mini \u2013 The Open-Source Robot for Today's and Tomorrow's AI Builders\n\nPublished July 9, 2025 Update on GitHub\n\nTiny price, small size, huge possibilities. Code, learn, share with AI builders of all ages, all around the globe.\n\nReachy Mini is an expressive, open-source robot designed for human-robot interaction, creative coding, and AI experimentation. Fully programmable in Python (and soon JavaScript, Scratch) and priced from $299, it's your gateway into robotics AI: fun, customizable, and ready to be part of your next coding project. Whether you're an AI developer, hacker, researcher, teacher, robot enthusiast, or just coding with your kids on the weekend, Reachy Mini lets you develop, test, deploy, and share real-world AI applications from your desk, using the latest AI models!\n\n\ud83d\udd29 Robot technical info\n\nReachy Mini measures 11\u201c/28cm in height and 6.3\u201c/16cm in width (approximately 9\u201c/23cm tall when in sleep mode) and weighs 3.3 lbs/1.5 kg. It comes as a kit and is available either in a lite version or as a fully autonomous system with onboard computing, wireless connectivity, and a battery.\n\nReachy Mini is currently in an early development phase. We\u2019re sharing it as-is, without warranties or guarantees, to engage with early adopters and gather feedback. For mulptiple-unit orders (+10 units), please contact sales@pollen-robotics.com Privacy: Reachy Mini does not store, transmit, or process personal data by default. Any use of its camera or microphone functions is fully under the user's control. Pollen Robotics and Hugging Face do not collect or access any user data from the robot.\n\nReachy Mini Lite Reachy Mini Compute \u274c\n\nCompatible with Mac and Linux\n\n(Windows soon) \u2714\ufe0f\n\nRaspberry Pi 4 Wifi \u274c \u2714\ufe0f Power supply Wired Wired & Battery Microphones 4 4 Speaker 5W \u2714\ufe0f \u2714\ufe0f Camera (wide angle) \u2714\ufe0f \u2714\ufe0f Accelerometer \u274c \u2714\ufe0f Head movement\n\n(6 Degrees of Freedom) \u2714\ufe0f \u2714\ufe0f Full body rotation \u2714\ufe0f \u2714\ufe0f 2 animated antennas \u2714\ufe0f \u2714\ufe0f Delivery ~90 days ~90 days Price $299 (+ taxes + shipping) $449 (+ taxes + shipping) Order the lite version Order the wireless version\n\n\ud83e\udde0 Built for Exploring, Playing, Learning & Sharing\n\nCompact & Affordable : Desktop-sized and starting from $299, ideal for creative prototyping, education and research.\n\n: Desktop-sized and starting from $299, ideal for creative prototyping, education and research. Plug & Play Behaviors : A set of 15+ robot behaviors is provided at launch on the Hugging Face hub so you can play and learn right out of the box.\n\n: A set of 15+ robot behaviors is provided at launch on the Hugging Face hub so you can play and learn right out of the box. Grow with the Community : With easy uploading, sharing, and downloading of new robot behaviors, the possibilities grow exponentially with the Reachy-Mini community.\n\n: With easy uploading, sharing, and downloading of new robot behaviors, the possibilities grow exponentially with the Reachy-Mini community. Programmable in real-life & simulation: Intuitive and accessible coding using the open-source Python SDK (and soon support for more languages). With the simulation SDK, you can even test and develop projects before receiving the robot.\n\n\ud83e\udd1d Designed for Human-Robot Interaction\n\nExpressive Movement : Motorized head and body rotation, and animated antennas for rich human-robot interactions.\n\n: Motorized head and body rotation, and animated antennas for rich human-robot interactions. Multimodal Sensing : Equipped with a camera, microphones, and speakers for AI powered audio-visual interaction.\n\n: Equipped with a camera, microphones, and speakers for AI powered audio-visual interaction. Assemble It Yourself: Sold as a kit, build your own companion robot with friends and kids.\n\n\ud83c\udf0d Open, Modular & Community-Powered\n\n\ud83e\udd17 Hugging Face Integration : Use state-of-the-art open-source models for speech, vision, and personality.\n\n: Use state-of-the-art open-source models for speech, vision, and personality. Open-Source Everything : Hardware, software, and simulation environments fully open-source and community-supported. To be released when ready.\n\n: Hardware, software, and simulation environments fully open-source and community-supported. To be released when ready. Community-Driven: Upload, share and download robot behaviors with the 10M users of the Hugging Face community.\n\nMade with \u2764\ufe0f by Pollen Robotics and Hugging Face.\n\n\ud83e\udd17 Join the Reachy Mini Community on Discord.",
    "link": "https://huggingface.co/blog/reachy-mini",
    "Summary": "Reachy Mini \u2013 The Open-Source Robot for Today's and Tomorrow's AI BuildersPublished July 9, 2025 Update on GitHubTiny price, small size, huge possibilities.\nReachy Mini is an expressive, open-source robot designed for human-robot interaction, creative coding, and AI experimentation.\nFor mulptiple-unit orders (+10 units), please contact sales@pollen-robotics.com Privacy: Reachy Mini does not store, transmit, or process personal data by default.\nGrow with the Community : With easy uploading, sharing, and downloading of new robot behaviors, the possibilities grow exponentially with the Reachy-Mini community.\n: With easy uploading, sharing, and downloading of new robot behaviors, the possibilities grow exponentially with the Reachy-Mini community.",
    "Keywords": [
      "fully",
      "opensource",
      "simulation",
      "todays",
      "mini",
      "face",
      "hugging",
      "ai",
      "behaviors",
      "builders",
      "robot",
      "tomorrows",
      "reachy"
    ]
  },
  {
    "Title": "Creating custom kernels for the AMD MI300",
    "Authors": [],
    "Publish Date": null,
    "Text": "Creating custom kernels for the AMD MI300\n\nPublished July 9, 2025 Update on GitHub\n\nAMD Kernels\n\nIntroduction\n\nMore than a billion per day: that\u2019s a low estimate of how many requests ChatGPT handles daily, a number which is unlikely to go down soon. For each request and each generated token, we run an inference of a multi-billion parameters model. This is why model optimization is paramount at each and every level: when one deals with these kinds of scale, even a 1% latency or power gain can bring huge savings.\n\nBut where might that gain come from? Model architectures are already well established, and popular models have had quantized weight for a long time now. However, a crucial level at which we can optimize model inference remains: the kernel level. Kernels are the algorithms executed when you do any operation in your network: there are matrix multiplication kernels, convolution kernels, batch normalization kernels, etc. Kernels are low-level, highly-optimized algorithms, often tailored for the device they will be running on. They are notoriously long and hard to write, and require a good understanding of the inner working of the GPU.\n\nKernels are essential for running operations in neural networks\u2014without a kernel, an operation effectively can't be used. Because of this, new innovations often launch with a \"day 0\" kernel, typically optimized only for the latest Nvidia hardware. This approach excludes many other devices, particularly AMD GPUs, which, despite offering comparable or superior specs, are often overlooked by kernel developers. Hugging Face collaborated with AMD to deliver state-of-the-art performance on AMD platforms and make it benefit the open source community. As part of this partnership, we decided with AMD to focus on delivering open-source optimized kernels to improve the performance of serving Llama 3.1 405B in FP8 on a node of 8 MI300X using VLLM.\n\nIn this blog post, we'll explore how we optimized performance for the MI300X and how each kernel was individually fine-tuned. But first, let\u2019s look at the performance gains achieved using our custom kernels. By combining the following three optimized kernels:\n\nFused residual connection, RMS norm and FP8 conversion kernel\n\nFused SwiGLU activation and FP8 conversion kernel\n\nSkinny GEMM kernel\n\nwe achieved significant speedups when running VLLM on a node powered by MI300X GPUs.\n\nMeasures were taken with input size 1 and output size 128 to mimic decoding regime. We measure decoding latency using the median over 30 iterations.\n\nThose performance gains were measured in VLLM, but you may also use the kernels separately, as described in the \u201cHow to\u201d section that follows.\n\nHow to use these kernels\n\nThe hf-rocm-kernels repo\n\nAll kernels described previously are available on the hf-rocm-kernels repository located here. In it, you will find instructions on how to install the package, the source code for each kernels, their respective python bindings, various benchmarking scripts and a test suite. Using benchmarking scripts and a MI300X, you may even reproduce from this blog post. To ensure same results for Torch or VLLM, you can use the same container as we did. You can also use the repo as a base to build your own kernels: it has instructions on how to bind a CUDA-style kernel to python and a simple sample kernel. You may even have a look at branches under development for new kernels, like a compute-and-communicate kernel as described here.\n\nIntegration in VLLM\n\nThe kernels described will soon be integrated in the AMD fork of the VLLM project, but if you want to have a look at how you might do something like that yourself, you may check out this branch and this document.\n\nOptimization process\n\nWe are first going to do a quick refresher on the architecture of the device we are working on: the MI300X. Then, we are going to take a look at the state of our model\u2019s inference before optimizing it. This will allow us to identify bottlenecks and know which custom kernels we need to write. Then, we will take a look at each kernel we have written, which will give us an opportunity to explore how kernel optimization is conducted through many angles.\n\nA quick introduction to the MI300X\n\nBefore we dive into optimizing GPU code, we need to know how a GPU works. There are a lot of resources out there that already do a great job of explaining the inner workings of your GPU, which I will link right here, here and here. We are still going to run through the different levels of the GPU, as a quick refresher. If you want to skip the refresher and get directly into the details of our custom kernels, click here!\n\nThreads\n\nThe smallest unit of work in the GPU is the thread. Any time any work is done on a GPU, it\u2019s because a thread executed an instruction. Instructions are basic operations like additions, multiplication, conversion from one data type to another, or loads and stores. Each thread has its own memory, called registers (or VGPRs), which only it can access. A thread can have a maximum of 256 registers, each 32-bit wide. Below is represented a thread with access to its 256 VGPRs.\n\nThreads, except when using load or store instructions, can only execute instructions on their own registers. For instance, to add two vectors A and B together, each thread is going to 1) load in its registers an element from A and 2) another from B, then 3) perform the addition and store the result in another register, and finally 4) store the value from that register in memory. That\u2019s a total of 4 instructions.\n\nWarps\n\nThe next unit of work is a warp: each warp is composed of 64 threads. Warps don\u2019t have their own memory, but they are of interest to us because all threads in a warp must execute the same instruction at the same time. This is both a guarantee and a constraint.\n\nWarps also allow for different threads to exchange information coming from their registers with other threads in the same warp. Although different threads in a warp have access to different data, the fact that they all have to execute the same instructions means that when writing a kernel, warp-level behavior is what you need to think about.\n\nCompute units\n\nWarps are bundled together into thread blocks: thread blocks are software abstractions, but run on a hardware component called a compute unit (CU). A single compute unit can run multiple thread blocks at once, but it can only fit 16 warps. Each compute unit has a dedicated L1 cache and shared memory. L1 cache cannot be controlled or allocated and helps with data reuse of all warps situated on the CU. Conversely, shared memory can be allocated and used as a storage shared by all warps. For instance, when we want all warps (and thus threads) in a compute unit to access the same buffer, we allocate it in shared memory. Both shared memory and L1 cache are fast to access because they are \u201cclose\u201d to the threads.\n\nThread blocks also offer the ability to synchronize all threads running inside: this is quite useful when dealing with operations that impact shared memory, like initializing an array in shared memory to zero or reduction operations. In general, when writing a kernel, thread blocks are the highest level to take into consideration: it\u2019s very hard to synchronize different thread blocks or make them interact in any way whatsoever. Kernel throughput is tightly linked to the number of compute unit present on the GPU: the more CUs there are, the more thread blocks can be run at the same time, which increases throughput if you manage to use all CUs.\n\nXCDs\n\nCompute units are then grouped into accelerator complex dies (XCDs), which hold 38 compute units each. Although CUs may not interact with each others, they all share a L2 cache which you can\u2019t control but still may prove useful when re-using data. For instance, when accessing memory, having two compute units located on the same XCD access the same data will reduce loading latency by a lot. L2 cache is quite large: it has a size of 4MB, while shared memory has a size of 64kB and L1 cache contains 32kB.\n\nThe entire GPU (MI300X)\n\nBy assembling 8 XCDs (which gives us 8 * 38 = 304 CUs) and adding a last level of cache (called infinity cache, with 256MB) and a huge quantity of video ram (192GB) we get the MI300X.\n\nAll XCDs, and thus all threads, have access to the VRAM, but getting there is quite slow. As you get further away from thread-level, memory becomes slower to access but has a larger size and larger scope, meaning it serves more threads. When optimizing a kernel, there is always a balance to strike between doing lots of operations or loading lots of data, but in general, you want to access the VRAM (commonly referred to as global memory) as little as possible.\n\nWhen looking at this figure, we can see why GPUs are referred to as \u201cmassively parallel\u201d: here, we have 304 compute units, which can each run 16 warps, each with 64 threads. This means that we can have up to 311296 threads running at the same time, each executing an instruction of its own. Keep in mind an instruction is something basic like an addition, so simple routines like Newton\u2019s method can be quite long to run for a single thread. GPUs are not optimized for instructions to run fast, i.e. for the latency of each instruction to be low: that would be a latency-oriented device. They are optimized for many threads to be run together, consuming and outputting a large quantity of data: it is a throughput-oriented device. When optimizing a kernel for the GPU, we adapt in consequence: it is better to have an algorithm running a few instructions on many threads at once, than having it run many instructions on a few threads. Hence calling algorithms running on GPUs \u201cparallel\u201d.\n\nWhat can get in the way of such algorithms running in an optimized manner are three things: when there is a lot of data to load (memory bound), when there are many operations to performs (compute bound) or when threads have to work together (synchronization overhead).\n\nDay 0 performance analysis\n\nWhen optimizing a workload, the first thing to do before writing a single line of code is to profile the current state of the workload. In our case, we are going to profile the model inference in VLLM to get an idea of how much time each operation is taking up. This can help identify major bottlenecks and which kernels we can tackle first for maximum speedup. For instance, here is the breakdown for batch size 32:\n\nWe can see the different parts of the network through each slice:\n\nthe \u201cAttention*\u201d slice, where we grouped RoPE, attention and KV cache kernels;\n\nthe \u201cAttention GEMMs\u201d, that encompass two projections, QKV and Output;\n\nthe \u201cCommunications\u201d, which is made up of two all-reduce operations, one after the Attention block and one after the MLP block, which are there because we are working in tensor parallel (TP8)\n\nthe \u201cMLP GEMMs\u201d, that encompass the two projections made in the MLP, Gate / Up and Down;\n\nthe \u201cRMS norm\u201d and \u201cSwiGLU\u201d slices, one for each kernel \u2014 note that the RMS norm kernel is called twice per block, once before the Attention and once before the MLP;\n\nthe \u201cOther\u201d slice that regroups the kernels that we did not tag as part of a larger category because their impact is minor.\n\nAlready we can see that most of the latency comes from GEMMs and communications, but also that attention and the operations surrounding it are not a major contributor to latency. This can come as a surprise, because a lot of papers focus on attention and reducing its cost, but it seems that through a combination of KV caching and FlashAttention, which has already been optimized in VLLM, this part may no longer be a top priority. Surprisingly, the two calls made to the \u201cRMS norm\u201d kernel are quite costly, so there might be a large benefit to optimizing that kernel. Along with the SwiGLU kernel, they represent 15% of the total latency, which is not negligible. All in all, working on those two kernels, plus trying to gain a small speedup on GEMMs may be our best course of action. To check that this performance breakdown is not a fluke, we can take a look at other batch sizes:\n\nWe can see the pattern that emerged for batch size 32 holds up for other batch sizes, albeit with the latency contribution of GEMMs and communications becoming greater as the batch size increases. Also, it seems that batch size 32 is an outlier when it comes to the latency of GEMMs: it\u2019s probably because the GEMMs chosen when batch size is 32 have been manually tuned or because batch size 32 presents good memory alignment patterns, so GEMMs for batch size 32 are faster than for batch size 24 or 28.\n\nNow that we have identified some hot spots to optimize, let\u2019s take a look at the first kernel we wrote: the RMS norm kernel.\n\nRMS norm kernel\n\nIn each decoder block, we have two main parts: an attention block and an MLP block. Both begin with a residual connection between two inputs: the current hidden states x x x and the residual r r r . Both have the same shape, which is n n n rows (as many as there are tokens) and d d d columns. After they are added together, we apply a row-wise Root Mean Square (RMS) norm to x x x and, since the model is in FP8, we quantize x x x to FP8 using a scale s s s . Simply fusing those three operations into a single kernel can deliver a nice performance boost. Mathematically, the operations we have to perform are the following:\n\ni + j + k x \u2190 x + r r \u2190 x V = \u2211 i = 1 d x i 2 x \u2190 x V + \u03f5 x Q = Q fp8 ( s \u2217 x \u2217 w ) \\begin{align} \\phantom{i + j + k} &\\begin{aligned} x &\\leftarrow x + r\\\\ r &\\leftarrow x \\end{aligned}\\\\ &\\begin{aligned} V &= \\sum_{i=1}^{d} x_i^2 \\end{aligned}\\\\ &\\begin{aligned} x &\\leftarrow \\frac{x}{\\sqrt{V + \\epsilon}} \\\\ x_Q &= Q_{\\text{fp8}} \\left( s * x * w\\right) \\end{aligned} \\end{align} i+j+k\u200bxr\u200b\u2190x+r\u2190x\u200bV\u200b=i=1\u2211d\u200bxi2\u200b\u200bxxQ\u200b\u200b\u2190V+\u03f5 \u200bx\u200b=Qfp8\u200b(s\u2217x\u2217w)\u200b\u200b\u200b\n\nwhere w w w is a d d d -sized weight vector. Steps (1) and (3) are pretty basic. For step (1), we just need to position each thread to a different location in the tensor, load some elements of x x x and r r r , add them and store back r r r . For step (3), each thread performs some scalar operations (addition, square root, division) and a conversion to FP8. All of this, each thread can do on its own: this is perfectly suited to the parallel nature of the GPU. The step to watch out for is (2): we need to sum over d d d , which means either each thread is going to visit each of the d d d columns, or we need to exchange data between threads. The greater d d d is, the more data we would have to load for the first option, so the less viable it becomes. We are going to pick the second option: synchronize threads at the block level, and they will exchange data using the shared memory. Each thread is going to accumulate a part of V V V on its own and then we are going to sum all of those parts across the thread block, which is what we call a reduction. Since V V V is computed across an entire row, we are going to assign a thread block for each row.\n\nWhen compared to out-of-the-box pytorch, the bare bones version of this kernel brings about a 10x speedup. But this is not enough: there are still many optimizations we can add on top of this.\n\nIn terms of latency, one of the most costly operation is accessing VRAM, also called global memory. Luckily, there are some easy-to-follow principles that can dramatically reduce the cost of loading data.\n\nFirst, we can take a look at how much data a single thread can load in a single instruction: using the MI300X instruction guide, we see that the largest load we can make from global memory is 128 bits wide. Since we are loading FP16 data, we are going to load 128b / 16b = 8 elements per load. For fp32 elements, it would correspond to 4 elements per load.\n\nSecondly, we make sure memory accesses are coalesced. Since each thread is part of a warp, when one thread reaches a \u201cload\u201d instruction, all other threads in the warp do too. For efficiency\u2019s sake, these \u201cload\u201d instructions are then bundled together across the warp. The warp then collectively fetches the data needed and each thread gets the data it requires. Maximum efficiency is reached when the warp fetches a single chunk of data without any gap in it: this is what we call contiguous data. An issue arises when we need to load more data that can be loaded in one \u201cload\u201d instruction, and is illustrated below.\n\nIn this hypothetical scenario, we have two threads in the same warp. They need to collectively load 16 fp32 elements, without constraint on which thread loads which element. This is a typical \u201creduction\u201d situation. Since a thread can only load 4 fp32 elements per instruction, we have at least two ways of reading the data, represented in scenario (a) and (b). To decide which scenario is best, we need to look at this from warp perspective, not thread perspective. In scenario (a), the first load fetches elements 0,1,2,3,8,9,10,11 : we see that the data is not contiguous, because there is a gap between elements 3 and 8. While in scenario (b), the first load fetches elements 0,1,2,3,4,5,6,7 : we load contiguous data. Same goes for the second load. Thus scenario (b) is better. Although in scenario (a) we end up with 8 contiguous elements per thread, this does not matter: what matters is whether or not the warp loads contiguous data. This matters because if the warp can only load 8 contiguous elements in one cycle, then each load of scenario (a) is processed in two cycles, while in scenario (b), each load only needs the one cycle.\n\nThird, we reduce the number of stores: when we look at steps (1) and (3) we can see that there are only two stores needed: one for r r r and one for x Q x_Q xQ\u200b . After step (1) we can already store r r r and be done with that. But we still need to access the modified version of x x x after step (2) is done. To do that, we can store the modified version of x x x in global memory and reload it after step (2) is done and rely on cache hits when reloading it. Or, if x x x is small enough, we can store its modified version in shared memory: if x x x is in FP16 and we only have one thread block per CU, then we can store 64KB / 2B = 32 * 1024 elements in shared memory per thread block. In the case of Llama 405B, d d d is equal to 16384, so that fits. Using shared memory provides a nice speedup over relying on cache hits, especially when many thread blocks are active at once: if the L1 cache is not big enough to fit the whole of x x x, then we have to rely on L2 cache, which is shared by 38 CUs.\n\nApart from memory access, we can also optimize computational efficiency, but we are going to leave that for the next kernel, as they will be similar in both cases.\n\nResults\n\nWhen we apply the optimizations discussed above, we get the following results:\n\nNumber of rows Torch (\u03bcs) VLLM (\u03bcs) Ours (\u03bcs) 1 38.8998 5.5145 4.18138 2 43.2469 5.65645 4.36976 4 41.1304 5.6893 4.37628 8 43.8883 5.72275 4.39081 16 46.8876 5.85667 4.48165 32 55.2276 6.08502 4.72017 64 75.6086 6.4629 5.54214 128 98.1122 7.49166 6.27341 256 119.727 11.8812 10.739 512 195.782 23.1595 18.5549 1024 355.42 44.8143 34.7204 2048 671.513 81.2089 73.35\n\nwith a [X, 16384] shaped FP16 input tensor. The most basic version of our kernel, referred to as \u201cPointwise\u201d, has no memory-related optimization and already shows at least a x4 speedup over torch. It is less optimal than VLLM\u2019s implementation of the kernel, but our \u201cVectorized\u201d implementation beats both \u201cPointwise\u201d and VLLM. This is the version of the kernel that implements coalesced 128 bits loads, which is only surpassed by the \u201cVectorized + SMEM\u201d (SMEM stands for shared memory) implementation, that offers a notably better speedup ratio than VLLM for both low and high batch sizes.\n\nSwiGLU kernel\n\nIn the MLP block, after the kernel we have just written about, comes a projection which we have referred up to this point as \u201cGate / Up\u201d projection. The reason we call it that way is because the \u201cGate / Up\u201d projection is actually a concatenation of two projections with the same input: \u201cGate\u201d and \u201cUp\u201d. Thus, we will write the result x x x of the \u201cGate / Up\u201d projection as x = x G \u2223 x U x = x_G | x_U x=xG\u200b\u2223xU\u200b where \u2223 | \u2223 is the concatenation operator applied along the column axis. x G x_G xG\u200b and x U x_U xU\u200b have the same dimensions. The reason we need those two projections is the SwiGLU activation function that comes right after, which results y y y is defined by equation (4). The SwiGLU activation function is followed by the \u201cDown\u201d projection, which in our case is in FP8, so we also need to quantize y y y as shown in equation (5):\n\ni + j + k y = \u03c3 ( x G ) \u22c5 x U y Q = Q FP8 ( s \u2217 y ) \\begin{align} \\phantom{i + j + k}& \\begin{aligned} y = \\sigma \\left( x_G \\right) \\cdot x_U \\\\\\end{aligned}\\\\ &\\begin{aligned} y_Q = Q_\\text{FP8} \\left( s * y \\right) \\end{aligned} \\end{align} i+j+k\u200by=\u03c3(xG\u200b)\u22c5xU\u200b\u200byQ\u200b=QFP8\u200b(s\u2217y)\u200b\u200b\u200b\n\nwhere \u03c3 \\sigma \u03c3 is the sigmoid function: \u03c3 ( x ) = e \u2212 x / ( 1 + x ) \\sigma (x) = e^{-x} / (1 + x) \u03c3(x)=e\u2212x/(1+x) . We are going to write a fused kernel that takes care of all of this. For this kernel, optimizations described for the RMS kernel are still relevant with the expection of the shared memory buffer. We will focus here on computation-related optimizations.\n\nThere are two ways we are going to increase the speed of our kernels: increase the volume of work done for each instruction executed and use faster instructions.\n\nTo increase the amount of work done per instruction, we can use packed instructions. Packed instruction are useful when we want to apply the same operator on several elements: rather than executing one instruction per element, we execute one instruction over a vector of element. In a CPU, packed (or vectorized) instructions are the bread-and-butter of single-threaded optimization, as the AVX family of instruction can attest to. There are a few packed instructions on GPU, but they can be quite useful in the right place. On the MI300X there is, among others, packed instruction for FP16 addition and multiplication, which we will use for both steps. There also exists packed conversion from FP32 to FP8, which can provide a nice boost in performance when compared to non-packed conversion. As a matter of fact, there is no conversion from any other data type than FP32 to FP8, so for the RMS norm kernel and this one, we have to go to FP32 precision in order to convert to FP8.\n\nHowever this is not an issue in this kernel: the sigmoid function \u03c3 \\sigma \u03c3 require us to compute an exponent, which is an operation that greatly benefits from FP32 precision. And this is in an instance where we can optimize computation by using a faster instruction: instead of using the exp instruction, we scale the input by log ( 2 ) \\text{log}(2) log(2) and use the exp2 instruction, which is much faster. We suffer an almost negligible loss in precision but also reduce latency.\n\nResults\n\nWe get the following table for a [X, 16384] shaped FP16 input tensor:\n\nNumber of rows 1 2 4 8 16 32 64 128 256 512 1024 2048 Torch (\u03bcs) 40.2731 29.923 35.305 23.5763 22.4738 25.3445 31.5829 40.3194 53.5369 79.8037 124.873 243.202 VLLM (\u03bcs) 3.84116 3.86192 3.92937 3.94151 4.01047 4.02421 4.08943 4.20317 4.48755 7.48465 13.7389 25.4306 Ours (\u03bcs) 1.92981 1.93904 1.93524 1.99316 2.00415 1.91563 2.04498 2.61763 3.57726 5.47608 10.0482 19.8957 Speedup (VLLM / Ours) 1.990434291 1.991665979 2.030430334 1.977518112 2.001082753 2.100724044 1.999740829 1.605715857 1.254465708 1.366789747 1.367299616 1.278195791\n\nWith memory and compute optimizations tailored for the MI300X, we get a kernel that is more than 14 times faster than Torch on average and from 27% to 100% faster than VLLM\u2019s kernel.\n\nSkinny GEMM kernel\n\nAs we have seen earlier, about 60% of the model\u2019s inference latency comes from projections, which rely on GEMM kernels. GEMM kernels are heavily optimized in dedicated libraries such as hipBLASLT rocBLAS on AMD, so writing a custom kernel that performs better in all cases is quite hard. But if we focus on some edge cases that are relevant to us, and write a GEMM kernel for those specific cases, then there is a chance our custom kernel may be faster than the ones in the dedicated libraries.\n\nIn both prefill and decoding, the input of any of the network\u2019s projection has as many rows as there tokens being processed. And during decoding, the number of tokens being processed is equal to the batch size. So during decoding, the number of input rows of all GEMM kernels is equal to the batch size, which for our purposes ranges between 1 and 256. We are going to take an interest with very low batch sizes. When we have a GEMM A \u2217 B = C A * B = C A\u2217B=C such that A A A has few rows and many columns, we say that the GEMM is skinny. The reason we have a specific term for such GEMMs is that they are ill-fitted for the classic GEMM algorithm we run on GPU. Usually, the efficiency of GEMM kernels comes from tiling: we divide the result matrix in many sub-matrices, called tiles, and we assign each tile to a different compute unit (CU). If we have many tiles, we can use many CUs and GPU usage is high. This is illustrated in the figure below.\n\nBut if the input A A A has very few rows, then only a few tiles can be formed, which results in only a few compute units active, hence low GPU utilization:\n\nSkinny GEMMs are fundamentally inconvenient for the GPU. In the next part, we are going to see how through a custom kernel that assumes we are in a skinny GEMM context, we can make them more convenient.\n\nSince the main issue of skinny GEMMs is that we use too few compute units, the first thing we can do is figure out a way to use more. To do this, we can exploit the following mind-breaking formula:\n\nc i j = \u2211 k = 1 K a i k b k j = ( \u2211 k = 1 K / 2 a i k b k j ) + ( \u2211 k = 1 + K / 2 K a i k b k j ) c_{ij} = \\sum_{k=1}^K a_{ik} b_{kj} = \\left( \\sum_{k=1}^{K/2} a_{ik} b_{kj} \\right) + \\left( \\sum_{k=1+K/2}^{K} a_{ik} b_{kj} \\right) cij\u200b=k=1\u2211K\u200baik\u200bbkj\u200b= \u200bk=1\u2211K/2\u200baik\u200bbkj\u200b \u200b+ \u200bk=1+K/2\u2211K\u200baik\u200bbkj\u200b \u200b\n\nThanks to the associativity of the sum, we can split the main GEMM along the shared axis (commonly referred to as the K axis) and replace one GEMM with several sub-GEMMs that are executed concurrently. Each sub-GEMM is going to use as many CUs as the main one would have, so the number of CUs used is multiplied by the number of times we split the K axis. This is shown in the figure below:\n\nHere, we set split K equal to 2 and thus double the amount of CU used at once. Since we get partial results, we need to add them up after the both sub-GEMMs are done. What may seem counter-intuitive is that we are adding an operation, summing the partial results, yet we claim to reduce the latency of the overall process. But since each CU needs to go through the entire K axis to compute the result, because we are cutting it in two, the amount of work done by each CU is also cut in two. If the amount of work saved this way counter balances the amount of work added by the summing up of the final results, then we have an overall optimization. This is generally true as long as K is large and the original GEMM uses less than 50% of the GPU.\n\nOptimization: removing padding\n\nIf we assume that through split-K, most compute units are busy with their own tile, we can focus the scope of optimization at the compute unit level. We are going to take a look at how the actual matrix multiplication is done, and how we can accelerate it.\n\nIn state of the art GPUs like the MI300X, matrix multiplication is handled by a dedicated hardware unit called tensor cores. Tensor cores only perform matrix multiplications, but they do so at very high speed. The format of tensor core instruction is mfma_MxNxK... where mfma stands for matrix fused multiply-add, M is the number of rows of the left-hand matrix, N the number of column of the right-hand matrix, and K is the shared dimension of both. We illustrate a hypothetical instruction mfma_2x2x4 below:\n\nThere are only a few tensor core instructions, but for any triplet MxNxK using the dedicated tensor core instruction is much faster than any other alternative. Tensor core instruction also come in two flavours: \u201cdense\u201d and \u201csparse\u201d. Dense instruction correspond to standard matrix multiplication. Sparse instructions assume that the left-hand side matrix A A A has a 4:2 structured sparsity pattern, which means that two out of every 4 elements along the matrix K axis are zero. Mathematically, for any i , j i, j i,j such that a i , 4 j + 3 a_{i, 4j+3} ai,4j+3\u200b is an element of A A A , we have at least two zeros in ( a i , 4 j , a i , 4 j + 1 , a i , 4 j + 2 , a i , 4 j + 3 ) \\left( a_{i,4j}, a_{i,4j+1}, a_{i,4j+2}, a_{i,4j+3} \\right) (ai,4j\u200b,ai,4j+1\u200b,ai,4j+2\u200b,ai,4j+3\u200b) . Below is an example of a sparse matrix.\n\nLet\u2019s get back to our model, Llama 405B in FP8. For FP8, we only have two dense tensor core instruction: 16x16x32 and 32x32x16 . We also have one sparse instruction of size 16x16x64 . For an input with 8 rows, using even the smallest dense instruction 16x16x32 means that we have to add 8 rows of padding to our input, which is a waste of compute resources. One can wonder if we can use the sparse instruction instead: after all, if half of a 16 rows matrix is 4:2 sparse, we can fully describe its non-zero coefficients using a dense 8 rows matrix. Conversely, if we have an 8 rows dense matrix, we can fit all of its data into a 16 rows matrix with 4:2 sparsity. And the benefit of using the sparse instruction is obvious: the dense instruction has K=32 while the sparse instruction has K=64 . For the same amount of cycles, the sparse instruction has twice the depth. We illustrate this sparsity trick in the figure below with a 1 row input and the 2x2x4 dense instruction and its sparse 2x2x8 counterpart.\n\nUsing this trick, we can notably speed up our GEMM for any input with 8 or less rows, which results in a reduction in per-token latency for any decoding batch that has less than 8 requests.\n\nOptimization: warp specialization and asynchronous execution\n\nWe have seen that in a skinny GEMM, the fact we have a little number of rows limits the number of output tiles, which in turns limit the GPU utilization. But the small number of rows also limits the number of rows each output tiles has, which in turns reduces what we call arithmetic intensity. Simply put, arithmetic intensity is the amount of work done divided by the amount of data loaded to do that work. Let us compare two examples:\n\ns n = \u2211 i = 1 n x i t n = \u2211 i = 1 n y i = y ( 1 + t n \u2212 1 ) s_n = \\sum_{i=1}^{n} x_i \\\\ t_n = \\sum_{i=1}^n y^i = y ~( 1 + t_{n-1}) sn\u200b=i=1\u2211n\u200bxi\u200btn\u200b=i=1\u2211n\u200byi=y (1+tn\u22121\u200b)\n\nwhere x x x is an n n n -sized vector and y y y is a scalar. To compute s n s_n sn\u200b , we load n n n elements and perform n \u2212 1 n-1 n\u22121 additions. To compute t n t_n tn\u200b , we load 1 element and perform 2 n \u2212 1 2n-1 2n\u22121 additions and multiplications. So the \u201carithmetic intensity\u201d of computing s n s_n sn\u200b is n \u2212 1 n \\frac{n-1}{n} nn\u22121\u200b while t n t_n tn\u200b is 2 n \u2212 1 2n - 1 2n\u22121 : the computation of t n t_n tn\u200b is more \u201carithmetically intensive\u201d than the computation of s n s_n sn\u200b . What we see here is that when the lower arithmetic intensity is, the more data we need to load to perform work.\n\nWhy does this matter to us? Well, we have seen that loading data from VRAM has a high latency cost, which is not great for the GPU. In other words, workloads with low arithmetic intensity are ill-suited for the GPU, and it turns out skinny GEMMs have lower arithmetic intensity than their non-skinny counterparts. This becomes intuitive when looking at the figure below: we can see that when we divide the amount of data loaded by two, we divide the number of output coefficients by 4, due to the quadratic nature of the GEMM\u2019s dimensions.\n\nIn a skinny GEMM the number of rows of the output tile is limited and so is the arithmetic intensity. Already this means that we are going to need to load a lot of data to compute an output tile. Furthermore, since we are using FP8 arithmetic, computation is quite fast, so we cannot rely on computation time to hide the latency of data loading. All in all, it would be ideal to have more threads in charge of loading data than threads in charge of computing the result.\n\nTo achieve this, we are going to use a technique called warp specialization. Instead having all warps in the thread block execute the same instructions, we are going to dedicate some warps to loading data only and some to computing the results only. The warps in charge of loading data are called producers and the ones that compute the results are named consumers. Producers and consumers work asynchronously: producers first load data from the VRAM, which is slow, and make it available to the consumers by storing it in a shared memory buffer. Until data is available in shared memory, the consumer is idle. After it data is made available, the consumer loads it from shared memory, which is fast, and computes the result. Coordination of producers and consumers is achieved through a queue stored in shared memory. When a producer finishes storing data in a shared memory buffer i i i , it changes the state of the i i i th variable of the queue to signal data is available there. The consumer is watching out for this, and begins loading data afterwards. When it is done, it changes the i i i th variable of the queue to signal that data can be written over in buffer i i i . In the figure below, we represent the steps involved in a simple asynchronous GEMM with one producer, one consumer and a queue of size 2.\n\nWhat makes the whole process work is that once buffer 0 0 0 is filled by a producer, it can start working on buffer 1 1 1 without waiting for the consumer to have loaded the data from buffer 0 0 0. The goal is to have a queue large enough for the producers to be constantly filling buffers and consumers constantly consuming them. The size of queue is constrained by the size of the shared memory.\n\nWe also need to tune the ratio of producers to consumers: we have said that we have a low arithmetic intensity, so we need to load a lot of data to do a relatively fast computation. Hence, we are going to have a lot of producer warps (typically 8 or 10) for a few consumer warps (something like 2 or 3). Furthermore, we can exploit the fact the GEMM is skinny by having separate producers for the input (the skinny matrix) and the weights (the non-skinny matrix). To make the output tile bigger in the dimension in which it is not constrained in, which is the columns dimension, we allocate more producers for the weights.\n\nFor a more in-depth blog post about asynchronous GEMMs, I encourage you to check out this blog post. A lot of its contents are not applicable in our case though: the MI300X has no warp-level barriers, only a single thread block-level barrier. This led to \u201cfun\u201d shenanigans like ASM to ensure warps waited at their barriers, shared memory loads and stores were resolved before checking the barrier state, and careful handling of the modular nature of the queue. All this would be out of place here, but I encourage you to check out the code or ask away in the comments. A deep dive on the details of async handling might be coming in the future.\n\nThrough warp specialization and asynchronous work, we can adapt our kernel to the low arithmetic intensity workload, but is that enough to come ahead of libraries like hipBLASLT? The answer is yes, in some cases.\n\nResults\n\nSince Torch already binds a highly optimized GEMM taken from AMD\u2019s linear algebra library, we are not going to get speedups in the same range as for the two last kernels. We are first going to take a look at the three GEMM dimension that are of interest to us: namely, the GEMMs dimensions associated with the QKV projection, the Gate / Up projection and the Down projection. The Output projection is being left out because its dimensions do not correspond to the skinny GEMM case.\n\nM (rows) N (cols) K (depth) Torch time (\u03bcs) SkG time (\u03bcs) Speedup 1 2304 16384 14.938 \u00b1 0.292 11.685 \u00b1 0.299 127.84 % 8 2304 16384 16.300 \u00b1 0.282 12.342 \u00b1 0.375 132.07 % 16 2304 16384 16.693 \u00b1 0.233 13.909 \u00b1 0.295 120.02 % 32 2304 16384 16.817 \u00b1 0.124 17.021 \u00b1 0.133 98.80 % 1 13312 16384 77.636 \u00b1 0.364 54.717 \u00b1 0.628 141.88 % 8 13312 16384 80.031 \u00b1 0.449 58.355 \u00b1 0.612 137.15 % 16 13312 16384 75.236 \u00b1 0.378 59.973 \u00b1 1.922 125.45 % 32 13312 16384 82.198 \u00b1 0.590 69.483 \u00b1 1.672 118.30 % 1 16384 6656 31.066 \u00b1 0.193 27.613 \u00b1 0.218 112.51 % 8 16384 6656 31.559 \u00b1 0.200 28.134 \u00b1 0.209 112.17 % 16 16384 6656 31.671 \u00b1 0.250 30.233 \u00b1 0.267 104.76 % 32 16384 6656 35.561 \u00b1 0.335 35.052 \u00b1 1.365 101.45 %\n\nMeasures are taken after 500 warmups iterations, over 2000 profiling iterations, using CUDA graph and multiple weights to avoid cache hits. In order, the GEMM dimensions shown above correspond to QKV projection (N = 2304 and K = 16384), Gate / Up projection (N = 13312 and K = 16384) and Down projection (N= 16384 and K = 6656). We can see that for those dimensions, which have been tuned for, there is a notable speedup for low number of rows (M = 1, 8, 16) but less so for more rows (M = 32). Especially for dimensions in which we can use our sparsity trick (M = 1, 8) we see a notable speedup over Torch, which probably pads everything to 16 rows to use the smallest MFMA instruction.\n\nConclusion\n\nIn this post, we explored just a handful of the many kernel optimization techniques available. If you're interested in experimenting with them, feel free to dive into the hf-rocm-kernels repository and start tinkering! And if you develop a kernel you like of and want to distribute it, be sure to check out kernel-builder and kernels \u2014 two Hugging Face packages designed to help kernel builders make their work widely available and more impactful.",
    "link": "https://huggingface.co/blog/mi300kernels",
    "Summary": "Both begin with a residual connection between two inputs: the current hidden states x x x and the residual r r r .\nBoth have the same shape, which is n n n rows (as many as there are tokens) and d d d columns.\nAfter they are added together, we apply a row-wise Root Mean Square (RMS) norm to x x x and, since the model is in FP8, we quantize x x x to FP8 using a scale s s s .\nBut we still need to access the modified version of x x x after step (2) is done.\nThus, we will write the result x x x of the \u201cGate / Up\u201d projection as x = x G \u2223 x U x = x_G | x_U x=xG\u200b\u2223xU\u200b where \u2223 | \u2223 is the concatenation operator applied along the column axis.",
    "Keywords": [
      "data",
      "load",
      "k",
      "creating",
      "mi300",
      "x",
      "amd",
      "custom",
      "thread",
      "kernel",
      "n",
      "kernels",
      "instruction",
      "memory"
    ]
  },
  {
    "Title": "Upskill your LLMs with Gradio MCP Servers",
    "Authors": [],
    "Publish Date": null,
    "Text": "Upskill your LLMs with Gradio MCP Servers\n\nPublished July 9, 2025 Update on GitHub\n\nUpskill your LLMs With Gradio MCP Servers Have you ever wanted your favorite Large Language Model (LLM) to do more than just answer questions? What if it could edit images for you, browse the web, or organize your email inbox?\n\nWell, now it can! In this blog post, I'll show you:\n\nWhat the MCP protocol is and how it works similarly to the smartphone apps we're all used to, but for LLMs.\n\nHow you can find thousands of MCP servers via the \"MCP App Store.\"\n\nHow to add one of these servers to your favorite LLM of choice to grant it a new ability. We'll work through an example using Flux.1 Kontext[dev] which edits images from plain text instructions.\n\nA Brief Intro To MCP\n\nThe Model Context Protocol (MCP) is an open standard that enables developers to build secure, two-way connections between an LLM and a set of tools. For example, if you create an MCP server that exposes a tool capable of transcribing a video, then you can connect an LLM client (such as Cursor, Claude Code, or Cline) to the server. The LLM will then know how to transcribe videos and use this tool for you depending on your request.\n\nIn short, an MCP server is a standard way to upskill your LLM by granting it a new ability. Think of it like the apps on your smartphone. On its own, your smartphone can't edit images, but you can download an app to do this from the app store. Now, if only there were an app store for MCP servers? \ud83e\udd14\n\nHugging Face Spaces: The MCP App Store\n\nHugging Face Spaces is the world's largest collection of AI applications. Most of these spaces perform a specialized task with an AI model. For example:\n\nThese spaces are implemented with Gradio, an open source python package for creating AI-powered web servers. As of version 5.28.0 , Gradio apps support the MCP protocol.\n\nThat means that Hugging Face Spaces is the one place where you can find thousands of AI-powered abilities for your LLM, aka the MCP App Store!\n\nWant to browse the app store? Visit this link. Manually, you can filter for MCP Compatible in https://hf.co/spaces .\n\nAn Example: An LLM that can edit images\n\nFlux.1 Kontext[dev] is an impressive model that can edit an image from a plain text prompt. For example, if you ask it to \"dye my hair blue\" and upload a photo of yourself, the model will return the photo but with you having blue hair!\n\nLet's plug this model as an MCP server into an LLM and have it edit images for us. Follow these steps:\n\nGo to Hugging Face and create a free account. In your settings, on the left hand side click on MCP . You may have to scroll down in the page to see it. Now, scroll to the bottom of the page. You should see a section called Spaces Tools . In the search bar, type Flux.1-Kontext-Dev and select the space called black-forest-labs/Flux.1-Kontext-Dev . The page should look like this after you click on it:\n\nFor this demo, we'll use Cursor, but any MCP client should follow a similar procedure. Scroll back up to the top of MCP settings page, and click on the Cursor icon of the Setup with your AI assistant section. Now, copy that code snippet and place it in your cursor settings file.\n\nNow, when you start a new chat session in cursor you can ask it to edit an image! Note that for now the image must be available via a public URL. You can create a Hugging Face Dataset to store your images online.\n\nUsing a popular public space as a tool may mean you have to wait longer to receive results. If you visit the space, you can click \"Duplicate This Space\" to create a private version of the space for yourself. If the space is using \"ZeroGPU\", you may need to update to a PRO account to duplicate it.\n\nBonus: You can also search for MCP-compatible spaces with the Hugging Face MCP server! After completing step 4, you can also ask your LLM to find spaces that accomplish a certain task:\n\nConclusion\n\nThis blog post has walked you through the exciting new capabilities that the Model Context Protocol (MCP) brings to Large Language Models. We've seen how Gradio apps, particularly those hosted on Hugging Face Spaces, are now fully MCP compliant, effectively turning Spaces into a vibrant \"App Store\" for LLM tools. By connecting these specialized MCP servers, your LLM can transcend basic question-answering and gain powerful new abilities, from image editing to transcription, to anything you can imagine!",
    "link": "https://huggingface.co/blog/gradio-mcp-servers",
    "Summary": "Upskill your LLMs with Gradio MCP ServersPublished July 9, 2025 Update on GitHubUpskill your LLMs With Gradio MCP Servers Have you ever wanted your favorite Large Language Model (LLM) to do more than just answer questions?\nHow you can find thousands of MCP servers via the \"MCP App Store.\"\n\ud83e\udd14Hugging Face Spaces: The MCP App StoreHugging Face Spaces is the world's largest collection of AI applications.\nBonus: You can also search for MCP-compatible spaces with the Hugging Face MCP server!\nBy connecting these specialized MCP servers, your LLM can transcend basic question-answering and gain powerful new abilities, from image editing to transcription, to anything you can imagine!",
    "Keywords": [
      "servers",
      "gradio",
      "spaces",
      "space",
      "llms",
      "face",
      "hugging",
      "upskill",
      "app",
      "edit",
      "mcp",
      "llm",
      "model"
    ]
  },
  {
    "Title": "SmolLM3: smol, multilingual, long-context reasoner",
    "Authors": [],
    "Publish Date": null,
    "Text": "SmolLM3: smol, multilingual, long-context reasoner\n\nPublished July 8, 2025 Update on GitHub\n\nBase model: https://hf.co/HuggingFaceTB/SmolLM3-3B-Base\n\nInstruct and reasoning model: https://hf.co/HuggingFaceTB/SmolLM3-3B\n\nSmall language models are becoming increasingly important as users seek capable models that can be deployed efficiently. The community has produced a fascinating range of capable small models, each pushing the boundaries of what's possible at this scale. With SmolLM3, we're excited to contribute a new competitive fully open 3B model:\n\nSmolLM3 sits in the efficiency sweet spot. Our 3B model outperforms Llama-3.2-3B and Qwen2.5-3B while staying competitive with larger 4B alternatives (Qwen3 & Gemma3). Beyond the performance numbers, we're sharing exactly how we built it using public datasets and training frameworks.\n\n\n\n\n\nModel summary:\n\n3B model trained on 11T tokens, SoTA at the 3B scale and competitive with 4B models\n\ntrained on 11T tokens, SoTA at the 3B scale and competitive with 4B models Instruct model with dual mode reasoning, supporting think / no_think modes\n\nwith supporting / modes Multilingual support for 6 languages: English, French, Spanish, German, Italian, and Portuguese\n\nfor 6 languages: English, French, Spanish, German, Italian, and Portuguese Long context up to 128k with NoPE and using YaRN\n\nThe complete recipe: We're releasing SmolLM3 with our engineering blueprint. It includes architecture details, exact data mixtures showing how we progressively boost performance across domains in a three-stage pretraining approach, and the methodology for building a hybrid reasoning model. Usually, achieving these results would require months of reverse engineering. Instead, we're providing the full methodology.\n\n\n\n\n\nWhether you're building your own models or want to understand what drives performance at this scale, this blueprint shows the engineering story behind competitive 3B performance.\n\nLet\u2019s have a look at the pretraining stage.\n\nPretraining\n\nSmolLM3 both changed the architecture and data mixture over its predecessors. Let\u2019s have a look at the architecture and training configurations first!\n\nArchitecture and training details\n\n\n\n\n\nSmolLM3 follows a transformer decoder architecture with tied embedding similar to SmolLM2, building on Llama architecture with some key modifications optimized for efficiency and long context performance.\n\nGrouped Query Attention (GQA): We replaced multi-head attention with grouped-query attention using 4 groups. Our ablations on a 3B model trained with 100B tokens from FineWeb-Edu showed that GQA matches the performance of multi-head attention while significantly reducing the KV cache size during inference.\n\nNoPE: We implemented NoPE from \"RoPE to NoRoPE and Back Again: A New Hybrid Attention Strategy\" (Yang et al., 2025), selectively removing rotary position embeddings from every 4th layer. This approach improves long context performance without affecting short context capabilities, as confirmed by our ablations.\n\nIntra-Document Masking: Following \"Analysing The Impact of Sequence Composition on Language Model Pre-Training\", during training, we use attention masking to ensure tokens from different documents in the same training sequence don't attend to each other. Similar to Llama 3, this helps with faster and more stable long context training while maintaining short context performance.\n\nTraining Stability: Following OLMo 2, we remove weight decay from embedding layers to improve training stability. This modification contributed to more stable training dynamics, with embedding norms naturally stabilizing at healthier values during training without impacting overall performance in our ablations.\n\nAll these changes were validated through ablations using the same 3B architecture trained on 100B tokens from FineWeb-Edu, ensuring each modification either improved performance or maintained it while offering other benefits.\n\nTraining Configuration: We use a global batch size of 2.36M tokens with 4096 sequence length, a learning rate of 2e-4, and the AdamW optimizer (beta1: 0.9, beta2: 0.95) with weight decay of 0.1 and gradient clipping of 1. We use the WSD (Warmup-Stable-Decay) scheduler, with 2000 warmup steps, and a linear decay to 0 in the final 10% training steps. We use nanotron framework for the training, datatrove for data processing and lighteval for evaluation. The model was trained on 384 H100 GPUs for 24 days. You can see the distributed training setup in the following figure.\n\n\n\n\n\nIn addition to architecture changes we also ablated and improved the training recipe. Let\u2019s have a closer look.\n\nData mixture and training stages\n\nFollowing SmolLM2's multi-stage approach, we train SmolLM3 on 11.2T tokens using a three-stage training strategy that mixes web, math, and code data with evolving proportions. We conducted extensive ablations on 3B models trained on 50B to 100B tokens to determine the data mixture and ratios.\n\n\n\n\n\nThe pretraining consists of these stages, also shown in the figure above:\n\nStage 1: Stable phase (0T \u2192 8T tokens) This foundation stage establishes strong general capabilities with our core dataset mixture: Web: 85% (12% multilingual) - FineWeb-Edu, DCLM, FineWeb2 and FineWeb2-HQ Code: 12% - The Stack v2 (16 programming languages), StarCoder2 pull requests, Jupyter and Kaggle notebooks, GitHub issues, and StackExchange. Math: 3% - FineMath3+ and InfiWebMath3+\n\nThis foundation stage establishes strong general capabilities with our core dataset mixture: Stage 2: Stable phase (8T \u2192 10T tokens) We introduce higher quality math and code datasets while maintaining good web coverage: Web: 75% (12% Multilingual) Code: 15% - Adding Stack-Edu Math: 10% - Introducing FineMath4+, InfiWebMath4+, and MegaMath (including Qwen Q&A, Pro synthetic rewrites, and text-code interleaved blocks)\n\nWe introduce higher quality math and code datasets while maintaining good web coverage: Stage 3: Decay Phase (10T \u2192 11.1T tokens) The final stage further upsamples math and code data Web: 63% (12% Multilingual) Code: 24% - upsampling of high-quality code data Math: 13% - upsampling math data and introducing instruction and reasoning datasets such as OpenMathReasoning\n\nThe final stage further upsamples math and code data\n\nWith these stages and mixtures we achieved very competitive performance for the base model. More on that in the evaluation section. The nanotron training configs with exact data weights can be found here. We will also share our training logs along with intermediate checkpoints.\n\nAfter the main pretraining we improved the model in a mid-training stage for long context and reasoning.\n\nWe call the long context adaptation and reasoning adaptation \u201cmid-training\u201d. They are much shorter than the main pretraining but still somewhat general and aimed at improving the model in those two domains. Let\u2019s first have a look at long context training.\n\nLong Context extension\n\n\n\n\n\nAfter the main pretraining, we trained SmolLM3 on an additional 100B tokens to extend its context length. We sequentially extended the context window in two stages for 50B tokens each: first transitioning from 4k to 32k context with RoPE theta increased to 1.5M, then from 32k to 64k context with RoPE theta increased to 5M. Both stages upsampled math, code, and reasoning data. During ablations, we found that upsampling specific long context data such as code repositories, books, and long web pages (beyond the naturally long samples in our mixture) didn't further boost performance on RULER and HELMET benchmarks. Using NoPE and training on the decay mixture with longer sequences and increased RoPE theta values was sufficient to achieve competitive long context performance up to 64k.\n\nFollowing Qwen2.5, we use YARN to extrapolate beyond the training context length. During inference, the model can handle up to 128k context (2x extension beyond the 64k training length).\n\nReasoning Mid-training\n\nAfter extending the context length of the model, we trained it at a mid-training stage to incorporate reasoning capabilities. The main difference between the mid-training stage and the pre- and post-training stages is that we targeted a general capability without yet focusing on a specific domain. In our case, we wanted to train the model to reason without targeting a particular domain, such as mathematics or computer code.\n\nOur mid-training dataset contained 35B tokens sourced from Open Thought\u2019s OpenThoughts3-1.2M and a subset from NVIDIA\u2019s Llama-Nemotron-Post-Training-Dataset-v1.1 with reasoning traces from R1. We used the ChatML chat template and wrapped packing to avoid providing too much structure to the model. We trained the model for 4 (~140B tokens) epochs and used the checkpoint for subsequent SFT stages.\n\nThe release of reasoning models like DeepSeek R1 and Qwen3 has demonstrated the powerful capabilities that emerge when models can engage in explicit reasoning. However, the community still lacks fully open recipes with public datasets to build dual instruction models that support both reasoning and non-reasoning modes. Most existing approaches involve complex reinforcement learning processes and proprietary datasets, making it difficult for researchers to reproduce and build upon these results.\n\nIn this section, we explain how we tackled these challenges and share our complete recipe for building a dual instruction model. We detail how we balance performance between reasoning and non-reasoning modes through a carefully designed training pipeline that includes mid-training for general reasoning capabilities, supervised fine-tuning with synthetic data generation, and alignment using Anchored Preference Optimization (APO) - a recent variant of DPO.\n\n\n\n\n\nBuilding the Chat Template\n\nBefore diving into the training methodology, it's essential to establish how users interact with our dual-mode model. The chat template serves as the interface that enables seamless switching between reasoning and non-reasoning modes, and its design directly impacts both our training data format and model behavior. SmolLM3's chat template allows users to control the reasoning mode during a conversation. Users can activate reasoning or non-reasoning modes by including the /think and /no_think flags, respectively, in the system prompt. In non-reasoning mode, we pre-fill the model's response with empty think blocks, similar to Qwen3, to ensure direct answers without explicit reasoning.\n\nSmolLM3 supports tool calling, and its chat template incorporates two distinct sections for tool descriptions: XML Tools and Python Tools. This specific categorization proved beneficial in our experiments for the model's accurate interpretation of tool definitions in each format.\n\nThe chat template provides a default system message for both reasoning modes, along with a metadata section that includes the date, knowledge cut-off date, and current reasoning mode. Users can replace the default system message by providing one with the system role. The metadata section can be excluded by using the /system_override flag in the system prompt, offering flexibility for specific use cases.\n\nSupervised Finetuning\n\nFollowing the reasoning mid-training stage, where we trained the model on 140B tokens of general reasoning data, we proceed with Supervised Finetuning (SFT) to incorporate capabilities across both reasoning and non-reasoning modes for math, code, general reasoning, instruction following, multilinguality, and tool calling. Training a dual-mode model requires carefully balancing the data mixture to maintain strong performance in both modes across all target domains. To evaluate SmolLM3\u2019s performance throughout training, we tracked the following domains: math, code, general reasoning, instruction following, and multilinguality.\n\nThe primary challenge we encountered when building the reasoning mode dataset was the scarcity of datasets containing reasoning traces for certain domains. To address this gap, we generated synthetic data by prompting Qwen3-32B in reasoning mode with prompts from existing non-reasoning datasets. This allowed us to improve performance in domains where the model initially struggled in reasoning mode, such as multi-turn conversations, multilinguality, and everyday conversations.\n\n\n\n\n\nOur final data mixture was the result of extensive ablations examining the optimal ratio of reasoning to non-reasoning tokens and the composition within each mode. The resulting SFT dataset contains 1.8B tokens: 1B in non-reasoning mode and 0.8B in reasoning mode, comprising 12 non-reasoning datasets and 10 datasets with reasoning traces. We trained for 4 epochs (~8B tokens) using BFD (best-fit decreasing) packing with the loss masked on user turns and the results from tool calls.\n\nWe will release this data mixture along with our full training scripts to enable the community to reproduce and build upon our work.\n\nOff-policy model alignment with Anchored Preference Optimization (APO)\n\nAfter the SFT step, we performed a round of model alignment using a combination of the Tulu3 preference dataset for non-reasoning mode and new synthetic preference pairs for reasoning mode, that we generated from Qwen3-32B and Qwen3-0.6B. To ensure full coverage of all domains in the non-thinking dataset, we generated complementing thinking mode preference pairs. We selected generations from Qwen3-32B as \u201cchosen\u201d and responses from Qwen3-0.6B as \u201crejected\u201d for alignment with Anchored Preference Optimization.\n\n\n\n\n\nAnchored Preference Optimization (APO) is a variant of Direct Preference Optimization (DPO) that provides a more stable optimization objective. In DPO, the reward function r_\u03b8(x,y) measures the log-ratio of the probability of the sequence during training compared to the model at the start of training, the reference model:\n\n\n\n\n\nHere \u03b2 controls how much the model being optimized can change relative to the reference model. The DPO loss optimizes triplets of prompts x, chosen y_w and rejected y_l responses:\n\n\n\n\n\nThe APO objective has been shown to be more stable, and we also observed higher downstream performance in our internal ablations.\n\n\n\n\n\nWhile downstream evaluations showed improvements across mathematics, science, instruction following, coding, chat, and multilingual tasks, we observed performance degradation on long context benchmarks like RULER. We traced this degradation back to the reasoning mid-training stage, where the focus on reasoning capabilities impacted long context performance. Additionally, the APO training data was limited to 24k tokens since the vast majority of our reasoning dataset fell below this length.\n\nTo mitigate this performance drop, we explored model merging as a solution.\n\nModel Merging\n\nModel merging is a popular and powerful technique that allows combining the strengths of different models without the computational overhead of ensembling or the need for additional training. We used the MergeKit library to perform the model merging, as it includes several merging methods, including linear and non-linear merging.\n\nOur merging recipe consists of two steps:\n\nTake each APO checkpoint and create a model \u201csoup\u201d. Combine the model soup with a mid-training checkpoint that has strong long-content performance. A linear merge with weights of 0.9 and 0.1 for the APO model soup and mid-training checkpoint, respectively, achieved the best performance. We were able to recover the base model\u2019s RULER score on contexts up to 128k tokens.\n\nThe resulting model is the checkpoint we are releasing today. It maintains performance across a wide range of tasks. So let\u2019s turn to the evaluation result both of this model as well as the base model.\n\nEvaluation\n\nWe evaluate base models and the instruct model both in reasoning and non-reasoning mode. Let\u2019s first cover the base model\u2019s performance!\n\nBase model\n\nThe plot below shows SmolLM3's win rate across 12 popular benchmarks evaluating knowledge, reasoning, math, and coding capabilities. SmolLM3 consistently outperforms other 3B models and achieves competitive performance with larger 4B models including Qwen3-4B and Gemma3-4B.\n\nEvaluation benchmarks used for the win rate: HellaSwag, ARC, Winogrande, CommonsenseQA, MMLU-CF, MMLU Pro CF, PIQA, OpenBookQA, GSM8K, MATH, HumanEval+, MBPP+\n\n\n\n\n\nSmolLM3 achieves first or second place on knowledge and reasoning benchmarks (HellaSwag, ARC, BoolQ), demonstrating strong performance in these core capabilities. Math and coding performance is competitive within the 3B class. Long-context performance on Ruler 64k shows the model can handle extended sequences effectively.\n\n\n\n\n\nThe model demonstrates strong multilingual performance across five major European languages when evaluated on multilingual benchmarks including Global MMLU, MLMM HellaSwag, Flores-200, Belebele, testing knowledge, commonsense reasoning, text understanding, and translation. This shows SmolLM3 maintains consistent performance beyond English.\n\n\n\n\n\nIn summary, the base model shows very strong performance across many domains. Let\u2019s see how this translates to the instruct model\u2019s performance.\n\nDual Instruct / Reasoning model\n\nSince SmolLM3 has both an instruct and reasoning mode we need to evaluate the model in both modes and compare to models with same capabilities.\n\nNo extending thinking evaluation\n\nWe evaluate SmolLM3 against other 3B non-reasoning models and compare it to Qwen3 reasoning models in no thinking mode across multiple benchmarks. As shown in the performance chart, SmolLM3 outperforms other 3B non-reasoning models including Llama3.2 3B Instruct and Qwen2.5 3B Instruct and sits at an efficiency sweet spot between reasoning models, significantly outperforming Qwen3 1.7B while getting close to the 4B model performance at a lower computational cost.\n\n\n\n\n\nSo the instruct model sits right at the pareto front of performance and cost. Let\u2019s see how the reasoning model does!\n\nExtending thinking evaluation\n\nWhen evaluating SmolLM3's reasoning capabilities with extended thinking enabled, the model shows substantial improvements across most benchmarks compared to its non-reasoning counterpart. We observe notable gains in challenging tasks like AIME 2025 (36.7% vs 9.3%), competitive programming on LiveCodeBench (30.0% vs 15.2%), and graduate-level reasoning on GPQA Diamond (41.7% vs 35.7%).\n\nWhile Qwen3 4B generally achieves the highest scores across both thinking and non-thinking modes, SmolLM3 demonstrates competitive performance within the 3B parameter class, particularly excelling in mathematical reasoning and complex problem-solving tasks. The model's dual-mode capability allows users to choose between faster inference without reasoning or more thorough analysis with extended thinking.\n\n\n\n\n\nSo the last question is: how can you use the model?\n\nHow to run locally\n\nThe modeling code for SmolLM3 is available in transformers v4.53.0 , so make sure to upgrade your transformers version. You can also load the model with the latest vllm which uses transformers as a backend.\n\npip install -U transformers\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer model_name = \"HuggingFaceTB/SmolLM3-3B\" device = \"cuda\" tokenizer = AutoTokenizer.from_pretrained(model_name) model = AutoModelForCausalLM.from_pretrained( model_name, ).to(device) prompt = \"Give me a brief explanation of gravity in simple terms.\" messages_think = [ { \"role\" : \"user\" , \"content\" : prompt} ] text = tokenizer.apply_chat_template( messages_think, tokenize= False , add_generation_prompt= True , ) model_inputs = tokenizer([text], return_tensors= \"pt\" ).to(model.device) generated_ids = model.generate(**model_inputs, max_new_tokens= 32768 ) output_ids = generated_ids[ 0 ][ len (model_inputs.input_ids[ 0 ]) :] print (tokenizer.decode(output_ids, skip_special_tokens= True ))\n\nWe recommend setting temperature=0.6 and top_p=0.95 in the sampling parameters.\n\nEnabling and Disabling Extended Thinking Mode\n\nWe enable extended thinking by default, so the example above generates the output with a reasoning trace. For choosing between enabling, you can provide the /think and /no_think flags through the system prompt as shown in the snippet below for extended thinking disabled. The code for generating the response with extended thinking would be the same except that the system prompt should have /think instead of /no_think .\n\nprompt = \"Give me a brief explanation of gravity in simple terms.\" messages = [ { \"role\" : \"system\" , \"content\" : \"/no_think\" }, { \"role\" : \"user\" , \"content\" : prompt} ] text = tokenizer.apply_chat_template( messages, tokenize= False , add_generation_prompt= True , )\n\nAgentic Usage\n\nSmolLM3 supports tool calling! Just pass your list of tools under the argument xml_tools (for standard tool-calling), or python_tools (for calling tools like python functions in a <code> snippet).\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer checkpoint = \"HuggingFaceTB/SmolLM3-3B\" tokenizer = AutoTokenizer.from_pretrained(checkpoint) model = AutoModelForCausalLM.from_pretrained(checkpoint) tools = [ { \"name\" : \"get_weather\" , \"description\" : \"Get the weather in a city\" , \"parameters\" : { \"type\" : \"object\" , \"properties\" : { \"city\" : { \"type\" : \"string\" , \"description\" : \"The city to get the weather for\" }}}} ] messages = [ { \"role\" : \"user\" , \"content\" : \"Hello! How is the weather today in Copenhagen?\" } ] inputs = tokenizer.apply_chat_template( messages, enable_thinking= False , xml_tools=tools, add_generation_prompt= True , tokenize= True , return_tensors= \"pt\" ) outputs = model.generate(inputs) print (tokenizer.decode(outputs[ 0 ]))\n\nConclusion\n\nWe release SmolLM3, a small, long-context, multilingual, reasoner with up to 128k context. In addition to the model checkpoint we release the full training recipe including pre-training, mid-training, post-training, and synthetic data generation as well as the datasets (coming shortly). We hope this model proves useful to the community and the recipe will allow other groups to improve upon it.\n\nResources\n\nModels collection with quantized checkpoints: Link\n\nSmolLM GitHub repo with pretraining configs and evaluation code: https://github.com/huggingface/smollm\n\nOur HuggingFace org: https://huggingface.co/HuggingFaceTB\n\nCitation",
    "link": "https://huggingface.co/blog/smollm3",
    "Summary": "SmolLM3: smol, multilingual, long-context reasonerPublished July 8, 2025 Update on GitHubBase model: https://hf.co/HuggingFaceTB/SmolLM3-3B-BaseInstruct and reasoning model: https://hf.co/HuggingFaceTB/SmolLM3-3BSmall language models are becoming increasingly important as users seek capable models that can be deployed efficiently.\nThis approach improves long context performance without affecting short context capabilities, as confirmed by our ablations.\nSimilar to Llama 3, this helps with faster and more stable long context training while maintaining short context performance.\nThe release of reasoning models like DeepSeek R1 and Qwen3 has demonstrated the powerful capabilities that emerge when models can engage in explicit reasoning.\nThe primary challenge we encountered when building the reasoning mode dataset was the scarcity of datasets containing reasoning traces for certain domains.",
    "Keywords": [
      "data",
      "models",
      "training",
      "smol",
      "performance",
      "multilingual",
      "longcontext",
      "reasoning",
      "mode",
      "context",
      "tokens",
      "code",
      "reasoner",
      "smollm3",
      "model"
    ]
  },
  {
    "Title": "Three Mighty Alerts Supporting Hugging Face\u2019s Production Infrastructure",
    "Authors": [],
    "Publish Date": null,
    "Text": "Three Mighty Alerts Supporting Hugging Face\u2019s Production Infrastructure\n\nPublished July 8, 2025 Update on GitHub\n\nThe Infrastructure team at Hugging Face is excited to share a behind-the-scenes look at the inner workings of Hugging Face's production infrastructure, which we\u2019ve had the privilege of helping to build and maintain. Our team's dedication to designing and implementing a robust monitoring and alerting system has been instrumental in ensuring the stability and scalability of our platforms. We\u2019re constantly reminded of the impact that our alerts have on our ability to identify and respond to potential issues before they become major incidents.\n\nIn this blog post, we\u2019ll dive into the details of three mighty alerts that play their unique role in supporting our production infrastructure, and explore how they've helped us maintain the high level of performance and uptime that our community relies on.\n\nHigh NAT Gateway Throughput\n\nOftentimes, with cloud computing architectures, where data flows between private and public networks, implementing a NAT (Network Address Translation) gateway stands as a steadfast best practice. This gateway acts as a strategic gatekeeper, monitoring and facilitating all outbound traffic towards the public Internet. By centralizing egress traffic, the NAT gateway offers a strategic vantage point for comprehensive visibility. Our team can easily query and analyze this traffic, making it an invaluable asset when working through security, cost optimization or various other investigative scenarios.\n\nCost optimization is a critical aspect of cloud infrastructure management, and understanding the pricing dynamics is key. In data centers, pricing structures often differentiate between east-west traffic (typically communication within the same rack or building) and north-south traffic (communication between further away private networks or the internet). By monitoring network traffic volume, Hugging Face gains valuable insights into these traffic patterns. This awareness allows us to make informed decisions regarding infrastructure configuration and architecture, ensuring we limit incurring needless costs.\n\nOne of our key alerts is designed to notify us when our network traffic volume surpasses a predefined threshold. This alert serves multiple purposes. Firstly, it acts as an early warning system, alerting us to any unusual spikes in traffic that might indicate potential issues or unexpected behavior. Secondly, it prompts us to regularly review our traffic trends, ensuring we stay on top of our infrastructure's growth and evolving needs. This alert is set at a static threshold, which we have fine-tuned over time, ensuring it remains relevant and effective. When triggered, it often coincides with periods of refactoring our infrastructure.\n\nFor instance, when integrating third-party security and autoscaling tools, we've observed increased telemetry data egress from our nodes, triggering the alert and prompting us to optimize our configurations.\n\nOn another occasion, adjustments to our infrastructure lead to mistakenly avoiding a private, low-cost path between product-specific infrastructure (ex. traffic destined to the Hub from a Space to interact with repository data). To elaborate further, the most impactful workloads in terms of cost savings we\u2019ve found are those that access object storage. Fetching objects directly prices cheaper than going through CDN-hosted assets for our LFS repository storage and additionally does not require the same security measures that our WAF provides compared to public requests arriving at our front door. Leveraging DNS overrides to switch traffic through private network paths and public network paths has become a valuable technique for us, driven by the CDKTF AWS provider.\n\nnew Route53ResolverFirewallRule ( stack, `dns-override-rule- ${key} - ${j} ` , { provider : group?. provider !, name : `dns-override- ${dnsOverride.name} - ${rule[ 0 ]} ` , action : 'BLOCK' , blockOverrideDnsType : 'CNAME' , blockOverrideDomain : ` ${rule[ 1 ]} .` , blockOverrideTtl : dnsOverride. ttl , blockResponse : 'OVERRIDE' , firewallDomainListId : list. id , firewallRuleGroupId : group!. id , priority : 100 + j, }, );\n\nAs a final note, while we have configuration-as-code ensuring the desired state is always in effect, having an additional layer of alerting around helps in case mistakes are made when expressing the desired state through code.\n\nHub Request Logs Archival Success Rate\n\nThe logging infrastructure at Hugging Face is a sophisticated system designed to collect, process, and store vast amounts of log data generated by our applications and services. At the heart of this system is the Hub application logging pipeline, a well-architected solution that ensures Hub model usage data is efficiently captured, enriched, and stored for reporting and archival purposes. The pipeline begins with Filebeat, a lightweight log shipper that runs as a daemonset alongside our application pods in each Kubernetes cluster. Filebeat's role is to collect logs from various sources, including application containers, and forward them to the next stage of the pipeline.\n\nOnce logs are collected by Filebeat, they are sent to Logstash, a powerful log processing tool. Logstash acts as the data processing workhorse, applying a series of mutations and transformations to the incoming logs. This includes enriching logs with GeoIP data for geolocation insights, routing logs to specific Elasticsearch indexes based on predefined criteria, and manipulating log fields by adding, removing, or reformatting them to ensure consistency and ease of analysis. After Logstash has processed the logs, they are forwarded to an Elasticsearch cluster.\n\nElasticsearch, a distributed search and analytics engine, forms the core of our log storage and analysis platform. It receives the logs from Logstash and applies its own set of processing rules through Elasticsearch pipelines. These pipelines perform minimal processing tasks, such as adding timestamp fields to indicate the time of processing, which is crucial for log analysis and correlation. Elasticsearch provides a scalable and flexible storage solution, allowing us to buffer logs for operational use and real-time analysis.\n\nTo manage the lifecycle of logs within Elasticsearch, we employ a robust storage and lifecycle management strategy. This ensures that logs are retained in Elasticsearch for a defined period, providing quick access for operational and troubleshooting purposes. After this retention period, logs are offloaded to long-term archival storage. The archival process involves an automated tool that reads logs from Elasticsearch indexes, formats them as Parquet files\u2014an efficient columnar storage format\u2014and writes them to our object storage system.\n\nThe final stage of our logging pipeline leverages AWS data warehousing services. Here, AWS Glue crawlers are utilized to discover and classify data in our object storage, automatically generating a Glue Data Catalog, which provides a unified metadata repository. The Glue table schema is periodically refreshed to ensure it remains up-to-date with the evolving structure of our log data. This integration with AWS Glue enables us to query the archived logs using Amazon Athena, a serverless interactive query service. Athena allows us to run SQL queries directly against the data in object storage, providing a cost-effective and scalable solution for log analysis and historical data exploration.\n\nThe logging pipeline, while meticulously designed, is not without its challenges and potential points of failure. One of the most critical vulnerabilities lies in the elasticity of the system, particularly in the Elasticsearch cluster. Elasticsearch, being a distributed system, can experience backpressure in various scenarios, such as high ingress traffic, intensive querying, or internal operations like shard relocation. When backpressure occurs, it can lead to a cascade of issues throughout the pipeline. For instance, if the Elasticsearch cluster becomes overwhelmed, it may start rejecting or delaying log ingestion, causing backlogs in Logstash or even Filebeat, which can result in log loss or delayed processing.\n\nAnother point of fragility is the auto-schema detection mechanism in Elasticsearch. While it is designed to adapt to changing log structures, it can fail when application logs undergo significant field type changes. If the schema detection fails to recognize the new field types, it may lead to failed writes from Logstash to Elasticsearch, causing log processing bottlenecks and potential data inconsistencies. This issue highlights the importance of proactive log schema management and the need for robust monitoring to detect and address such issues promptly.\n\nMemory management is also a critical aspect of the pipeline's stability. The log processing tier, including Logstash and Filebeat, operates with limited memory resources to control costs. When backpressure occurs, these components can experience Out-of-Memory (OOM) issues, especially during system slowdowns. As logs accumulate and backpressure increases, the memory footprint of these processes grows, pushing them closer to their limits. If not addressed promptly, this can lead to process crashes or further exacerbation of the backpressure problem.\n\nArchival jobs, responsible for transferring logs from Elasticsearch to object storage, have also encountered challenges. On occasion, these jobs can be resource-intensive, with their performance becoming sensitive to node size and memory availability. In cases where junk data or unusually large log entries pass through the pipeline, they can strain the archival process, leading to failures due to memory exhaustion or node capacity limits. This underscores the importance of data validation and filtering earlier in the pipeline to prevent such issues from reaching the archival stage.\n\nTo mitigate these potential failures, we've implemented a powerful alert system with a unique motivation: validating end-to-end log flow. The alert is designed to compare the number of requests received by our Application Load Balancer (ALB) with the number of logs successfully archived, providing a comprehensive view of log data flow throughout the entire pipeline. This approach allows us to quickly identify any discrepancies that might indicate potential log loss or processing issues.\n\nThe alert mechanism is based on a simple yet effective comparison: the number of requests hitting our ALB, which represents the total log volume entering the system, versus the number of logs successfully archived in our long-term storage. By monitoring this ratio, we can ensure that what goes in must come out, providing a robust validation of our logging infrastructure's health. When the alert is triggered, it indicates a potential mismatch, prompting immediate investigation and remediation.\n\nIn practice, this alert has proven to be a valuable tool, especially during periods of infrastructure refactoring. For instance, when we migrated our ALB to a VPC origin, the alert was instrumental in identifying and addressing the resulting log flow discrepancies. However, it has also saved us in less obvious scenarios. For example, when archive jobs failed to run due to unforeseen issues, the alert flagged the missing archived logs, allowing us to promptly investigate and resolve the problem before it impacted our log analysis and retention processes.\n\nWhile this alert is a powerful tool, it is just one part of our comprehensive monitoring strategy. We continuously refine and adapt our logging infrastructure to handle the ever-increasing volume and complexity of log data. By combining proactive monitoring, efficient resource management, and a deep understanding of our system's behavior, Hugging Face ensures that our logging pipeline remains resilient, reliable, and capable of supporting our platform's growth and evolving needs. This alert is a testament to our commitment to maintaining a robust and transparent logging system, providing our teams with the insights they need to keep Hugging Face running smoothly.\n\nKubernetes API Request Errors and Rate Limiting\n\nWhen operating cloud-native applications and Kubernetes-based infrastructures, even seemingly minor issues can escalate into significant downtime if left unchecked. This is particularly true for the Kubernetes API, which serves as the central nervous system of a Kubernetes cluster, orchestrating the creation, management, and networking of containers. At Hugging Face, we've learned through experience that monitoring the Kubernetes API error rate and rate limiting metrics is a crucial practice, one that can prevent potential disasters.\n\nHugging Face's infrastructure is deeply integrated with Kubernetes, and the kube-rs library has been instrumental in building and managing this ecosystem efficiently. kube-rs offers a Rust-centric approach to Kubernetes application development, providing developers with a familiar and powerful toolkit. At its core, kube-rs introduces three key concepts: reflectors, controllers, and custom resource interfaces. Reflectors ensure real-time synchronization of Kubernetes resources, enabling applications to react swiftly to changes. Controllers, the decision-makers, continuously reconcile the desired and actual states of resources, making Kubernetes self-healing. Custom resource interfaces extend Kubernetes, allowing developers to define application-specific resources for better abstraction.\n\nAdditionally, kube-rs introduces watchers and finalizers. Watchers monitor specific resources for changes, triggering actions in response to events. Finalizers, on the other hand, ensure proper cleanup and resource termination by defining custom logic. By providing Rust-based abstractions for these Kubernetes concepts, kube-rs allows developers to build robust, efficient applications, leveraging the Kubernetes platform's power and flexibility while maintaining a Rust-centric development approach. This integration streamlines the process of building and managing complex Kubernetes applications, making it a valuable tool in Hugging Face's infrastructure.\n\nHugging Face's integration with Kubernetes is a cornerstone of our infrastructure, and the kube-rs library plays a pivotal role in managing this ecosystem. The kube::api:: module is instrumental in automating various tasks, such as managing HTTPS certificates for custom domains supporting our Spaces product. By programmatically handling certificate lifecycles, we ensure the security and accessibility of our services, providing users with a seamless experience. Additionally, we have used this module outside of user-facing features during routine maintenance to facilitate node draining and termination providing cluster stability during infrastructure updates.\n\nThe kube::runtime:: module has been equally crucial for us, enabling the development and deployment of custom controllers that enhance our infrastructure's automation and resilience. For instance, we've implemented controllers for billing management in our managed services, where watchers and finalizers on customer pods ensure accurate resource tracking and billing. This level of customization allows us to adapt Kubernetes to our specific needs.\n\nThrough kube-rs , Hugging Face has achieved a high level of efficiency, reliability, and control over our cloud-native applications. The library's Rust-centric design aligns with our engineering philosophy, allowing us to leverage Rust's strengths in managing Kubernetes resources. By automating critical tasks and building custom controllers, we've created a scalable, self-healing infrastructure that meets the diverse and evolving needs of our users and enterprise customers. This integration demonstrates our commitment to harnessing the full potential of Kubernetes while maintaining a development approach tailored to our unique requirements.\n\nWhile our infrastructure rarely encounters issues related to the Kubernetes API, we remain vigilant, especially during and after deployments. The Kubernetes API is a critical component in our use of kube::runtime:: for managing customer pods and cloud networking resources. Any disruptions or inefficiencies in API communication can have cascading effects on our services, potentially leading to downtime or degraded performance.\n\nThe importance of monitoring these API metrics is underscored by the experiences of other users of Kubernetes. OpenAI, for instance, shared a status update detailing how DNS availability issues resulted in significant downtime. While not directly related to the Kubernetes API, their experience highlights the interconnectedness of various infrastructure components and the potential for cascading failures. Just as DNS availability is vital for application accessibility, a healthy and responsive Kubernetes API is essential for managing and orchestrating our containerized workloads.\n\nAs a best practice, we've integrated these API metrics into our monitoring and alerting systems, ensuring that any anomalies or trends are promptly brought to our attention. This allows us to take a proactive approach, investigating and addressing issues before they impact our customers. For instance, on one occasion a single cluster started rate limiting requests to the Kubernetes API. We were able to trace this back to one of our third-party tools hitting a bug and repeatedly requesting a node be drained even though it had already done so. In response we were able to flush the malfunctioning job from the system before any noticeable degradation impacted our users. This is a great example that alerting scenarios do not just happen directly after deploying new versions of our custom controllers \u2014 bugs can take time to manifest as production issues.\n\nIn conclusion, while our infrastructure is robust and well-architected, we recognize that vigilance and proactive monitoring are essential to maintaining its health and stability. By keeping a close eye on the Kubernetes API error rate and rate limiting metrics, we safeguard our managed services, ensure smooth customer experiences, and uphold our commitment to reliability and performance. This is a testament to our belief that in the world of cloud-native technologies, every component, no matter how small, plays a significant role in the overall resilience and success of our platform.\n\nBonus Alert: New Cluster Sending Zero Metrics\n\nAnd a final bonus alert for reading through this far into the post!\n\nAt Hugging Face, our experiments are constantly in flux, often with purpose-fit clusters spinning up and down as we iterate on features and products. To add to the entropy, our growth is also a significant factor, with clusters expanding to their limits and triggering meiosis-like splits to maintain balance. To navigate this dynamic environment without resorting to hardcoding or introducing an additional cluster discovery layer, we've devised a clever alert that adapts to these changes.\n\n( ( sum by (cluster) (rate(container_network_transmit_packets_total{pod= \"prometheus\" }[1h] ) ) > 0) or (-1 * ( sum by (cluster) (rate(container_network_transmit_packets_total{pod= \"prometheus\" }[1h] offset 48h) ) > 0)) ) < 0\n\nThe metric used in this query is container_network_transmit_packets_total , which represents the total number of packets transmitted by a container. The query is filtering for metrics from a cluster\u2019s local Prometheus instance, which is tasked with metric collection as well as remote writing to our central metric store \u2014 Grafana Mimir. Transmission of packets approximates healthy remote writes, which is what we want to ensure across all active clusters.\n\nThe first part of the query performs a current rate check. The second part of the query performs a historical rate check by using the same calculation as the current rate check plus an offset 48h clause. The -1 * multiplication is used to invert the result, so that if the historical rate is greater than 0, the result will be less than 0.\n\nThe or operator combines the two parts of the query. The query will return true if either of the following conditions is met:\n\nThe current rate of packet transmission for a cluster is greater than 0.\n\nThe historical rate of packet transmission for a cluster (48 hours ago) is greater than 0, but the current rate is not.\n\nThe outer < 0 condition checks if the result of the or operation is less than 0. This means that the query will only trigger if neither of the conditions is met, i.e., if a cluster has never sent any metrics (both current and historical rates are 0).\n\nThere are two cases where the query will trigger:\n\nNew cluster with no metrics: A new cluster is added, but it has not sent any metrics yet. In this case, both the current and historical rates will be 0, and the query will trigger. Cluster that has never sent metrics: A cluster has been present for more than 48 hours, but it has never sent any metrics. In this case, the historical rate will be 0, and the current rate will also be 0, triggering the query.\n\nIn both cases, the query will detect that the cluster is not sending any metrics and trigger the alert.\n\nThis simple yet effective solution fires in scenarios where our metrics infrastructure crashes, during cluster setup, and when they're torn down providing us with timely insights into our infrastructure's health. While it may not be the most critical alert in our arsenal, it holds a special place since it was born out of collaboration. It is a testament to the power of teamwork through rigorous code review, made possible by the expertise and willingness to help of fellow colleagues in the Hugging Face infrastructure team \ud83e\udd17\n\nWrapping Up\n\nIn this post we shared some of our favourite alerts supporting infrastructure at Hugging Face. We'd love to hear your team's favourites as well!\n\nHow are you monitoring your ML infrastructure? Which alerts keep your team coming back for fixes? What breaks often in your infrastructure or conversely what have you never monitored and just works?\n\nShare with us in the comments below!",
    "link": "https://huggingface.co/blog/infrastructure-alerting",
    "Summary": "Three Mighty Alerts Supporting Hugging Face\u2019s Production InfrastructurePublished July 8, 2025 Update on GitHubThe Infrastructure team at Hugging Face is excited to share a behind-the-scenes look at the inner workings of Hugging Face's production infrastructure, which we\u2019ve had the privilege of helping to build and maintain.\nOn another occasion, adjustments to our infrastructure lead to mistakenly avoiding a private, low-cost path between product-specific infrastructure (ex.\nThe Glue table schema is periodically refreshed to ensure it remains up-to-date with the evolving structure of our log data.\nWe continuously refine and adapt our logging infrastructure to handle the ever-increasing volume and complexity of log data.\nThis integration streamlines the process of building and managing complex Kubernetes applications, making it a valuable tool in Hugging Face's infrastructure.",
    "Keywords": [
      "production",
      "data",
      "cluster",
      "alert",
      "hugging",
      "kubernetes",
      "infrastructure",
      "log",
      "traffic",
      "supporting",
      "logs",
      "query",
      "rate",
      "mighty",
      "faces",
      "alerts"
    ]
  },
  {
    "Title": "Efficient MultiModal Data Pipeline",
    "Authors": [],
    "Publish Date": null,
    "Text": "Efficient MultiModal Data Pipeline\n\nPublished July 8, 2025 Update on GitHub\n\nYou've got everything ready - data, model, a beefy GPU setup. You hit \"run\" and... wait. And wait some more. Your GPUs are barely breaking a sweat while your wallet's getting lighter by the hour.\n\nSound familiar? We've been there. After some detective work on our nanoVLM project, we discovered the real culprit wasn't our model or hardware, it was our data pipeline being incredibly wasteful.\n\nHere's what we found:\n\nIdle GPUs: Our model was literally waiting around for data to show up Padding hell: Every batch was stuffed with useless padding tokens that contributed nothing to training\n\nIn this post we build an efficient pipeline in five stages. In each stage we add or remove from the previous step and comment on what went right and what did not.\n\nTable of Contents:\n\n[Stage 0] Preparation\n\nTo make it easier to follow the data preparation tasks, we created a separate repo laser-focused on the data pipeline only. We hope this will be much easier to understand that reading the code once integrated with the nanoVLM repository. In addition, this could be useful to bootstrap other data pipelines!\n\nRepository: https://github.com/ariG23498/mmdp\n\nTo follow along, all you need to do is clone the repository. It contains the final data preparation tasks, but it's designed to showcase each step of the way.\n\n$ git clone https://github.com/ariG23498/mmdp.git\n\n[Stage 1] Visualising the Dataset\n\nBefore optimizing anything, we need to understand what we are working with. Our multimodal dataset has images, text prompts, and responses.\n\n$ uv run 01_check_dataset.py\n\nGetting familiar with your training data is crucial for success. The previous script shows a random sample each time you run it; you may want to copy the snippet to a notebook and run it multiple times to get a feeling about the data.\n\n[Stage 2] Naive Padding\n\nOur first training attempt used the obvious (and very frequent) approach:\n\nTokenize everything\n\nFind the longest sequence in each batch\n\nPad everything else to match\n\n$ uv run 02_naive_pad_dataloader.py\n\nThe results were painful. Look at this visualization:\n\nSee all that gray? That's padding. That's the GPU processing absolutely nothing while you pay for compute time. We were wasting roughly 60% of our batch on empty tokens.\n\n[Stage 3] Constrained Padding\n\nOur next move was simple. Set a global maximum length and stick to it. If a sample was too long, we'd just drop it.\n\nAs you might have noticed that the batch now has one sample less. This is due to the filtering process. This helped, but we were still padding everything to the same fixed length regardless of actual content. Better than before, but still wasteful.\n\n[Stage 4]: Packing Smarter with Knapsacks\n\nNow we\u2019re ready to rethink batching entirely. Padding is the enemy, and we need a strategy to minimize it while maximizing how much data we can fit into each batch. Enter the knapsack problem, a classic from computer science that\u2019s perfect for this.\n\nImagine you\u2019re packing a backpack for a hike. It can only hold so much weight, and you want to cram in as many useful items as possible. In our case:\n\nThe backpack is a training batch with a maximum token limit ( max_length ).\n\nis a training batch with a maximum token limit ( ). Each item is a sequence (a tokenized prompt-response pair), and its weight is the number of tokens.\n\nis a sequence (a tokenized prompt-response pair), and its is the number of tokens. Our goal is to pack as many sequences as possible into the batch without going over the token limit, minimizing wasted space.\n\nTo test this idea, we start with a toy dataset: just a list of numbers from 1 to 25, each representing a sequence length. This lets us experiment without the complexity of images and text.\n\nSwitching to an Iterable Dataset\n\nMost PyTorch datasets are map-style (you access them with dataset[i] ). But for dynamic batching, we need something more flexible. So, we built an iterable-style dataset by subclassing torch.utils.data.IterableDataset . This lets us generate batches on the fly and handle tricks like sharding data across multiple workers:\n\ndef _get_data_range ( self ): worker_info = get_worker_info() if worker_info is None : return self.start, self.end else : per_worker = int ( math.ceil((self.end - self.start) / worker_info.num_workers) ) worker_id = worker_info. id iter_start = self.start + worker_id * per_worker iter_end = min (iter_start + per_worker, self.end) return iter_start, iter_end\n\nProducer-Consumer Magic\n\nPacking sequences can be slow, especially if we\u2019re sorting or shuffling. To keep things moving, we use a producer-consumer pattern using Python queues:\n\ndef _producer ( self, data_iter, queue, stop_signal ): if self.strategy == \"greedy\" : for pack in self._greedy_packing(data_iter): queue.put(pack) elif self.strategy == \"binpack\" : while True : buffer = list (itertools.islice(data_iter, self.buffer_size)) if not buffer: break knapsacks = self._bin_packing(buffer) for pack in knapsacks: queue.put(pack) queue.put(stop_signal)\n\nThe producer thread packs batches and puts them in a queue, while the main thread pulls them out as needed. This overlap keeps the pipeline flowing smoothly.\n\nGreedy Packing\n\nFirst, we try a simple greedy packing strategy:\n\ndef _greedy_packing ( self, iterator ): pack, pack_sum = [], 0 for item in iterator: if item > self.max_length: continue if pack_sum + item <= self.max_length: pack.append(item) pack_sum += item else : yield pack pack = [item] pack_sum = item if pack: yield pack\n\nThis walks through the data sequentially, adding items to a pack until it\u2019s full, then starting a new one. It\u2019s fast but not perfect. Here\u2019s what the batches look like:\n\n=== Strategy: GREEDY === [tensor([1]), tensor([2]), tensor([3]), tensor([4]), tensor([5]), tensor([6]), tensor([7]), tensor([8]), tensor([9]), tensor([10]), tensor([11]), tensor([12]), tensor([13])] [tensor([14]), tensor([15]), tensor([16]), tensor([17]), tensor([18]), tensor([19])] [tensor([20]), tensor([21]), tensor([22]), tensor([23])] [tensor([24])]\n\nNotice how later batches get sparse? We\u2019re leaving gaps.\n\nBin-Packing for Tighter Fits\n\nLet\u2019s try a smarter approach: bin-packing (specifically, First Fit Decreasing):\n\ndef _bin_packing ( self, buffer: List [ int ] ): buffer = sorted (buffer, reverse= True ) knapsacks = [] for item in buffer: for pack in knapsacks: if sum (pack) + item <= self.max_length: pack.append(item) break else : knapsacks.append([item])\n\nThis sorts sequences by length (longest first) and tries to fit each one into the first pack that has room. If none fits, it starts a new pack. The result?\n\n=== Strategy: BINPACK === [tensor([24]), tensor([23]), tensor([22]), tensor([21]), tensor([10])] [tensor([20]), tensor([19]), tensor([18]), tensor([17]), tensor([16]), tensor([9]), tensor([1])] [tensor([15]), tensor([14]), tensor([13]), tensor([12]), tensor([11]), tensor([8]), tensor([7]), tensor([6]), tensor([5]), tensor([4]), tensor([3]), tensor([2])]\n\nThese batches are much tighter, with less wasted space. It\u2019s like playing Tetris with your data, fitting pieces together snugly.\n\n[Stage 5] Knapsacks for Multimodal Data\n\nNow for the real deal, applying knapsack packing to our multimodal dataset.\n\nWe\u2019re back to images, prompts, and responses, and we need to pack them efficiently while respecting both token limits and image budgets. Image budgeting is done so that images per sample are balanced. We would like to avoid the case where one GPU needs to process way more images than another.\n\nOur new ConstantLengthDataset class handles the heavy lifting. Here\u2019s how it works, compared to Stage 4:\n\nConcept Stage 4 (Toy Data) Stage 5 (Multimodal Data) Function(s) Item Integer (sequence length) Full sample (image, prompt, response) VQADataset.__getitem__ Weight The integer itself Number of tokens ( len(input_ids) ) \u2014 Knapsack Batch of integers \u2264 max_length Batch of samples \u2264 seq_length and image limit _balanced_greedy_knapsack Packing Strategy Greedy or Binpack Greedy packing with token and image constraints _balanced_greedy_knapsack Producer-Consumer Producer fills queue Same as the toy example, but with multimodal samples _producer , __iter__ Sample Filtering Skip integers > max_length Skip samples with too many tokens or images _producer Sharding Split integer range Shard dataset indices make_base_iterator() Batching Group integers Concatenate and align tokens/images _pack_one_group Output List of integers Dict with input_ids , labels , attention_mask , images yield from __iter__\n\nThe ConstantLengthDataset does it all:\n\nReads samples (images and text).\n\nFilters out samples that are too long or have too many images.\n\nPacks samples into batches using a greedy knapsack strategy, balancing token count and image count.\n\nPads the final batches to a fixed length, but with way less padding than before.\n\nHere\u2019s the result:\n\nLook at that! The gray (padding) is minimal, and the batches are dense with useful data. It\u2019s like packing a suitcase so well you can still zip it up without sitting on it.\n\nThe image might seem unintuive at the first glance, but let us look at the image side by side with constrained padding.\n\nKnapsack Constrained\n\nHere you will notice that the samples in knapsack are more evenly distributed. We also do not run into the issue of having less samples in the batch due to filtering.\n\nConclusion\n\nWhat started as a simple \"why is training so slow?\" investigation led to a complete rethink of how we handle multimodal data.\n\nThe balanced knapsack strategy for data pipeline comes from the Eagle 2: Building Post-Training Data Strategies from Scratch for Frontier Vision-Language Models paper from NVIDIA.\n\nThe key lessons:\n\nPadding everything to the longest sequences is a good first approach (but wasteful)\n\nThink of batching as a packing problem\n\nConsider all your constraints (text length, image memory, etc.)\n\nTest with toy data first to validate your approach\n\nWant to dig deeper? Check out:\n\nHappy training (and may your GPUs stay busy)!",
    "link": "https://huggingface.co/blog/mmdp",
    "Summary": "Efficient MultiModal Data PipelinePublished July 8, 2025 Update on GitHubYou've got everything ready - data, model, a beefy GPU setup.\nAfter some detective work on our nanoVLM project, we discovered the real culprit wasn't our model or hardware, it was our data pipeline being incredibly wasteful.\nTable of Contents:[Stage 0] PreparationTo make it easier to follow the data preparation tasks, we created a separate repo laser-focused on the data pipeline only.\n[Stage 5] Knapsacks for Multimodal DataNow for the real deal, applying knapsack packing to our multimodal dataset.\nThe balanced knapsack strategy for data pipeline comes from the Eagle 2: Building Post-Training Data Strategies from Scratch for Frontier Vision-Language Models paper from NVIDIA.",
    "Keywords": [
      "data",
      "images",
      "image",
      "pack",
      "multimodal",
      "batch",
      "samples",
      "efficient",
      "item",
      "packing",
      "batches",
      "pipeline",
      "padding"
    ]
  },
  {
    "Title": "Training and Finetuning Sparse Embedding Models with Sentence Transformers v5",
    "Authors": [],
    "Publish Date": null,
    "Text": "Training and Finetuning Sparse Embedding Models with Sentence Transformers v5\n\nPublished July 1, 2025 Update on GitHub\n\nv3.0: (improved) Sentence Transformer (Dense Embedding) model training\n\nv4.0: (improved) Cross Encoder (Reranker) model training\n\nv5.0: (new) Sparse Embedding model training\n\nSentence Transformers is a Python library for using and training embedding and reranker models for a wide range of applications, such as retrieval augmented generation, semantic search, semantic textual similarity, paraphrase mining, and more. The last few major versions have introduced significant improvements to training:\n\nIn this blogpost, I'll show you how to use it to finetune a sparse encoder/embedding model and explain why you might want to do so. This results in sparse-encoder/example-inference-free-splade-distilbert-base-uncased-nq, a cheap model that works especially well in hybrid search or retrieve and rerank scenarios.\n\nFinetuning sparse embedding models involves several components: the model, datasets, loss functions, training arguments, evaluators, and the trainer class. I'll have a look at each of these components, accompanied by practical examples of how they can be used for finetuning strong sparse embedding models.\n\nIn addition to training your own models, you can choose from a wide range of pretrained sparse encoders available on the Hugging Face Hub. To help navigate this growing space, we\u2019ve curated a SPLADE Models collection highlighting some of the most relevant models.\n\nWe list the most prominent ones along with their benchmark results in Pretrained Models in the documentation.\n\nTable of Contents\n\nWhat are Sparse Embedding models?\n\nThe broader term \"embedding models\" refer to models that convert some input, usually text, into a vector representation (embedding) that captures the semantic meaning of the input. Unlike with the raw inputs, you can perform mathematical operations on these embeddings, resulting in similarity scores that can be used for various tasks, such as search, clustering, or classification.\n\nWith dense embedding models, i.e. the common variety, the embeddings are typically low-dimensional vectors (e.g., 384, 768, or 1024 dimensions) where most values are non-zero. Sparse embedding models, on the other hand, produce high-dimensional vectors (e.g., 30,000+ dimensions) where most values are zero. Usually, each active dimension (i.e. the dimension with a non-zero value) in a sparse embedding corresponds to a specific token in the model's vocabulary, allowing for interpretability.\n\nLet's have a look at naver/splade-v3, a state-of-the-art sparse embedding model, as an example:\n\nfrom sentence_transformers import SparseEncoder model = SparseEncoder( \"naver/splade-v3\" ) sentences = [ \"The weather is lovely today.\" , \"It's so sunny outside!\" , \"He drove to the stadium.\" , ] embeddings = model.encode(sentences) print (embeddings.shape) similarities = model.similarity(embeddings, embeddings) print (similarities) decoded = model.decode(embeddings, top_k= 10 ) for decoded, sentence in zip (decoded, sentences): print ( f\"Sentence: {sentence} \" ) print ( f\"Decoded: {decoded} \" ) print ()\n\nSentence: The weather is lovely today. Decoded: [('weather', 2.754288673400879), ('today', 2.610959529876709), ('lovely', 2.431990623474121), ('currently', 1.5520408153533936), ('beautiful', 1.5046082735061646), ('cool', 1.4664798974990845), ('pretty', 0.8986214995384216), ('yesterday', 0.8603134155273438), ('nice', 0.8322536945343018), ('summer', 0.7702118158340454)] Sentence: It's so sunny outside! Decoded: [('outside', 2.6939032077789307), ('sunny', 2.535827398300171), ('so', 2.0600898265838623), ('out', 1.5397940874099731), ('weather', 1.1198079586029053), ('very', 0.9873268604278564), ('cool', 0.9406591057777405), ('it', 0.9026399254798889), ('summer', 0.684999406337738), ('sun', 0.6520509123802185)] Sentence: He drove to the stadium. Decoded: [('stadium', 2.7872302532196045), ('drove', 1.8208855390548706), ('driving', 1.6665740013122559), ('drive', 1.5565159320831299), ('he', 1.4721972942352295), ('stadiums', 1.449463129043579), ('to', 1.0441515445709229), ('car', 0.7002660632133484), ('visit', 0.5118278861045837), ('football', 0.502326250076294)]\n\nIn this example, the embeddings are 30,522-dimensional vectors, where each dimension corresponds to a token in the model's vocabulary. The decode method returned the top 10 tokens with the highest values in the embedding, allowing us to interpret which tokens contribute most to the embedding.\n\nWe can even determine the intersection or overlap between embeddings, very useful for determining why two texts are deemed similar or dissimilar:\n\nintersection_embedding = model.intersection(embeddings[ 0 ], embeddings[ 1 ]) decoded_intersection = model.decode(intersection_embedding) print (decoded_intersection)\n\nDecoded: [('weather', 3.0842742919921875), ('cool', 1.379457712173462), ('summer', 0.5275946259498596), ('comfort', 0.3239051103591919), ('sally', 0.22571465373039246), ('julian', 0.14787325263023376), ('nature', 0.08582140505313873), ('beauty', 0.0588383711874485), ('mood', 0.018594780936837196), ('nathan', 0.000752730411477387)]\n\nQuery and Document Expansion\n\nA key component of neural sparse embedding models is query/document expansion. Unlike traditional lexical methods like BM25, which only match exact tokens, neural sparse models generally automatically expand the original text with semantically related terms:\n\nTraditional, Lexical (e.g. BM25): Only matches on exact tokens in the text\n\nOnly matches on exact tokens in the text Neural Sparse Models: Automatically expand with related terms\n\nFor example, in the code output above, the sentence \"The weather is lovely today\" is expanded to include terms like \"beautiful\", \"cool\", \"pretty\", and \"nice\" which weren't in the original text. Similarly, \"It's so sunny outside!\" is expanded to include \"weather\", \"summer\", and \"sun\".\n\nThis expansion allows neural sparse models to match semantically related content or synonyms even without exact token matches, handle misspellings, and overcome vocabulary mismatch problems. This is why neural sparse models like SPLADE often outperform traditional lexical search methods while maintaining the efficiency benefits of sparse representations.\n\nHowever, expansion has its risks. For example, query expansion for \"What is the weather on Tuesday?\" will likely also expand to \"monday\", \"wednesday\", etc., which may not be desired.\n\nWhy Use Sparse Embedding Models?\n\nIn short, neural sparse embedding models fall in a valuable niche between traditional lexical methods like BM25 and dense embedding models like Sentence Transformers. They have the following advantages:\n\nHybrid potential: Very effectively combined with dense models, which may struggle with searches where lexical matches are important\n\nVery effectively combined with dense models, which may struggle with searches where lexical matches are important Interpretability: You can see exactly which tokens contribute to a match\n\nYou can see exactly which tokens contribute to a match Performance: Competitive or better than dense models in many retrieval tasks\n\nThroughout this blogpost, I'll use \"sparse embedding model\" and \"sparse encoder model\" interchangeably.\n\nWhy Finetune?\n\nThe majority of (neural) sparse embedding models employ the aforementioned query/document expansion so that you can match texts with nearly identical meaning, even if they don't share any words. In short, the model has to recognize synonyms so those tokens can be placed in the final embedding.\n\nMost out-of-the-box sparse embedding models will easily recognize that \"supermarket\", \"food\", and \"market\" are useful expansions of a text containing \"grocery\", but for example:\n\n\"The patient complained of severe cephalalgia.\"\n\nexpands to:\n\n'##lal', 'severe', '##pha', 'ce', '##gia', 'patient', 'complaint', 'patients', 'complained', 'warning', 'suffered', 'had', 'disease', 'complain', 'diagnosis', 'syndrome', 'mild', 'pain', 'hospital', 'injury'\n\nwhereas we wish for it to expand to \"headache\", the common word for \"cephalalgia\". This example expands to many domains, e.g. not recognizing that \"Java\" is a programming language, that \"Audi\" makes cars, or that \"NVIDIA\" is a company that makes graphics cards.\n\nThrough finetuning, the model can learn to focus exclusively on the domain and/or language that matters to you.\n\nTraining Components\n\nTraining Sentence Transformer models involves the following components:\n\nModel: The model to train or finetune, which can be a pre-trained Sparse Encoder model or a base model. Dataset: The data used for training and evaluation. Loss Function: A function that quantifies the model's performance and guides the optimization process. Training Arguments (optional): Parameters that influence training performance and tracking/debugging. Evaluator (optional): A tool for evaluating the model before, during, or after training. Trainer: Brings together the model, dataset, loss function, and other components for training.\n\nNow, let's dive into each of these components in more detail.\n\nModel\n\nSparse Encoder models consist of a sequence of Modules, Sparse Encoder specific Modules or Custom Modules, allowing for a lot of flexibility. If you want to further finetune a Sparse Encoder model (e.g. it has a modules.json file), then you don't have to worry about which modules are used:\n\nfrom sentence_transformers import SparseEncoder model = SparseEncoder( \"naver/splade-cocondenser-ensembledistil\" )\n\nBut if instead you want to train from another checkpoint, or from scratch, then these are the most common architectures you can use:\n\nSplade\n\nSplade models use the MLMTransformer followed by a SpladePooling modules. The former loads a pretrained Masked Language Modeling transformer model (e.g. BERT, RoBERTa, DistilBERT, ModernBERT, etc.) and the latter pools the output of the MLMHead to produce a single sparse embedding of the size of the vocabulary.\n\nfrom sentence_transformers import models, SparseEncoder from sentence_transformers.sparse_encoder.models import MLMTransformer, SpladePooling mlm_transformer = MLMTransformer( \"google-bert/bert-base-uncased\" ) splade_pooling = SpladePooling(pooling_strategy= \"max\" ) model = SparseEncoder(modules=[mlm_transformer, splade_pooling])\n\nThis architecture is the default if you provide a fill-mask model architecture to SparseEncoder, so it's easier to use the shortcut:\n\nfrom sentence_transformers import SparseEncoder model = SparseEncoder( \"google-bert/bert-base-uncased\" )\n\nInference-free Splade\n\nInference-free Splade uses a Router module with different modules for queries and documents. Usually for this type of architecture, the documents part is a traditional Splade architecture (a MLMTransformer followed by a SpladePooling module) and the query part is an SparseStaticEmbedding module, which just returns a pre-computed score for every token in the query.\n\nfrom sentence_transformers import SparseEncoder from sentence_transformers.models import Router from sentence_transformers.sparse_encoder.models import SparseStaticEmbedding, MLMTransformer, SpladePooling doc_encoder = MLMTransformer( \"google-bert/bert-base-uncased\" ) router = Router.for_query_document( query_modules=[SparseStaticEmbedding(tokenizer=doc_encoder.tokenizer, frozen= False )], document_modules=[doc_encoder, SpladePooling( \"max\" )], ) model = SparseEncoder(modules=[router], similarity_fn_name= \"dot\" )\n\nThis architecture allows for fast query-time processing using the lightweight SparseStaticEmbedding approach, that can be trained and seen as a linear weights, while documents are processed with the full MLM transformer and SpladePooling.\n\nInference-free Splade is particularly useful for search applications where query latency is critical, as it shifts the computational complexity to the document indexing phase which can be done offline.\n\nWhen training models with the Router module, you must use the router_mapping argument in the SparseEncoderTrainingArguments to map the training dataset columns to the correct route (\"query\" or \"document\"). For example, if your dataset(s) have [\"question\", \"answer\"] columns, then you can use the following mapping: args = SparseEncoderTrainingArguments( ..., router_mapping={ \"question\" : \"query\" , \"answer\" : \"document\" , } ) Additionally, it is recommended to use a much higher learning rate for the SparseStaticEmbedding module than for the rest of the model. For this, you should use the learning_rate_mapping argument in the SparseEncoderTrainingArguments to map parameter patterns to their learning rates. For example, if you want to use a learning rate of 1e-3 for the SparseStaticEmbedding module and 2e-5 for the rest of the model, you can do this: args = SparseEncoderTrainingArguments( ..., learning_rate= 2e-5 , learning_rate_mapping={ r\"SparseStaticEmbedding\\.*\" : 1e-3 , } )\n\nContrastive Sparse Representation (CSR)\n\nContrastive Sparse Representation (CSR) models, introduced in Beyond Matryoshka: Revisiting Sparse Coding for Adaptive Representation, apply a SparseAutoEncoder module on top of a dense Sentence Transformer model, which usually consist of a Transformer followed by a Pooling module. You can initialize one from scratch like so:\n\nfrom sentence_transformers import models, SparseEncoder from sentence_transformers.sparse_encoder.models import SparseAutoEncoder transformer = models.Transformer( \"google-bert/bert-base-uncased\" ) pooling = models.Pooling(transformer.get_word_embedding_dimension(), pooling_mode= \"mean\" ) sparse_auto_encoder = SparseAutoEncoder( input_dim=transformer.get_word_embedding_dimension(), hidden_dim= 4 * transformer.get_word_embedding_dimension(), k= 256 , k_aux= 512 , ) model = SparseEncoder(modules=[transformer, pooling, sparse_auto_encoder])\n\nOr if your base model is 1) a dense Sentence Transformer model or 2) a non-MLM Transformer model (those are loaded as Splade models by default), then this shortcut will automatically initialize the CSR model for you:\n\nfrom sentence_transformers import SparseEncoder model = SparseEncoder( \"mixedbread-ai/mxbai-embed-large-v1\" )\n\nUnlike (Inference-free) Splade models, sparse embeddings by CSR models don't have the same size as the vocabulary of the base model. This means you can't directly interpret which words are activated in your embedding like you can with Splade models, where each dimension corresponds to a specific token in the vocabulary. Beyond that, CSR models are most effective on dense encoder models that use high-dimensional representations (e.g. 1024-4096 dimensions).\n\nArchitecture Picker Guide\n\nIf you're unsure which architecture to use, here's a quick guide:\n\nDo you want to sparsify an existing Dense Embedding model? If yes, use CSR .\n\n. Do you want your query inference to be instantaneous at the cost of slight performance? If yes, use Inference-free SPLADE .\n\n. Otherwise, use SPLADE.\n\nDataset\n\nThe SparseEncoderTrainer uses datasets.Dataset or datasets.DatasetDict instances for training and evaluation. You can load data from the Hugging Face Datasets Hub or use local data in various formats such as CSV, JSON, Parquet, Arrow, or SQL.\n\nNote: Lots of public datasets that work out of the box with Sentence Transformers have been tagged with sentence-transformers on the Hugging Face Hub, so you can easily find them on https://huggingface.co/datasets?other=sentence-transformers. Consider browsing through these to find ready-to-go datasets that might be useful for your tasks, domains, or languages.\n\nData on the Hugging Face Hub\n\nYou can use the load_dataset function to load data from datasets in the Hugging Face Hub\n\nfrom datasets import load_dataset train_dataset = load_dataset( \"sentence-transformers/natural-questions\" , split= \"train\" ) print (train_dataset) \"\"\" Dataset({ features: ['query', 'answer'], num_rows: 100231 }) \"\"\"\n\nSome datasets, like nthakur/swim-ir-monolingual , have multiple subsets with different data formats. You need to specify the subset name along with the dataset name, e.g. dataset = load_dataset(\"nthakur/swim-ir-monolingual\", \"de\", split=\"train\") .\n\nLocal Data (CSV, JSON, Parquet, Arrow, SQL)\n\nYou can also use load_dataset for loading local data in certain file formats:\n\nfrom datasets import load_dataset dataset = load_dataset( \"csv\" , data_files= \"my_file.csv\" ) dataset = load_dataset( \"json\" , data_files= \"my_file.json\" )\n\nLocal Data that requires pre-processing\n\nYou can use datasets.Dataset.from_dict if your local data requires pre-processing. This allows you to initialize your dataset with a dictionary of lists:\n\nfrom datasets import Dataset queries = [] documents = [] dataset = Dataset.from_dict({ \"query\" : queries, \"document\" : documents, })\n\nEach key in the dictionary becomes a column in the resulting dataset.\n\nDataset Format\n\nIt's crucial to ensure that your dataset format matches your chosen loss function. This involves checking two things:\n\nIf your loss function requires a Label (as indicated in the Loss Overview table), your dataset must have a column named \"label\" or \"score\". All columns other than \"label\" or \"score\" are considered Inputs (as indicated in the Loss Overview table). The number of these columns must match the number of valid inputs for your chosen loss function. The names of the columns don't matter, only their order matters.\n\nFor example, if your loss function accepts (anchor, positive, negative) triplets , then your first, second, and third dataset columns correspond with anchor , positive , and negative , respectively. This means that your first and second column must contain texts that should embed closely, and that your first and third column must contain texts that should embed far apart. That is why depending on your loss function, your dataset column order matters.\n\nConsider a dataset with columns [\"text1\", \"text2\", \"label\"] , where the \"label\" column contains floating point similarity scores. This dataset can be used with SparseCoSENTLoss , SparseAnglELoss , and SparseCosineSimilarityLoss because:\n\nThe dataset has a \"label\" column, which is required by these loss functions. The dataset has 2 non-label columns, matching the number of inputs required by these loss functions.\n\nIf the columns in your dataset are not ordered correctly, use Dataset.select_columns to reorder them. Additionally, remove any extraneous columns (e.g., sample_id , metadata , source , type ) using Dataset.remove_columns , as they will be treated as inputs otherwise.\n\nLoss Function\n\nLoss functions measure how well a model performs on a given batch of data and guide the optimization process. The choice of loss function depends on your available data and target task. Refer to the Loss Overview for a comprehensive list of options.\n\nTo train a SparseEncoder , you either need a SpladeLoss or CSRLoss , depending on the architecture. These are wrapper losses that add sparsity regularization on top of a main loss function, which must be provided as a parameter. The only loss that can be used independently is SparseMSELoss , as it performs embedding-level distillation, ensuring sparsity by directly copying the teacher's sparse embedding.\n\nMost loss functions can be initialized with just the SparseEncoder that you're training, alongside some optional parameters, e.g.:\n\nfrom datasets import load_dataset from sentence_transformers import SparseEncoder from sentence_transformers.sparse_encoder.losses import SpladeLoss, SparseMultipleNegativesRankingLoss model = SparseEncoder( \"distilbert/distilbert-base-uncased\" ) loss = SpladeLoss( model=model, loss=SparseMultipleNegativesRankingLoss(model=model), query_regularizer_weight= 5e-5 , document_regularizer_weight= 3e-5 , ) train_dataset = load_dataset( \"sentence-transformers/natural-questions\" , split= \"train\" ) print (train_dataset) \"\"\" Dataset({ features: ['query', 'answer'], num_rows: 100231 }) \"\"\"\n\nDocumentation\n\nTraining Arguments\n\nThe SparseEncoderTrainingArguments class allows you to specify parameters that influence training performance and tracking/debugging. While optional, experimenting with these arguments can help improve training efficiency and provide insights into the training process.\n\nIn the Sentence Transformers documentation, I've outlined some of the most useful training arguments. I would recommend reading it in Training Overview > Training Arguments.\n\nHere's an example of how to initialize SparseEncoderTrainingArguments :\n\nfrom sentence_transformers import SparseEncoderTrainingArguments args = SparseEncoderTrainingArguments( output_dir= \"models/splade-distilbert-base-uncased-nq\" , num_train_epochs= 1 , per_device_train_batch_size= 16 , per_device_eval_batch_size= 16 , learning_rate= 2e-5 , warmup_ratio= 0.1 , fp16= True , bf16= False , batch_sampler=BatchSamplers.NO_DUPLICATES, eval_strategy= \"steps\" , eval_steps= 100 , save_strategy= \"steps\" , save_steps= 100 , save_total_limit= 2 , logging_steps= 100 , run_name= \"splade-distilbert-base-uncased-nq\" , )\n\nNote that eval_strategy was introduced in transformers version 4.41.0 . Prior versions should use evaluation_strategy instead.\n\nEvaluator\n\nYou can provide the SparseEncoderTrainer with an eval_dataset to get the evaluation loss during training, but it may be useful to get more concrete metrics during training, too. For this, you can use evaluators to assess the model's performance with useful metrics before, during, or after training. You can use both an eval_dataset and an evaluator, one or the other, or neither. They evaluate based on the eval_strategy and eval_steps Training Arguments.\n\nHere are the implemented Evaluators that come with Sentence Transformers for Sparse Encoder models:\n\nEvaluator Required Data SparseBinaryClassificationEvaluator Pairs with class labels. SparseEmbeddingSimilarityEvaluator Pairs with similarity scores. SparseInformationRetrievalEvaluator Queries (qid => question), Corpus (cid => document), and relevant documents (qid => set[cid]). SparseNanoBEIREvaluator No data required. SparseMSEEvaluator Source sentences to embed with a teacher model and target sentences to embed with the student model. Can be the same texts. SparseRerankingEvaluator List of {'query': '...', 'positive': [...], 'negative': [...]} dictionaries. SparseTranslationEvaluator Pairs of sentences in two separate languages. SparseTripletEvaluator (anchor, positive, negative) pairs.\n\nAdditionally, SequentialEvaluator should be used to combine multiple evaluators into one Evaluator that can be passed to the SparseEncoderTrainer .\n\nSometimes you don't have the required evaluation data to prepare one of these evaluators on your own, but you still want to track how well the model performs on some common benchmarks. In that case, you can use these evaluators with data from Hugging Face.\n\nSparseNanoBEIREvaluator\n\nDocumentation\n\nfrom sentence_transformers.sparse_encoder.evaluation import SparseNanoBEIREvaluator dev_evaluator = SparseNanoBEIREvaluator()\n\nSparseEmbeddingSimilarityEvaluator with STSb\n\nDocumentation\n\nfrom datasets import load_dataset from sentence_transformers.evaluation import SimilarityFunction from sentence_transformers.sparse_encoder.evaluation import SparseEmbeddingSimilarityEvaluator eval_dataset = load_dataset( \"sentence-transformers/stsb\" , split= \"validation\" ) dev_evaluator = SparseEmbeddingSimilarityEvaluator( sentences1=eval_dataset[ \"sentence1\" ], sentences2=eval_dataset[ \"sentence2\" ], scores=eval_dataset[ \"score\" ], main_similarity=SimilarityFunction.COSINE, name= \"sts-dev\" , )\n\nSparseTripletEvaluator with AllNLI\n\nDocumentation\n\nfrom datasets import load_dataset from sentence_transformers.evaluation import SimilarityFunction from sentence_transformers.sparse_encoder.evaluation import SparseTripletEvaluator max_samples = 1000 eval_dataset = load_dataset( \"sentence-transformers/all-nli\" , \"triplet\" , split= f\"dev[: {max_samples} ]\" ) dev_evaluator = SparseTripletEvaluator( anchors=eval_dataset[ \"anchor\" ], positives=eval_dataset[ \"positive\" ], negatives=eval_dataset[ \"negative\" ], main_distance_function=SimilarityFunction.DOT, name= \"all-nli-dev\" , )\n\nWhen evaluating frequently during training with a small eval_steps , consider using a tiny eval_dataset to minimize evaluation overhead. If you're concerned about the evaluation set size, a 90-1-9 train-eval-test split can provide a balance, reserving a reasonably sized test set for final evaluations. After training, you can assess your model's performance using trainer.evaluate(test_dataset) for test loss or initialize a testing evaluator with test_evaluator(model) for detailed test metrics. If you evaluate after training, but before saving the model, your automatically generated model card will still include the test results.\n\nWhen using Distributed Training, the evaluator only runs on the first device, unlike the training and evaluation datasets, which are shared across all devices.\n\nTrainer\n\nThe SparseEncoderTrainer is where all previous components come together. We only have to specify the trainer with the model, training arguments (optional), training dataset, evaluation dataset (optional), loss function, evaluator (optional) and we can start training. Let\u2019s have a look at a script where all of these components come together:\n\nimport logging from datasets import load_dataset from sentence_transformers import ( SparseEncoder, SparseEncoderModelCardData, SparseEncoderTrainer, SparseEncoderTrainingArguments, ) from sentence_transformers.models import Router from sentence_transformers.sparse_encoder.evaluation import SparseNanoBEIREvaluator from sentence_transformers.sparse_encoder.losses import SparseMultipleNegativesRankingLoss, SpladeLoss from sentence_transformers.sparse_encoder.models import SparseStaticEmbedding, MLMTransformer, SpladePooling from sentence_transformers.training_args import BatchSamplers logging.basicConfig( format = \"%(asctime)s - %(message)s\" , datefmt= \"%Y-%m-%d %H:%M:%S\" , level=logging.INFO) mlm_transformer = MLMTransformer( \"distilbert/distilbert-base-uncased\" , tokenizer_args={ \"model_max_length\" : 512 }) splade_pooling = SpladePooling( pooling_strategy= \"max\" , word_embedding_dimension=mlm_transformer.get_sentence_embedding_dimension() ) router = Router.for_query_document( query_modules=[SparseStaticEmbedding(tokenizer=mlm_transformer.tokenizer, frozen= False )], document_modules=[mlm_transformer, splade_pooling], ) model = SparseEncoder( modules=[router], model_card_data=SparseEncoderModelCardData( language= \"en\" , license= \"apache-2.0\" , model_name= \"Inference-free SPLADE distilbert-base-uncased trained on Natural-Questions tuples\" , ), ) full_dataset = load_dataset( \"sentence-transformers/natural-questions\" , split= \"train\" ).select( range ( 100_000 )) dataset_dict = full_dataset.train_test_split(test_size= 1_000 , seed= 12 ) train_dataset = dataset_dict[ \"train\" ] eval_dataset = dataset_dict[ \"test\" ] print (train_dataset) print (train_dataset[ 0 ]) loss = SpladeLoss( model=model, loss=SparseMultipleNegativesRankingLoss(model=model), query_regularizer_weight= 0 , document_regularizer_weight= 3e-3 , ) run_name = \"inference-free-splade-distilbert-base-uncased-nq\" args = SparseEncoderTrainingArguments( output_dir= f\"models/ {run_name} \" , num_train_epochs= 1 , per_device_train_batch_size= 16 , per_device_eval_batch_size= 16 , learning_rate= 2e-5 , learning_rate_mapping={ r\"SparseStaticEmbedding\\.weight\" : 1e-3 }, warmup_ratio= 0.1 , fp16= True , bf16= False , batch_sampler=BatchSamplers.NO_DUPLICATES, router_mapping={ \"query\" : \"query\" , \"answer\" : \"document\" }, eval_strategy= \"steps\" , eval_steps= 1000 , save_strategy= \"steps\" , save_steps= 1000 , save_total_limit= 2 , logging_steps= 200 , run_name=run_name, ) dev_evaluator = SparseNanoBEIREvaluator(dataset_names=[ \"msmarco\" , \"nfcorpus\" , \"nq\" ], batch_size= 16 ) trainer = SparseEncoderTrainer( model=model, args=args, train_dataset=train_dataset, eval_dataset=eval_dataset, loss=loss, evaluator=dev_evaluator, ) trainer.train() dev_evaluator(model) model.save_pretrained( f\"models/ {run_name} /final\" ) model.push_to_hub(run_name)\n\nIn this example I'm finetuning from distilbert/distilbert-base-uncased , a base model that is not yet a Sparse Encoder model. This requires more training data than finetuning an existing Sparse Encoder model, like naver/splade-cocondenser-ensembledistil .\n\nAfter running this script, the sparse-encoder/example-inference-free-splade-distilbert-base-uncased-nq model was uploaded for me. The model scores 0.5241 NDCG@10 on NanoMSMARCO, 0.3299 NDCG@10 on NanoNFCorpus and 0.5357 NDCG@10 NanoNQ, which is a good result for an inference-free distilbert-based model trained on just 100k pairs from the Natural Questions dataset.\n\nThe model uses an average of 184 active dimensions in the sparse embeddings for the documents, compared to 7.7 active dimensions for the queries (i.e. the average number of tokens in the query). This corresponds to a sparsity of 99.39% and 99.97%, respectively.\n\nAll of this information is stored in the automatically generated model card, including the base model, language, license, evaluation results, training & evaluation dataset info, hyperparameters, training logs, and more. Without any effort, your uploaded models should contain all the information that your potential users would need to determine whether your model is suitable for them.\n\nCallbacks\n\nThe Sentence Transformers trainer supports various transformers.TrainerCallback subclasses, including:\n\nWandbCallback for logging training metrics to W&B if wandb is installed\n\nfor logging training metrics to W&B if is installed TensorBoardCallback for logging training metrics to TensorBoard if tensorboard is accessible\n\nfor logging training metrics to TensorBoard if is accessible CodeCarbonCallback for tracking carbon emissions during training if codecarbon is installed\n\nThese are automatically used without you having to specify anything, as long as the required dependency is installed.\n\nRefer to the Transformers Callbacks documentation for more information on these callbacks and how to create your own.\n\nMulti-Dataset Training\n\nTop-performing models are often trained using multiple datasets simultaneously. The SparseEncoderTrainer simplifies this process by allowing you to train with multiple datasets without converting them to the same format. You can even apply different loss functions to each dataset. Here are the steps for multi-dataset training:\n\nUse a dictionary of datasets.Dataset instances (or a datasets.DatasetDict ) as the train_dataset and eval_dataset . (Optional) Use a dictionary of loss functions mapping dataset names to losses if you want to use different losses for different datasets.\n\nEach training/evaluation batch will contain samples from only one of the datasets. The order in which batches are sampled from the multiple datasets is determined by the MultiDatasetBatchSamplers enum, which can be passed to the SparseEncoderTrainingArguments via multi_dataset_batch_sampler . The valid options are:\n\nMultiDatasetBatchSamplers.ROUND_ROBIN : Samples from each dataset in a round-robin fashion until one is exhausted. This strategy may not use all samples from each dataset, but it ensures equal sampling from each dataset.\n\n: Samples from each dataset in a round-robin fashion until one is exhausted. This strategy may not use all samples from each dataset, but it ensures equal sampling from each dataset. MultiDatasetBatchSamplers.PROPORTIONAL (default): Samples from each dataset proportionally to its size. This strategy ensures that all samples from each dataset are used, and larger datasets are sampled from more frequently.\n\nEvaluation\n\nLet's evaluate our newly trained inference-free SPLADE model using the NanoMSMARCO dataset, and see how it compares to dense retrieval approaches. We'll also explore hybrid retrieval methods that combine sparse and dense vectors, as well as reranking to further improve search quality.\n\nAfter running a slightly modified version of our hybrid_search.py script, we get the following results for the NanoMSMARCO dataset, using these models:\n\nSparse : sparse-encoder/example-inference-free-splade-distilbert-base-uncased-nq (the model we just trained)\n\n: (the model we just trained) Dense : sentence-transformers/all-MiniLM-L6-v2\n\n: Reranker: cross-encoder/ms-marco-MiniLM-L6-v2\n\nSparse Dense Reranker NDCG@10 MRR@10 MAP x 52.41 43.06 44.20 x 55.40 47.96 49.08 x x 62.22 53.02 53.44 x x 66.31 59.45 60.36 x x 66.28 59.43 60.34 x x x 66.28 59.43 60.34\n\nThe Sparse and Dense rankings can be combined using Reciprocal Rank Fusion (RRF), which is a simple way to combine the results of multiple rankings. If a Reranker is applied, it will rerank the results of the prior retrieval step.\n\nThe results indicate that for this dataset, combining Dense and Sparse rankings is very performant, resulting in 12.3% and 18.7% increases over the Dense and Sparse baselines, respectively. In short, combining Sparse and Dense retrieval methods is a very effective way to improve search performance.\n\nFurthermore, applying a reranker on any of the rankings improved the performance to approximately 66.3 NDCG@10, showing that either Sparse, Dense, or Hybrid (Dense + Sparse) found the relevant documents in their top 100, which the reranker then ranked to the top 10. So, replacing a Dense -> Reranker pipeline with a Sparse -> Reranker pipeline might improve both latency and costs:\n\nSparse embeddings can be cheaper to store, e.g. our model only uses ~180 active dimensions for MS MARCO documents instead of the common 1024 dimensions for dense models.\n\nSome Sparse Encoders allow for inference-free query processing, allowing for a near-instant first-stage retrieval, akin to lexical solutions like BM25.\n\nTraining Tips\n\nSparse Encoder models have a few quirks that you should be aware of when training them:\n\nSparse Encoder models should not be evaluated solely using the evaluation scores, but also with the sparsity of the embeddings. After all, a low sparsity means that the model embeddings are expensive to store and slow to retrieve. The stronger Sparse Encoder models are trained almost exclusively with distillation from a stronger teacher model (e.g. a CrossEncoder model), instead of training directly from text pairs or triplets. See for example the SPLADE-v3 paper, which uses SparseDistillKLDivLoss and SparseMarginMSELoss for distillation. We don't cover this in detail in this blog as it requires more data preparation, but a distillation setup should be seriously considered.\n\nVector Database Integration\n\nAfter training sparse embedding models, the next crucial step is deploying them effectively in production environments. Vector databases provide the essential infrastructure for storing, indexing, and retrieving sparse embeddings at scale. Popular options include Qdrant, OpenSearch, Elasticsearch, and Seismic, among others.\n\nFor comprehensive examples covering vector databases mentioned above, refer to the semantic search with vector database documentation or below for the Qdrant example.\n\nQdrant Integration Example\n\nQdrant offers excellent support for sparse vectors with efficient storage and fast retrieval capabilities. Below is a comprehensive implementation example:\n\nQdrant running locally (or accessible), see the Qdrant Quickstart for more details.\n\nPython Qdrant Client installed: pip install qdrant-client\n\nThis example demonstrates how to set up Qdrant for sparse vector search by showing how to efficiently encode and index documents with sparse encoders, formulating search queries with sparse vectors, and providing an interactive query interface. See below:\n\nimport time from datasets import load_dataset from sentence_transformers import SparseEncoder from sentence_transformers.sparse_encoder.search_engines import semantic_search_qdrant dataset = load_dataset( \"sentence-transformers/natural-questions\" , split= \"train\" ) num_docs = 10_000 corpus = dataset[ \"answer\" ][:num_docs] queries = dataset[ \"query\" ][: 2 ] sparse_model = SparseEncoder( \"naver/splade-cocondenser-ensembledistil\" ) corpus_embeddings = sparse_model.encode_document( corpus, convert_to_sparse_tensor= True , batch_size= 16 , show_progress_bar= True ) corpus_index = None while True : start_time = time.time() query_embeddings = sparse_model.encode_query(queries, convert_to_sparse_tensor= True ) print ( f\"Encoding time: {time.time() - start_time: .6 f} seconds\" ) results, search_time, corpus_index = semantic_search_qdrant( query_embeddings, corpus_index=corpus_index, corpus_embeddings=corpus_embeddings if corpus_index is None else None , top_k= 5 , output_index= True , ) print ( f\"Search time: {search_time: .6 f} seconds\" ) for query, result in zip (queries, results): print ( f\"Query: {query} \" ) for entry in result: print ( f\"(Score: {entry[ 'score' ]: .4 f} ) {corpus[entry[ 'corpus_id' ]]} , corpus_id: {entry[ 'corpus_id' ]} \" ) print ( \"\" ) queries = [ input ( \"Please enter a question: \" )]\n\nAdditional Resources\n\nTraining Examples\n\nThe following pages contain training examples with explanations as well as links to code. We recommend that you browse through these to familiarize yourself with the training loop:\n\nModel Distillation - Examples to make models smaller, faster and lighter.\n\nMS MARCO - Example training scripts for training on the MS MARCO information retrieval dataset.\n\nRetrievers - Example training scripts for training on generic information retrieval datasets.\n\nNatural Language Inference - Natural Language Inference (NLI) data can be quite helpful to pre-train and fine-tune models to create meaningful sparse embeddings.\n\nQuora Duplicate Questions - Quora Duplicate Questions is a large set corpus with duplicate questions from the Quora community. The folder contains examples how to train models for duplicate questions mining and for semantic search.\n\nSTS - The most basic method to train models is using Semantic Textual Similarity (STS) data. Here, we use sentence pairs and a score indicating the semantic similarity.\n\nDocumentation\n\nAdditionally, the following pages may be useful to learn more about Sentence Transformers:\n\nAnd lastly, here are some advanced pages that might interest you:",
    "link": "https://huggingface.co/blog/train-sparse-encoder",
    "Summary": "Training and Finetuning Sparse Embedding Models with Sentence Transformers v5Published July 1, 2025 Update on GitHubv3.0: (improved) Sentence Transformer (Dense Embedding) model trainingv4.0: (improved) Cross Encoder (Reranker) model trainingv5.0: (new) Sparse Embedding model trainingSentence Transformers is a Python library for using and training embedding and reranker models for a wide range of applications, such as retrieval augmented generation, semantic search, semantic textual similarity, paraphrase mining, and more.\nFinetuning sparse embedding models involves several components: the model, datasets, loss functions, training arguments, evaluators, and the trainer class.\nSparse embedding models, on the other hand, produce high-dimensional vectors (e.g., 30,000+ dimensions) where most values are zero.\nIn short, neural sparse embedding models fall in a valuable niche between traditional lexical methods like BM25 and dense embedding models like Sentence Transformers.\nVector Database IntegrationAfter training sparse embedding models, the next crucial step is deploying them effectively in production environments.",
    "Keywords": [
      "import",
      "models",
      "dense",
      "training",
      "loss",
      "v5",
      "embedding",
      "dataset",
      "datasets",
      "sparse",
      "transformers",
      "finetuning",
      "sentence",
      "model"
    ]
  },
  {
    "Title": "Transformers backend integration in SGLang",
    "Authors": [],
    "Publish Date": null,
    "Text": "Transformers backend integration in SGLang\n\nPublished June 23, 2025 Update on GitHub\n\nHugging Face transformers library is the standard for working with state-of-the-art models \u2014 from experimenting with cutting-edge research to fine-tuning on custom data. Its simplicity, flexibility, and expansive model zoo make it a powerful tool for rapid development.\n\nBut once you're ready to move from notebooks to production, inference performance becomes mission-critical. That\u2019s where SGLang comes in.\n\nDesigned for high-throughput, low-latency inference, SGLang now offers seamless integration with transformers as a backend. This means you can pair the flexibility of transformers with the raw performance of SGLang.\n\nLet\u2019s dive into what this integration enables and how you can use it.\n\nSGLang now supports Hugging Face transformers as a backend, letting you run any transformers-compatible model with high-performance inference out of the box.\n\nimport sglang as sgl llm = sgl.Engine( \"meta-llama/Llama-3.2-1B-Instruct\" , impl= \"transformers\" ) print (llm.generate([ \"The capital of France is\" ], { \"max_new_tokens\" : 20 })[ 0 ])\n\nNo native support needed \u2014 SGLang automatically falls back to Transformers when needed, or you can set impl=\"transformers\" explicitly.\n\nTransformers and SGLang\n\nLet\u2019s walk through a simple text generation example with meta-llama/Llama-3.2-1B-Instruct to compare both approaches.\n\nTransformers\n\ntransformers library is great for experimentation, small-scale tasks and training, but it's not optimized for high-volume or low-latency scenarios.\n\nfrom transformers import pipeline pipe = pipeline( \"text-generation\" , model= \"meta-llama/Llama-3.2-1B-Instruct\" ) generate_kwargs = { \"top_p\" : 0.95 , \"top_k\" : 20 , \"temperature\" : 0.8 , \"max_new_tokens\" : 256 } result = pipe( \"The future of AI is\" , **generate_kwargs) print (result[ 0 ][ \"generated_text\" ])\n\nSGLang\n\nSGLang takes a different track, prioritizing efficiency with features like RadixAttention (a memory-efficient attention mechanism). Inference with SGLang is noticeably faster and more resource-efficient, especially under load. Here\u2019s the same task in SGlang using an offline engine:\n\nimport sglang as sgl if __name__ == '__main__' : llm = sgl.Engine(model_path= \"meta-llama/Llama-3.2-1B-Instruct\" ) prompts = [ \"The future of AI is\" ] sampling_params = { \"top_p\" : 0.95 , \"top_k\" : 20 , \"temperature\" : 0.8 , \"max_new_tokens\" : 256 } outputs = llm.generate(prompts, sampling_params) print (outputs[ 0 ])\n\nOr you can spin a server and send requests:\n\npython3 -m sglang.launch_server \\ --model-path meta-llama/Llama-3.2-1B-Instruct \\ --host 0.0.0.0 \\ --port 30000\n\nresponse = requests.post( \"http://localhost:30000/generate\" , json={ \"text\" : \"The future of AI is\" , \"sampling_params\" : { \"top_p\" : 0.95 , \"top_k\" : 20 , \"temperature\" : 0.8 , \"max_new_tokens\" : 256 }, }, ) print (response.json())\n\nNote that SGLang also offers an OpenAI-compatible API, making it a drop-in replacement for external services.\n\nTransformers backend in SGLang\n\nWith the new transformers backend integration, SGLang can now automatically fall back to using transformers models it doesn\u2019t natively support. This means in practice:\n\nInstant access to new models added to transformers\n\nSupport for custom models from the Hugging Face Hub\n\nLess engineering overhead\n\nThis unlocks faster inference and optimized deployment (e.g enabling RadixAttention) without sacrificing the simplicity and versatility of transformers ecosystem.\n\nUsage\n\nllm = sgl.Engine(model_path= \"meta-llama/Llama-3.2-1B-Instruct\" , impl= \"transformers\" )\n\nNote that specifying the impl parameter is optional. If the model is not natively supported by SGLang, it switches to transformers implementation on its own.\n\nAny model on the Hugging Face Hub that works with transformers using trust_remote_code=True and properly implements attention is compatible with SGLang. You can find the exact requirements in the official documentation. If your custom model meets these criteria, all you need to do is set trust_remote_code=True when loading it.\n\nllm = sgl.Engine(model_path= \"new-custom-transformers-model\" , impl= \"transformers\" , trust_remote_code= True )\n\nExample\n\nKyutai Team\u2019s Helium isn\u2019t yet natively supported by SGLang. This is where transformers backend shines, enabling optimized inference without waiting for native support.\n\npython3 -m sglang.launch_server \\ --model-path kyutai/helium-1-preview-2b \\ --impl transformers \\ --host 0.0.0.0 \\ --port 30000\n\nresponse = requests.post( \"http://localhost:30000/generate\" , json={ \"text\" : \"The capital of France is\" , \"sampling_params\" : { \"top_p\" : 0.95 , \"top_k\" : 20 , \"temperature\" : 0.8 , \"max_new_tokens\" : 256 }, }, ) print (response.json())\n\nNext steps\n\nThere are several key areas we are actively working on to enhance this integration:",
    "link": "https://huggingface.co/blog/transformers-backend-sglang",
    "Summary": "Transformers backend integration in SGLangPublished June 23, 2025 Update on GitHubHugging Face transformers library is the standard for working with state-of-the-art models \u2014 from experimenting with cutting-edge research to fine-tuning on custom data.\nDesigned for high-throughput, low-latency inference, SGLang now offers seamless integration with transformers as a backend.\nimport sglang as sgl llm = sgl.Engine( \"meta-llama/Llama-3.2-1B-Instruct\" , impl= \"transformers\" ) print (llm.generate([ \"The capital of France is\" ], { \"max_new_tokens\" : 20 })[ 0 ])No native support needed \u2014 SGLang automatically falls back to Transformers when needed, or you can set impl=\"transformers\" explicitly.\nTransformers backend in SGLangWith the new transformers backend integration, SGLang can now automatically fall back to using transformers models it doesn\u2019t natively support.\nUsagellm = sgl.Engine(model_path= \"meta-llama/Llama-3.2-1B-Instruct\" , impl= \"transformers\" )Note that specifying the impl parameter is optional.",
    "Keywords": [
      "inference",
      "sglang",
      "impl",
      "integration",
      "backend",
      "metallamallama321binstruct",
      "print",
      "max_new_tokens",
      "transformers",
      "model"
    ]
  },
  {
    "Title": "Gemma 3n fully available in the open-source ecosystem!",
    "Authors": [],
    "Publish Date": null,
    "Text": "Gemma 3n fully available in the open-source ecosystem!\n\nPublished June 26, 2025 Update on GitHub\n\nGemma 3n was announced as a preview during Google I/O. The on-device community got really excited, because this is a model designed from the ground up toon your hardware. On top of that, it\u2019s natively, supporting image, text, audio, and video inputs \ud83e\udd2f\n\nToday, Gemma 3n is finally available on the most used open source libraries. This includes transformers & timm, MLX, llama.cpp (text inputs), transformers.js, ollama, Google AI Edge, and others.\n\nThis post quickly goes through practical snippets to demonstrate how to use the model with these libraries, and how easy it is to fine-tune it for other domains.\n\nModels released today\n\nHere is the Gemma 3n Release Collection\n\nTwo model sizes have been released today, with two variants (base and instruct) each. The model names follow a non-standard nomenclature: they are called gemma-3n-E2B and gemma-3n-E4B . The E preceding the parameter count stands for Effective . Their actual parameter counts are 5B and 8B , respectively, but thanks to improvements in memory efficiency, they manage to only need 2B and 4B in VRAM (GPU memory).\n\nThese models, therefore, behave like 2B and 4B in terms of hardware support, but they punch over 2B/4B in terms of quality. The E2B model can run in as little as 2GB of GPU RAM, while E4B can run with just 3GB of GPU RAM.\n\nSize Base Instruct 2B google/gemma-3n-e2b google/gemma-3n-e2b-it 4B google/gemma-3n-e4b google/gemma-3n-e4b-it\n\nDetails of the models\n\nIn addition to the language decoder, Gemma 3n uses an audio encoder and a vision encoder. We highlight their main features below, and describe how they have been added to transformers and timm , as they are the reference for other implementations.\n\nVision Encoder (MobileNet-V5). Gemma 3n uses a new version of MobileNet: MobileNet-v5-300, which has been added to the new version of timm released today. Features 300M parameters. Supports resolutions of 256x256 , 512x512 , and 768x768 . Achieves 60 FPS on Google Pixel, outperforming ViT Giant while using 3x fewer parameters.\n\nGemma 3n uses a new version of MobileNet: MobileNet-v5-300, which has been added to the new version of released today. Audio Encoder: Based on the Universal Speech Model (USM). Processes audio in 160ms chunks. Enables speech-to-text and translation functionalities (e.g., English to Spanish/French).\n\nGemma 3n Architecture and Language Model. The architecture itself has been added to the new version of transformers released today. This implementation branches out to timm for image encoding, so there\u2019s a single reference implementation of the MobileNet architecture.\n\nArchitecture Highlights\n\nMatFormer Architecture: A nested transformer design, similar to Matryoshka embeddings, allows for various subsets of layers to be extracted as if they were individual models. E2B and E4B were trained together, configuring E2B as a sub-model of E4B. Users can \u201cmix and match\u201d layers, depending on their hardware characteristics and memory budget.\n\nPer-Layer Embeddings (PLE): Reduces accelerator memory usage by offloading embeddings to the CPU. This is the reason why the E2B model, while having 5B real parameters, takes about as much GPU memory as if it were a 2B parameter model.\n\nReduces accelerator memory usage by offloading embeddings to the CPU. This is the reason why the E2B model, while having 5B real parameters, takes about as much GPU memory as if it were a 2B parameter model. KV Cache Sharing: Accelerates long-context processing for audio and video, achieving 2x faster prefill compared to Gemma 3 4B.\n\nPerformance & Benchmarks:\n\nLMArena Score: E4B is the first sub-10B model to achieve a score of 1300+.\n\nE4B is the first sub-10B model to achieve a score of 1300+. MMLU Scores: Gemma 3n shows competitive performance across various sizes (E4B, E2B, and several Mix-n-Match configurations).\n\nGemma 3n shows competitive performance across various sizes (E4B, E2B, and several Mix-n-Match configurations). Multilingual Support: Supports 140 languages for text and 35 languages for multimodal interactions.\n\nDemo Space\n\nThe easiest way to vibe check the model is with the dedicated Hugging Face Space for the model. You can try out different prompts, with different modalities, here.\n\n\ud83d\udcf1 Space\n\nInference with transformers\n\nInstall the latest version of timm (for the vision encoder) and transformers to run inference, or if you want to fine tune it.\n\npip install -U -q timm pip install -U -q transformers\n\nInference with pipeline\n\nThe easiest way to start using Gemma 3n is by using the pipeline abstraction in transformers:\n\nimport torch from transformers import pipeline pipe = pipeline( \"image-text-to-text\" , model= \"google/gemma-3n-E4B-it\" , device= \"cuda\" , torch_dtype=torch.bfloat16 ) messages = [ { \"role\" : \"user\" , \"content\" : [ { \"type\" : \"image\" , \"url\" : \"https://huggingface.co/datasets/ariG23498/demo-data/resolve/main/airplane.jpg\" }, { \"type\" : \"text\" , \"text\" : \"Describe this image\" } ] } ] output = pipe(text=messages, max_new_tokens= 32 ) print (output[ 0 ][ \"generated_text\" ][- 1 ][ \"content\" ])\n\nOutput:\n\nThe image shows a futuristic, sleek aircraft soaring through the sky. It's designed with a distinctive, almost alien aesthetic, featuring a wide body and large\n\nDetailed inference with transformers\n\nInitialize the model and the processor from the Hub, and write the model_generation function that takes care of processing the prompts and running the inference on the model.\n\nfrom transformers import AutoProcessor, AutoModelForImageTextToText import torch model_id = \"google/gemma-3n-e4b-it\" processor = AutoProcessor.from_pretrained(model_id) model = AutoModelForImageTextToText.from_pretrained(model_id).to(device) def model_generation ( model, messages ): inputs = processor.apply_chat_template( messages, add_generation_prompt= True , tokenize= True , return_dict= True , return_tensors= \"pt\" , ) input_len = inputs[ \"input_ids\" ].shape[- 1 ] inputs = inputs.to(model.device, dtype=model.dtype) with torch.inference_mode(): generation = model.generate(**inputs, max_new_tokens= 32 , disable_compile= False ) generation = generation[:, input_len:] decoded = processor.batch_decode(generation, skip_special_tokens= True ) print (decoded[ 0 ])\n\nSince the model supports all modalities as inputs, here's a brief code explanation of how you can use them via transformers.\n\nText only\n\nmessages = [ { \"role\" : \"user\" , \"content\" : [ { \"type\" : \"text\" , \"text\" : \"What is the capital of France?\" } ] } ] model_generation(model, messages)\n\nOutput:\n\nThe capital of France is **Paris**.\n\nInterleaved with Audio\n\nmessages = [ { \"role\" : \"user\" , \"content\" : [ { \"type\" : \"text\" , \"text\" : \"Transcribe the following speech segment in English:\" }, { \"type\" : \"audio\" , \"audio\" : \"https://huggingface.co/datasets/ariG23498/demo-data/resolve/main/speech.wav\" }, ] } ] model_generation(model, messages)\n\nOutput:\n\nSend a text to Mike. I'll be home late tomorrow.\n\nInterleaved with Image/Video\n\nSupport for videos is done as a collection of frames of images\n\nmessages = [ { \"role\" : \"user\" , \"content\" : [ { \"type\" : \"image\" , \"image\" : \"https://huggingface.co/datasets/ariG23498/demo-data/resolve/main/airplane.jpg\" }, { \"type\" : \"text\" , \"text\" : \"Describe this image.\" } ] } ] model_generation(model, messages)\n\nOutput:\n\nThe image shows a futuristic, sleek, white airplane against a backdrop of a clear blue sky transitioning into a cloudy, hazy landscape below. The airplane is tilted at\n\nInference with MLX\n\nGemma 3n comes with day 0 support for MLX across all 3 modalities. Make sure to upgrade your mlx-vlm installation.\n\npip install -u mlx-vlm\n\nGet started with vision:\n\npython -m mlx_vlm.generate --model google/gemma-3n-E4B-it -- max -tokens 100 --temperature 0.5 --prompt \"Describe this image in detail.\" --image https://huggingface.co/datasets/ariG23498/demo-data/resolve/main/airplane.jpg\n\nAnd audio:\n\npython -m mlx_vlm.generate --model google/gemma-3n-E4B-it -- max -tokens 100 --temperature 0.0 --prompt \"Transcribe the following speech segment in English:\" --audio https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/audio-samples/jfk.wav\n\nInference with llama.cpp\n\nIn addition to MLX, Gemma 3n (text only) works out of the box with llama.cpp. Make sure to install llama.cpp/ Ollama from source.\n\nCheck out the Installation instruction for llama.cpp here: https://github.com/ggml-org/llama.cpp/blob/master/docs/install.md\n\nYou can run it as:\n\nllama-server -hf ggml-org/gemma-3n-E4B-it-GGUF:Q8_0\n\nInference with Transformers.js and ONNXRuntime\n\nFinally, we are also releasing ONNX weights for the gemma-3n-E2B-it model variant, enabling flexible deployment across diverse runtimes and platforms. For JavaScript developers, Gemma3n has been integrated into Transformers.js and is available as of version 3.6.0.\n\nFor more information on how to run the model with these libraries, check out the usage section in the model card.\n\nFine Tune in a Free Google Colab\n\nGiven the size of the model, it\u2019s pretty convenient to fine-tune it for specific downstream tasks across modalities. To make it easier for you to fine-tune the model, we\u2019ve created a simple notebook that allows you to experiment on a free Google Colab!\n\nWe also provide a dedicated notebook for fine-tuning on audio tasks, so you can easily adapt the model to your speech datasets and benchmarks!\n\nHugging Face Gemma Recipes\n\nWith this release, we also introduce the Hugging Face Gemma Recipes repository. One will find notebooks and scripts to run the models and fine tune them.\n\nWe would love for you to use the Gemma family of models and add more recipes to it! Feel free to open Issues and create Pull Requests to the repository.\n\nConclusion\n\nWe are always excited to host Google and their Gemma family of models. We hope the community will get together and make the most of these models. Multimodal, small sized, and highly capable, make a great model release!\n\nIf you want to discuss the models in more detail, go ahead and start a discussion right below this blog post. We will be more than happy to help!\n\nA huge thanks to Arthur, Cyril, Raushan, Lysandre, and everyone at Hugging Face who took care of the integration and made it available to the community!",
    "link": "https://huggingface.co/blog/gemma3n",
    "Summary": "Gemma 3n fully available in the open-source ecosystem!\nOn top of that, it\u2019s natively, supporting image, text, audio, and video inputs \ud83e\udd2fToday, Gemma 3n is finally available on the most used open source libraries.\nText onlymessages = [ { \"role\" : \"user\" , \"content\" : [ { \"type\" : \"text\" , \"text\" : \"What is the capital of France?\"\nInterleaved with Audiomessages = [ { \"role\" : \"user\" , \"content\" : [ { \"type\" : \"text\" , \"text\" : \"Transcribe the following speech segment in English:\" }, { \"type\" : \"audio\" , \"audio\" : \"https://huggingface.co/datasets/ariG23498/demo-data/resolve/main/speech.wav\" }, ] } ] model_generation(model, messages)Output:Send a text to Mike.\nInterleaved with Image/VideoSupport for videos is done as a collection of frames of imagesmessages = [ { \"role\" : \"user\" , \"content\" : [ { \"type\" : \"image\" , \"image\" : \"https://huggingface.co/datasets/ariG23498/demo-data/resolve/main/airplane.jpg\" }, { \"type\" : \"text\" , \"text\" : \"Describe this image.\"",
    "Keywords": [
      "fully",
      "opensource",
      "models",
      "version",
      "ecosystem",
      "image",
      "audio",
      "e2b",
      "3n",
      "type",
      "gemma",
      "text",
      "model",
      "available"
    ]
  },
  {
    "Title": "Learn the Hugging Face Kernel Hub in 5 Minutes",
    "Authors": [],
    "Publish Date": null,
    "Text": "\ud83c\udfce\ufe0f Enhance Your Models in 5 Minutes with the Hugging Face Kernel Hub\n\nPublished June 12, 2025 Update on GitHub\n\nToday, we'll explore an exciting development from Hugging Face: the Kernel Hub! As ML practitioners, we know that maximizing performance often involves diving deep into optimized code, custom CUDA kernels, or complex build systems. The Kernel Hub simplifies this process dramatically!\n\nBelow is a short example of how to use a kernel in your code.\n\nimport torch from kernels import get_kernel activation = get_kernel( \"kernels-community/activation\" ) x = torch.randn(( 10 , 10 ), dtype=torch.float16, device= \"cuda\" ) y = torch.empty_like(x) activation.gelu_fast(y, x) print (y)\n\nIn the next sections we'll cover the following topics:\n\nWhat is the Kernel Hub? - Understanding the core concept. How to use the Kernel Hub - A quick code example. Adding a Kernel to a Simple Model - A practical integration using RMSNorm. Reviewing Performance Impact - Benchmarking the RMSNorm difference. Real world use cases - Examples of how the kernels library is being used in other projects.\n\nWe'll introduce these concepts quickly \u2013 the core idea can be grasped in about 5 minutes (though experimenting and benchmarking might take a bit longer!).\n\n1. What is the Kernel Hub?\n\nThe Kernel Hub (\ud83d\udc48 Check it out!) allows Python libraries and applications to load optimized compute kernels directly from the Hugging Face Hub. Think of it like the Model Hub, but for low-level, high-performance code snippets (kernels) that accelerate specific operations, often on GPUs.\n\nExamples include advanced attention mechanisms (like FlashAttention for dramatic speedups and memory savings). Custom quantization kernels (enabling efficient computation with lower-precision data types like INT8 or INT4). Specialized kernels required for complex architectures like Mixture of Experts (MoE) layers, which involve intricate routing and computation patterns. As well as activation functions, and normalization layers (like LayerNorm or RMSNorm).\n\nInstead of manually managing complex dependencies, wrestling with compilation flags, or building libraries like Triton or CUTLASS from source, you can use the kernels library to instantly fetch and run pre-compiled, optimized kernels.\n\nFor example, to enable FlashAttention you need just one line\u2014no builds, no flags:\n\nfrom kernels import get_kernel flash_attention = get_kernel( \"kernels-community/flash-attn\" )\n\nkernels detects your exact Python, PyTorch, and CUDA versions, then downloads the matching pre\u2011compiled binary\u2014typically in seconds (or a minute or two on a slow connection).\n\nBy contrast, compiling FlashAttention yourself requires:\n\nCloning the repository and installing every dependency.\n\nConfiguring build flags and environment variables.\n\nReserving ~96 GB of RAM and plenty of CPU cores.\n\nand plenty of CPU cores. Waiting 10 minutes to several hours, depending on your hardware. (See the project\u2019s own installation guide for details.)\n\nKernel Hub erases all that friction: one function call, instant acceleration.\n\nBenefits of the Kernel Hub:\n\nInstant Access to Optimized Kernels : Load and run kernels optimized for various hardware starting with NVIDIA and AMD GPUs, without local compilation hassles.\n\n: Load and run kernels optimized for various hardware starting with NVIDIA and AMD GPUs, without local compilation hassles. Share and Reuse : Discover, share, and reuse kernels across different projects and the community.\n\n: Discover, share, and reuse kernels across different projects and the community. Easy Updates : Stay up-to-date with the latest kernel improvements simply by pulling the latest version from the Hub.\n\n: Stay up-to-date with the latest kernel improvements simply by pulling the latest version from the Hub. Accelerate Development : Focus on your model architecture and logic, not on the intricacies of kernel compilation and deployment.\n\n: Focus on your model architecture and logic, not on the intricacies of kernel compilation and deployment. Improve Performance : Leverage kernels optimized by experts to potentially speed up training and inference.\n\n: Leverage kernels optimized by experts to potentially speed up training and inference. Simplify Deployment : Reduce the complexity of your deployment environment by fetching kernels on demand.\n\n: Reduce the complexity of your deployment environment by fetching kernels on demand. Develop and Share Your Own Kernels: If you create optimized kernels, you can easily share them on the Hub for others to use. This encourages collaboration and knowledge sharing within the community.\n\nAs many machine learning developers know, managing dependencies and building low-level code from source can be a time-consuming and error-prone process. The Kernel Hub aims to simplify this by providing a centralized repository of optimized compute kernels that can be easily loaded and run.\n\nSpend more time building great models and less time fighting build systems!\n\n2. How to Use the Kernel Hub (Basic Example)\n\nUsing the Kernel Hub is designed to be straightforward. The kernels library provides the main interface. Here's a quick example that loads an optimized GELU activation function kernel. (Later on, we'll see another example about how to integrate a kernel in our model).\n\nFile: activation_validation_example.py\n\nimport torch import torch.nn.functional as F from kernels import get_kernel DEVICE = \"cuda\" torch.manual_seed( 42 ) activation_kernels = get_kernel( \"kernels-community/activation\" ) x = torch.randn(( 4 , 4 ), dtype=torch.float16, device=DEVICE) y = torch.empty_like(x) activation_kernels.gelu_fast(y, x) expected = F.gelu(x) torch.testing.assert_close(y, expected, rtol= 1e-2 , atol= 1e-2 ) print ( \"\u2705 Kernel output matches PyTorch GELU!\" ) print ( \"\n\nInput tensor:\" ) print (x) print ( \"\n\nFast GELU kernel output:\" ) print (y) print ( \"\n\nPyTorch GELU output:\" ) print (expected) print ( \"\n\nAvailable functions in 'kernels-community/activation':\" ) print ( dir (activation_kernels))\n\n(Note: If you have uv installed, you can save this script as script.py and run uv run script.py to automatically handle dependencies.)\n\nWhat's happening here?\n\nImport get_kernel : This function is the entry point to the Kernel Hub via the kernels library. get_kernel(\"kernels-community/activation\") : This line looks for the activation kernel repository under the kernels-community organization. It downloads, caches, and loads the appropriate pre-compiled kernel binary. Prepare Tensors: We create input ( x ) and output ( y ) tensors on the GPU. activation_kernels.gelu_fast(y, x) : We call the specific optimized function ( gelu_fast ) provided by the loaded kernel module. Verification: We check the output.\n\nThis simple example shows how easily you can fetch and execute highly optimized code. Now let's look at a more practical integration using RMS Normalization.\n\n3. Add a Kernel to a Simple Model\n\nLet's integrate an optimized RMS Normalization kernel into a basic model. We'll use the LlamaRMSNorm implementation provided in the kernels-community/triton-layer-norm repository (note: this repo contains various normalization kernels) and compare it against a baseline PyTorch implementation of RMSNorm.\n\nFirst, define a simple RMSNorm module in PyTorch and a baseline model using it:\n\nFile: rmsnorm_baseline.py\n\nimport torch import torch.nn as nn DEVICE = \"cuda\" DTYPE = torch.float16 class RMSNorm (nn.Module): def __init__ ( self, hidden_size, variance_epsilon= 1e-5 ): super ().__init__() self.weight = nn.Parameter(torch.ones(hidden_size)) self.eps = variance_epsilon self.hidden_size = hidden_size def forward ( self, x ): input_dtype = x.dtype variance = x.to(torch.float32). pow ( 2 ).mean(- 1 , keepdim= True ) x = x * torch.rsqrt(variance + self.eps) return (self.weight * x).to(input_dtype) class BaselineModel (nn.Module): def __init__ ( self, input_size, hidden_size, output_size, eps= 1e-5 ): super ().__init__() self.linear1 = nn.Linear(input_size, hidden_size) self.norm = RMSNorm(hidden_size, variance_epsilon=eps) self.activation = nn.GELU() self.linear2 = nn.Linear(hidden_size, output_size) with torch.no_grad(): self.linear1.weight.fill_( 1 ) self.linear1.bias.fill_( 0 ) self.linear2.weight.fill_( 1 ) self.linear2.bias.fill_( 0 ) self.norm.weight.fill_( 1 ) def forward ( self, x ): x = self.linear1(x) x = self.norm(x) x = self.activation(x) x = self.linear2(x) return x input_size = 128 hidden_size = 256 output_size = 10 eps_val = 1e-5 baseline_model = ( BaselineModel(input_size, hidden_size, output_size, eps=eps_val) .to(DEVICE) .to(DTYPE) ) dummy_input = torch.randn( 32 , input_size, device=DEVICE, dtype=DTYPE) output = baseline_model(dummy_input) print ( \"Baseline RMSNorm model output shape:\" , output.shape)\n\nNow, let's create a version using the LlamaRMSNorm kernel loaded via kernels .\n\nFile: rmsnorm_kernel.py\n\nimport torch import torch.nn as nn from kernels import get_kernel, use_kernel_forward_from_hub from rmsnorm_baseline import BaselineModel DEVICE = \"cuda\" DTYPE = torch.float16 layer_norm_kernel_module = get_kernel( \"kernels-community/triton-layer-norm\" ) class OriginalRMSNorm (nn.Module): def __init__ ( self, hidden_size, variance_epsilon= 1e-5 ): super ().__init__() self.weight = nn.Parameter(torch.ones(hidden_size)) self.eps = variance_epsilon self.hidden_size = hidden_size def forward ( self, x ): input_dtype = x.dtype variance = x.to(torch.float32). pow ( 2 ).mean(- 1 , keepdim= True ) x = x * torch.rsqrt(variance + self.eps) return (self.weight * x).to(input_dtype) class KernelModel (nn.Module): def __init__ ( self, input_size, hidden_size, output_size, device= \"cuda\" , dtype=torch.float16, eps= 1e-5 , ): super ().__init__() self.linear1 = nn.Linear(input_size, hidden_size) self.norm = OriginalRMSNorm(hidden_size, variance_epsilon=eps) self.activation = nn.GELU() self.linear2 = nn.Linear(hidden_size, output_size) with torch.no_grad(): self.linear1.weight.fill_( 1 ) self.linear1.bias.fill_( 0 ) self.linear2.weight.fill_( 1 ) self.linear2.bias.fill_( 0 ) self.norm.weight.fill_( 1 ) def forward ( self, x ): x = self.linear1(x) x = self.norm(x) x = self.activation(x) x = self.linear2(x) return x input_size = 128 hidden_size = 256 output_size = 10 eps_val = 1e-5 kernel_model = ( KernelModel( input_size, hidden_size, output_size, device=DEVICE, dtype=DTYPE, eps=eps_val ) .to(DEVICE) .to(DTYPE) ) baseline_model = ( BaselineModel(input_size, hidden_size, output_size, eps=eps_val) .to(DEVICE) .to(DTYPE) ) dummy_input = torch.randn( 32 , input_size, device=DEVICE, dtype=DTYPE) output = baseline_model(dummy_input) output_kernel = kernel_model(dummy_input) print ( \"Kernel RMSNorm model output shape:\" , output_kernel.shape) try : torch.testing.assert_close(output, output_kernel, rtol= 1e-2 , atol= 1e-2 ) print ( \"\n\nBaseline and Kernel RMSNorm model outputs match!\" ) except AssertionError as e: print ( \"\n\nBaseline and Kernel RMSNorm model outputs differ slightly:\" ) print (e) except NameError: print ( \"\n\nSkipping output comparison as kernel model output was not generated.\" )\n\nImportant Notes on the KernelModel :\n\nKernel Inheritance: The KernelRMSNorm class inherits from layer_norm_kernel_module.layers.LlamaRMSNorm , which is the RMSNorm implementation in the kernel. This allows us to use the optimized kernel directly.\n\nThe class inherits from , which is the RMSNorm implementation in the kernel. This allows us to use the optimized kernel directly. Accessing the Function: The exact way to access the RMSNorm function ( layer_norm_kernel_module.layers.LlamaRMSNorm.forward , layer_norm_kernel_module.rms_norm_forward , or something else) depends entirely on how the kernel creator structured the repository on the Hub. You may need to inspect the loaded layer_norm_kernel_module object (e.g., using dir() ) or check the kernel's documentation on the Hub to find the correct function/method and its signature. I've used rms_norm_forward as a plausible placeholder and added error handling.\n\nThe exact way to access the RMSNorm function ( , , or something else) You may need to inspect the loaded object (e.g., using ) or check the kernel's documentation on the Hub to find the correct function/method and its signature. I've used as a plausible placeholder and added error handling. Parameters: We now only define rms_norm_weight (no bias), consistent with RMSNorm.\n\n4. Benchmarking the Performance Impact\n\nHow much faster is the optimized Triton RMSNorm kernel compared to the standard PyTorch version? Let\u2019s benchmark the forward pass to find out.\n\nFile: rmsnorm_benchmark.py\n\nimport torch from rmsnorm_baseline import BaselineModel from rmsnorm_kernel import KernelModel DEVICE = \"cuda\" DTYPE = torch.float16 def benchmark_model ( model, input_tensor, num_runs= 100 , warmup_runs= 10 ): model. eval () dtype = input_tensor.dtype model = model.to(input_tensor.device).to(dtype) for _ in range (warmup_runs): _ = model(input_tensor) torch.cuda.synchronize() start_event = torch.cuda.Event(enable_timing= True ) end_event = torch.cuda.Event(enable_timing= True ) start_event.record() for _ in range (num_runs): _ = model(input_tensor) end_event.record() torch.cuda.synchronize() elapsed_time_ms = start_event.elapsed_time(end_event) avg_time_ms = elapsed_time_ms / num_runs return avg_time_ms input_size_bench = 4096 hidden_size_bench = 4096 output_size_bench = 10 eps_val_bench = 1e-5 baseline_model_bench = ( BaselineModel( input_size_bench, hidden_size_bench, output_size_bench, eps=eps_val_bench ) .to(DEVICE) .to(DTYPE) ) kernel_model_bench = ( KernelModel( input_size_bench, hidden_size_bench, output_size_bench, device=DEVICE, dtype=DTYPE, eps=eps_val_bench, ) .to(DEVICE) .to(DTYPE) ) warmup_input = torch.randn( 4096 , input_size_bench, device=DEVICE, dtype=DTYPE) _ = kernel_model_bench(warmup_input) _ = baseline_model_bench(warmup_input) batch_sizes = [ 256 , 512 , 1024 , 2048 , 4096 , 8192 , 16384 , 32768 , ] print ( f\" { 'Batch Size' :< 12 } | { 'Baseline Time (ms)' :< 18 } | { 'Kernel Time (ms)' :< 18 } | { 'Speedup' } \" ) print ( \"-\" * 74 ) for batch_size in batch_sizes: torch.cuda.synchronize() bench_input = torch.randn(batch_size, input_size_bench, device=DEVICE, dtype=DTYPE) baseline_time = benchmark_model(baseline_model_bench, bench_input) kernel_time = - 1 kernel_time = benchmark_model(kernel_model_bench, bench_input) baseline_time = round (baseline_time, 4 ) kernel_time = round (kernel_time, 4 ) speedup = round (baseline_time / kernel_time, 2 ) if kernel_time > 0 else \"N/A\" if kernel_time < baseline_time: speedup = f\" {speedup: .2 f} x\" elif kernel_time == baseline_time: speedup = \"1.00x (identical)\" else : speedup = f\" {kernel_time / baseline_time: .2 f} x slower\" print ( f\" {batch_size:< 12 } | {baseline_time:< 18 } | {kernel_time:< 18 } | {speedup} \" )\n\nExpected Outcome: As with LayerNorm, a well-tuned RMSNorm implementation using Triton can deliver substantial speedups over PyTorch\u2019s default version\u2014especially for memory-bound workloads on compatible hardware (e.g., NVIDIA Ampere or Hopper GPUs) and with low-precision types like float16 or bfloat16 .\n\nKeep in Mind:\n\nResults may vary depending on your GPU, input size, and data type.\n\nMicrobenchmarks can misrepresent real-world performance.\n\nPerformance hinges on the quality of the kernel implementation.\n\nOptimized kernels might not benefit small batch sizes due to overhead.\n\nActual results will depend on your hardware and the specific kernel implementation. Here's an example of what you might see (on a L4 GPU):\n\nBatch Size Baseline Time (ms) Kernel Time (ms) Speedup 256 0.2122 0.2911 0.72x 512 0.4748 0.3312 1.43x 1024 0.8946 0.6864 1.30x 2048 2.0289 1.3889 1.46x 4096 4.4318 2.2467 1.97x 8192 9.2438 4.8497 1.91x 16384 18.6992 9.8805 1.89x 32768 37.079 19.9461 1.86x 65536 73.588 39.593 1.86x\n\n5. Real World Use Cases\n\nThe kernels library is still growing but is already being used in various real world projects, including:\n\nText Generation Inference: The TGI project uses the kernels library to load optimized kernels for text generation tasks, improving performance and efficiency.\n\nlibrary to load optimized kernels for text generation tasks, improving performance and efficiency. Transformers: The Transformers library has integrated the kernels library to use drop in optimized layers without requiring any changes to the model code. This allows users to easily switch between standard and optimized implementations.\n\nGet Started and Next Steps!\n\nYou've seen how easy it is to fetch and use optimized kernels with the Hugging Face Kernel Hub. Ready to try it yourself?\n\nInstall the library: pip install kernels torch numpy Ensure you have a compatible PyTorch version and gpu driver installed. Browse the Hub: Explore available kernels on the Hugging Face Hub under the kernels tag or within organizations like kernels-community . Look for kernels relevant to your operations (activations, attention, normalization like LayerNorm/RMSNorm, etc.). Experiment: Try replacing components in your own models. Use get_kernel(\"user-or-org/kernel-name\") . Crucially, inspect the loaded kernel object (e.g., print(dir(loaded_kernel)) ) or check its Hub repository documentation to understand how to correctly call its functions/methods and what parameters (weights, biases, inputs, epsilon) it expects. Benchmark: Measure the performance impact on your specific hardware and workload. Don't forget to check for numerical correctness ( torch.testing.assert_close ). (Advanced) Contribute: If you develop optimized kernels, consider sharing them on the Hub!\n\nConclusion\n\nThe Hugging Face Kernel Hub provides a powerful yet simple way to access and leverage optimized compute kernels. By replacing standard PyTorch components with optimized versions for operations like RMS Normalization, you can potentially unlock significant performance improvements without the traditional complexities of custom builds. Remember to check the specifics of each kernel on the Hub for correct usage. Give it a try and see how it can accelerate your workflows!",
    "link": "https://huggingface.co/blog/hello-hf-kernels",
    "Summary": "\ud83c\udfce\ufe0f Enhance Your Models in 5 Minutes with the Hugging Face Kernel HubPublished June 12, 2025 Update on GitHubToday, we'll explore an exciting development from Hugging Face: the Kernel Hub!\nHow to Use the Kernel Hub (Basic Example)Using the Kernel Hub is designed to be straightforward.\n) except AssertionError as e: print ( \"Baseline and Kernel RMSNorm model outputs differ slightly:\" ) print (e) except NameError: print ( \"Skipping output comparison as kernel model output was not generated.\"\nYou've seen how easy it is to fetch and use optimized kernels with the Hugging Face Kernel Hub.\nConclusionThe Hugging Face Kernel Hub provides a powerful yet simple way to access and leverage optimized compute kernels.",
    "Keywords": [
      "output",
      "optimized",
      "rmsnorm",
      "hidden_size",
      "hub",
      "x",
      "face",
      "hugging",
      "print",
      "kernel",
      "minutes",
      "learn",
      "kernels",
      "model"
    ]
  },
  {
    "Title": "Groq on Hugging Face Inference Providers \ud83d\udd25",
    "Authors": [],
    "Publish Date": null,
    "Text": "Groq on Hugging Face Inference Providers \ud83d\udd25\n\nPublished June 16, 2025 Update on GitHub\n\nWe're thrilled to share that Groq is now a supported Inference Provider on the Hugging Face Hub! Groq joins our growing ecosystem, enhancing the breadth and capabilities of serverless inference directly on the Hub\u2019s model pages. Inference Providers are also seamlessly integrated into our client SDKs (for both JS and Python), making it super easy to use a wide variety of models with your preferred providers.\n\nGroq supports a wide variety of text and conversational models, including the latest open-source models such as Meta's Llama 4, Qwen's QWQ-32B, and many more.\n\nAt the heart of Groq's technology is the Language Processing Unit (LPU\u2122), a new type of end-to-end processing unit system that provides the fastest inference for computationally intensive applications with a sequential component, such as Large Language Models (LLMs). LPUs are designed to overcome the limitations of GPUs for inference, offering significantly lower latency and higher throughput. This makes them ideal for real-time AI applications.\n\nGroq offers fast AI inference for openly-available models. They provide an API that allows developers to easily integrate these models into their applications. It offers an on-demand, pay-as-you-go model for accessing a wide range of openly-available LLMs.\n\nYou can now use Groq's Inference API as an Inference Provider on Huggingface. We're quite excited to see what you'll build with this new provider.\n\nRead more about how to use Groq as Inference Provider in its dedicated documentation page.\n\nSee the list of supported models here.\n\nHow it works\n\nIn the website UI\n\nIn your user account settings, you are able to:\n\nSet your own API keys for the providers you\u2019ve signed up with. If no custom key is set, your requests will be routed through HF.\n\nOrder providers by preference. This applies to the widget and code snippets in the model pages.\n\nAs mentioned, there are two modes when calling Inference Providers:\n\nCustom key (calls go directly to the inference provider, using your own API key of the corresponding inference provider)\n\nRouted by HF (in that case, you don't need a token from the provider, and the charges are applied directly to your HF account rather than the provider's account)\n\nModel pages showcase third-party inference providers (the ones that are compatible with the current model, sorted by user preference)\n\nFrom the client SDKs\n\nfrom Python, using huggingface_hub\n\nThe following example shows how to use Meta's Llama 4 using Groq as the inference provider. You can use a Hugging Face token for automatic routing through Hugging Face, or your own Groq API key if you have one.\n\nInstall huggingface_hub from source (see instructions). Official support will be released soon in version v0.33.0.\n\nimport os from huggingface_hub import InferenceClient client = InferenceClient( provider= \"groq\" , api_key=os.environ[ \"HF_TOKEN\" ], ) messages = [ { \"role\" : \"user\" , \"content\" : \"What is the capital of France?\" } ] completion = client.chat.completions.create( model= \"meta-llama/Llama-4-Scout-17B-16E-Instruct\" , messages=messages, ) print (completion.choices[ 0 ].message)\n\nfrom JS using @huggingface/inference\n\nimport { InferenceClient } from \"@huggingface/inference\" ; const client = new InferenceClient (process. env . HF_TOKEN ); const chatCompletion = await client. chatCompletion ({ model : \"meta-llama/Llama-4-Scout-17B-16E-Instruct\" , messages : [ { role : \"user\" , content : \"What is the capital of France?\" , }, ], provider : \"groq\" , }); console . log (chatCompletion. choices [ 0 ]. message );\n\nBilling\n\nFor direct requests, i.e. when you use the key from an inference provider, you are billed by the corresponding provider. For instance, if you use a Groq API key you're billed on your Groq account.\n\nFor routed requests, i.e. when you authenticate via the Hugging Face Hub, you'll only pay the standard provider API rates. There's no additional markup from us, we just pass through the provider costs directly. (In the future, we may establish revenue-sharing agreements with our provider partners.)\n\nImportant Note \u203c\ufe0f PRO users get $2 worth of Inference credits every month. You can use them across providers. \ud83d\udd25\n\nSubscribe to the Hugging Face PRO plan to get access to Inference credits, ZeroGPU, Spaces Dev Mode, 20x higher limits, and more.\n\nWe also provide free inference with a small quota for our signed-in free users, but please upgrade to PRO if you can!\n\nFeedback and next steps\n\nWe would love to get your feedback! Share your thoughts and/or comments here: https://huggingface.co/spaces/huggingface/HuggingDiscussions/discussions/49",
    "link": "https://huggingface.co/blog/inference-providers-groq",
    "Summary": "Groq on Hugging Face Inference Providers \ud83d\udd25Published June 16, 2025 Update on GitHubWe're thrilled to share that Groq is now a supported Inference Provider on the Hugging Face Hub!\nYou can now use Groq's Inference API as an Inference Provider on Huggingface.\nYou can use a Hugging Face token for automatic routing through Hugging Face, or your own Groq API key if you have one.\nFor instance, if you use a Groq API key you're billed on your Groq account.\n\ud83d\udd25Subscribe to the Hugging Face PRO plan to get access to Inference credits, ZeroGPU, Spaces Dev Mode, 20x higher limits, and more.",
    "Keywords": [
      "inference",
      "models",
      "face",
      "hugging",
      "api",
      "providers",
      "key",
      "groq",
      "provider",
      "model"
    ]
  },
  {
    "Title": "(LoRA) Fine-Tuning FLUX.1-dev on Consumer Hardware",
    "Authors": [],
    "Publish Date": null,
    "Text": "(LoRA) Fine-Tuning FLUX.1-dev on Consumer Hardware\n\nPublished June 19, 2025 Update on GitHub\n\nIn our previous post, Exploring Quantization Backends in Diffusers, we dived into how various quantization techniques can shrink diffusion models like FLUX.1-dev, making them significantly more accessible for inference without drastically compromising performance. We saw how bitsandbytes , torchao , and others reduce memory footprints for generating images.\n\nPerforming inference is cool, but to make these models truly our own, we also need to be able to fine-tune them. Therefore, in this post, we tackle efficient fine-tuning of these models with peak memory use under ~10 GB of VRAM on a single GPU. This post will guide you through fine-tuning FLUX.1-dev using QLoRA with the diffusers library. We'll showcase results from an NVIDIA RTX 4090. We'll also highlight how FP8 training with torchao can further optimize speed on compatible hardware.\n\nTable of Contents\n\nDataset\n\nWe aim to fine-tune black-forest-labs/FLUX.1-dev to adopt the artistic style of Alphonse Mucha, using a small dataset.\n\nFLUX Architecture\n\nThe model consists of three main components:\n\nText Encoders (CLIP and T5)\n\nTransformer (Main Model - Flux Transformer)\n\nVariational Auto-Encoder (VAE)\n\nIn our QLoRA approach, we focus exclusively on fine-tuning the transformer component. The text encoders and VAE remain frozen throughout training.\n\nQLoRA Fine-tuning FLUX.1-dev with Diffusers\n\nWe used a diffusers training script (slightly modified from here designed for DreamBooth-style LoRA fine-tuning of FLUX models. Also, a shortened version to reproduce the results in this blogpost (and used in the Google Colab) is available here. Let's examine the crucial parts for QLoRA and memory efficiency:\n\nKey Optimization Techniques\n\nLoRA (Low-Rank Adaptation) Deep Dive: LoRA makes model training more efficient by keeping track of the weight updates with low-rank matrices. Instead of updating the full weight matrix W W W, LoRA learns two smaller matrices A A A and B B B. The update to the weights for the model is \u0394 W = B A \\Delta W = B A \u0394W=BA, where A \u2208 R r \u00d7 k A \\in \\mathbb{R}^{r \\times k} A\u2208Rr\u00d7k and B \u2208 R d \u00d7 r B \\in \\mathbb{R}^{d \\times r} B\u2208Rd\u00d7r. The number r r r (called rank) is much smaller than the original dimensions, which means less parameters to update. Lastly, \u03b1 \\alpha \u03b1 is a scaling factor for the LoRA activations. This affects how much LoRA affects the updates, and is often set to the same value as the r r r or a multiple of it. It helps balance the influence of the pre-trained model and the LoRA adapter. For a general introduction to the concept, check out our previous blog post: Using LoRA for Efficient Stable Diffusion Fine-Tuning.\n\nQLoRA: The Efficiency Powerhouse: QLoRA enhances LoRA by first loading the pre-trained base model in a quantized format (typically 4-bit via bitsandbytes ), drastically cutting the base model's memory footprint. It then trains LoRA adapters (usually in FP16/BF16) on top of this quantized base. This dramatically lowers the VRAM needed to hold the base model.\n\nFor instance, in the DreamBooth training script for HiDream 4-bit quantization with bitsandbytes reduces the peak memory usage of a LoRA fine-tune from ~60GB down to ~37GB with negligible-to-none quality degradation. The very same principle is what we apply here to fine-tune FLUX.1 on a consumer-grade hardware.\n\n8-bit Optimizer (AdamW): Standard AdamW optimizer maintains first and second moment estimates for each parameter in 32-bit (FP32), which consumes a lot of memory. The 8-bit AdamW uses block-wise quantization to store optimizer states in 8-bit precision, while maintaining training stability. This technique can reduce optimizer memory usage by ~75% compared to standard FP32 AdamW. Enabling it in the script is straightforward:\n\nif args.use_8bit_adam: optimizer_class = bnb.optim.AdamW8bit else : optimizer_class = torch.optim.AdamW optimizer = optimizer_class( params_to_optimize, betas=(args.adam_beta1, args.adam_beta2), weight_decay=args.adam_weight_decay, eps=args.adam_epsilon, )\n\nGradient Checkpointing: During forward pass, intermediate activations are typically stored for backward pass gradient computation. Gradient checkpointing trades computation for memory by only storing certain checkpoint activations and recomputing others during backpropagation.\n\nif args.gradient_checkpointing: transformer.enable_gradient_checkpointing()\n\nCache Latents: This optimization technique pre-processes all training images through the VAE encoder before the beginning of the training. It stores the resulting latent representations in memory. During the training, instead of encoding images on-the-fly, the cached latents are directly used. This approach offers two main benefits:\n\neliminates redundant VAE encoding computations during training, speeding up each training step allows the VAE to be completely removed from GPU memory after caching. The trade-off is increased RAM usage to store all cached latents, but this is typically manageable for small datasets.\n\nif args.cache_latents: latents_cache = [] for batch in tqdm(train_dataloader, desc= \"Caching latents\" ): with torch.no_grad(): batch[ \"pixel_values\" ] = batch[ \"pixel_values\" ].to( accelerator.device, non_blocking= True , dtype=weight_dtype ) latents_cache.append(vae.encode(batch[ \"pixel_values\" ]).latent_dist) del vae free_memory()\n\nSetting up 4-bit Quantization ( BitsAndBytesConfig ):\n\nThis section demonstrates the QLoRA configuration for the base model:\n\nbnb_4bit_compute_dtype = torch.float32 if args.mixed_precision == \"fp16\" : bnb_4bit_compute_dtype = torch.float16 elif args.mixed_precision == \"bf16\" : bnb_4bit_compute_dtype = torch.bfloat16 nf4_config = BitsAndBytesConfig( load_in_4bit= True , bnb_4bit_quant_type= \"nf4\" , bnb_4bit_compute_dtype=bnb_4bit_compute_dtype, ) transformer = FluxTransformer2DModel.from_pretrained( args.pretrained_model_name_or_path, subfolder= \"transformer\" , quantization_config=nf4_config, torch_dtype=bnb_4bit_compute_dtype, ) transformer = prepare_model_for_kbit_training(transformer, use_gradient_checkpointing= False )\n\nDefining LoRA Configuration ( LoraConfig ): Adapters are added to the quantized transformer:\n\ntransformer_lora_config = LoraConfig( r=args.rank, lora_alpha=args.rank, init_lora_weights= \"gaussian\" , target_modules=[ \"to_k\" , \"to_q\" , \"to_v\" , \"to_out.0\" ], ) transformer.add_adapter(transformer_lora_config) print ( f\"trainable params: {transformer.num_parameters(only_trainable= True )} || all params: {transformer.num_parameters()} \" )\n\nOnly these LoRA parameters become trainable.\n\nPre-computing Text Embeddings (CLIP/T5)\n\nBefore we launch the QLoRA fine-tune we can save a huge chunk of VRAM and wall-clock time by caching outputs of text encoders once.\n\nAt training time the dataloader simply reads the cached embeddings instead of re-encoding the caption, so the CLIP/T5 encoder never has to sit in GPU memory.\n\nCode import argparse import pandas as pd import torch from datasets import load_dataset from huggingface_hub.utils import insecure_hashlib from tqdm.auto import tqdm from transformers import T5EncoderModel from diffusers import FluxPipeline MAX_SEQ_LENGTH = 77 OUTPUT_PATH = \"embeddings.parquet\" def generate_image_hash ( image ): return insecure_hashlib.sha256(image.tobytes()).hexdigest() def load_flux_dev_pipeline (): id = \"black-forest-labs/FLUX.1-dev\" text_encoder = T5EncoderModel.from_pretrained( id , subfolder= \"text_encoder_2\" , load_in_8bit= True , device_map= \"auto\" ) pipeline = FluxPipeline.from_pretrained( id , text_encoder_2=text_encoder, transformer= None , vae= None , device_map= \"balanced\" ) return pipeline def compute_embeddings ( pipeline, prompts, max_sequence_length ): all_prompt_embeds = [] all_pooled_prompt_embeds = [] all_text_ids = [] for prompt in tqdm(prompts, desc= \"Encoding prompts.\" ): ( prompt_embeds, pooled_prompt_embeds, text_ids, ) = pipeline.encode_prompt(prompt=prompt, prompt_2= None , max_sequence_length=max_sequence_length) all_prompt_embeds.append(prompt_embeds) all_pooled_prompt_embeds.append(pooled_prompt_embeds) all_text_ids.append(text_ids) max_memory = torch.cuda.max_memory_allocated() / 1024 / 1024 / 1024 print ( f\"Max memory allocated: {max_memory: .3 f} GB\" ) return all_prompt_embeds, all_pooled_prompt_embeds, all_text_ids def run ( args ): dataset = load_dataset( \"Norod78/Yarn-art-style\" , split= \"train\" ) image_prompts = {generate_image_hash(sample[ \"image\" ]): sample[ \"text\" ] for sample in dataset} all_prompts = list (image_prompts.values()) print ( f\" { len (all_prompts)=} \" ) pipeline = load_flux_dev_pipeline() all_prompt_embeds, all_pooled_prompt_embeds, all_text_ids = compute_embeddings( pipeline, all_prompts, args.max_sequence_length ) data = [] for i, (image_hash, _) in enumerate (image_prompts.items()): data.append((image_hash, all_prompt_embeds[i], all_pooled_prompt_embeds[i], all_text_ids[i])) print ( f\" { len (data)=} \" ) embedding_cols = [ \"prompt_embeds\" , \"pooled_prompt_embeds\" , \"text_ids\" ] df = pd.DataFrame(data, columns=[ \"image_hash\" ] + embedding_cols) print ( f\" { len (df)=} \" ) for col in embedding_cols: df[col] = df[col].apply( lambda x: x.cpu().numpy().flatten().tolist()) df.to_parquet(args.output_path) print ( f\"Data successfully serialized to {args.output_path} \" ) if __name__ == \"__main__\" : parser = argparse.ArgumentParser() parser.add_argument( \"--max_sequence_length\" , type = int , default=MAX_SEQ_LENGTH, help = \"Maximum sequence length to use for computing the embeddings. The more the higher computational costs.\" , ) parser.add_argument( \"--output_path\" , type = str , default=OUTPUT_PATH, help = \"Path to serialize the parquet file.\" ) args = parser.parse_args() run(args) How to use it python compute_embeddings.py \\ --max_sequence_length 77 \\ --output_path embeddings_alphonse_mucha.parquet\n\nBy combining this with cached VAE latents ( --cache_latents ) you whittle the active model down to just the quantized transformer + LoRA adapters, keeping the whole fine-tune comfortably under 10 GB of GPU memory.\n\nSetup & Results\n\nFor this demonstration, we leveraged an NVIDIA RTX 4090 (24GB VRAM) to explore its performance. The full training command using accelerate is shown below.\n\naccelerate launch --config_file=accelerate.yaml \\ train_dreambooth_lora_flux_miniature.py \\ --pretrained_model_name_or_path= \"black-forest-labs/FLUX.1-dev\" \\ --data_df_path= \"embeddings_alphonse_mucha.parquet\" \\ --output_dir= \"alphonse_mucha_lora_flux_nf4\" \\ --mixed_precision= \"bf16\" \\ --use_8bit_adam \\ --weighting_scheme= \"none\" \\ --width=512 \\ --height=768 \\ --train_batch_size=1 \\ --repeats=1 \\ --learning_rate=1e-4 \\ --guidance_scale=1 \\ --report_to= \"wandb\" \\ --gradient_accumulation_steps=4 \\ --gradient_checkpointing \\ --lr_scheduler= \"constant\" \\ --lr_warmup_steps=0 \\ --cache_latents \\ --rank=4 \\ --max_train_steps=700 \\ --seed= \"0\"\n\nConfiguration for RTX 4090: On our RTX 4090, we used a train_batch_size of 1, gradient_accumulation_steps of 4, mixed_precision=\"bf16\" , gradient_checkpointing=True , use_8bit_adam=True , a LoRA rank of 4, and resolution of 512x768. Latents were cached with cache_latents=True .\n\nMemory Footprint (RTX 4090):\n\nQLoRA: Peak VRAM usage for QLoRA fine-tuning was approximately 9GB.\n\nPeak VRAM usage for QLoRA fine-tuning was approximately 9GB. BF16 LoRA: Running standard LoRA (with the base FLUX.1-dev in FP16) on the same setup consumed 26 GB VRAM.\n\nRunning standard LoRA (with the base FLUX.1-dev in FP16) on the same setup consumed 26 GB VRAM. BF16 full finetuning: An estimate would be ~120 GB VRAM with no memory optimizations.\n\nTraining Time (RTX 4090): Fine-tuning for 700 steps on the Alphonse Mucha dataset took approximately 41 minutes on the RTX 4090 with train_batch_size of 1 and resolution of 512x768.\n\nOutput Quality: The ultimate measure is the generated art. Here are samples from our QLoRA fine-tuned model on the derekl35/alphonse-mucha-style dataset:\n\nThis table compares the primary bf16 precision results. The goal of the fine-tuning was to teach the model the distinct style of Alphonse Mucha.\n\nPrompt Base Model Output QLoRA Fine-tuned Output (Mucha Style) \"Serene raven-haired woman, moonlit lilies, swirling botanicals, alphonse mucha style\" \"a puppy in a pond, alphonse mucha style\" \"Ornate fox with a collar of autumn leaves and berries, amidst a tapestry of forest foliage, alphonse mucha style\"\n\nThe fine-tuned model nicely captured Mucha's iconic art nouveau style, evident in the decorative motifs and distinct color palette. The QLoRA process maintained excellent fidelity while learning the new style.\n\nClick to see the fp16 comparison The results are nearly identical, showing that QLoRA performs effectively with both fp16 and bf16 mixed precision. Model Comparison: Base vs. QLoRA Fine-tuned (fp16) Prompt Base Model Output QLoRA Fine-tuned Output (Mucha Style) \"Serene raven-haired woman, moonlit lilies, swirling botanicals, alphonse mucha style\" \"a puppy in a pond, alphonse mucha style\" \"Ornate fox with a collar of autumn leaves and berries, amidst a tapestry of forest foliage, alphonse mucha style\"\n\nFP8 Fine-tuning with TorchAO\n\nFor users with NVIDIA GPUs possessing compute capability 8.9 or greater (such as the H100, RTX 4090), even greater speed efficiencies can be achieved by leveraging FP8 training via the torchao library.\n\nWe fine-tuned FLUX.1-dev LoRA on an H100 SXM GPU slightly modified diffusers-torchao training scripts. The following command was used:\n\naccelerate launch train_dreambooth_lora_flux.py \\ --pretrained_model_name_or_path=black-forest-labs/FLUX.1-dev \\ --dataset_name=derekl35/alphonse-mucha-style --instance_prompt= \"a woman, alphonse mucha style\" --caption_column= \"text\" \\ --output_dir=alphonse_mucha_fp8_lora_flux \\ --mixed_precision=bf16 --use_8bit_adam \\ --weighting_scheme=none \\ --height=768 --width=512 --train_batch_size=1 --repeats=1 \\ --learning_rate=1e-4 --guidance_scale=1 --report_to=wandb \\ --gradient_accumulation_steps=1 --gradient_checkpointing \\ --lr_scheduler=constant --lr_warmup_steps=0 --rank=4 \\ --max_train_steps=700 --checkpointing_steps=600 --seed=0 \\ --do_fp8_training --push_to_hub\n\nThe training run had a peak memory usage of 36.57 GB and completed in approximately 20 minutes.\n\nQualitative results from this FP8 fine-tuned model are also available:\n\nKey steps to enable FP8 training with torchao involve:\n\nInjecting FP8 layers into the model using convert_to_float8_training from torchao.float8 . Defining a module_filter_fn to specify which modules should and should not be converted to FP8.\n\nFor a more detailed guide and code snippets, please refer to this gist and the diffusers-torchao repository.\n\nInference with Trained LoRA Adapters\n\nAfter training your LoRA adapters, you have two main approaches for inference.\n\nOption 1: Loading LoRA Adapters\n\nOne approach is to load your trained LoRA adapters on top of the base model.\n\nBenefits of Loading LoRA:\n\nFlexibility: Easily switch between different LoRA adapters without reloading the base model\n\nEasily switch between different LoRA adapters without reloading the base model Experimentation: Test multiple artistic styles or concepts by swapping adapters\n\nTest multiple artistic styles or concepts by swapping adapters Modularity: Combine multiple LoRA adapters using set_adapters() for creative blending\n\nCombine multiple LoRA adapters using for creative blending Storage efficiency: Keep a single base model and multiple small adapter files\n\nCode from diffusers import FluxPipeline, FluxTransformer2DModel, BitsAndBytesConfig import torch ckpt_id = \"black-forest-labs/FLUX.1-dev\" pipeline = FluxPipeline.from_pretrained( ckpt_id, torch_dtype=torch.float16 ) pipeline.load_lora_weights( \"derekl35/alphonse_mucha_qlora_flux\" , weight_name= \"pytorch_lora_weights.safetensors\" ) pipeline.enable_model_cpu_offload() image = pipeline( \"a puppy in a pond, alphonse mucha style\" , num_inference_steps= 28 , guidance_scale= 3.5 , height= 768 , width= 512 , generator=torch.manual_seed( 0 ) ).images[ 0 ] image.save( \"alphonse_mucha.png\" )\n\nOption 2: Merging LoRA into Base Model\n\nFor when you want maximum efficiency with a single style, you can merge the LoRA weights into the base model.\n\nBenefits of Merging LoRA:\n\nVRAM efficiency: No additional memory overhead from adapter weights during inference\n\nNo additional memory overhead from adapter weights during inference Speed: Slightly faster inference as there's no need to apply adapter computations\n\nSlightly faster inference as there's no need to apply adapter computations Quantization compatibility: Can re-quantize the merged model for maximum memory efficiency\n\nCode from diffusers import FluxPipeline, AutoPipelineForText2Image, FluxTransformer2DModel, BitsAndBytesConfig import torch ckpt_id = \"black-forest-labs/FLUX.1-dev\" pipeline = FluxPipeline.from_pretrained( ckpt_id, text_encoder= None , text_encoder_2= None , torch_dtype=torch.float16 ) pipeline.load_lora_weights( \"derekl35/alphonse_mucha_qlora_flux\" , weight_name= \"pytorch_lora_weights.safetensors\" ) pipeline.fuse_lora() pipeline.unload_lora_weights() pipeline.transformer.save_pretrained( \"fused_transformer\" ) bnb_4bit_compute_dtype = torch.bfloat16 nf4_config = BitsAndBytesConfig( load_in_4bit= True , bnb_4bit_quant_type= \"nf4\" , bnb_4bit_compute_dtype=bnb_4bit_compute_dtype, ) transformer = FluxTransformer2DModel.from_pretrained( \"fused_transformer\" , quantization_config=nf4_config, torch_dtype=bnb_4bit_compute_dtype, ) pipeline = AutoPipelineForText2Image.from_pretrained( ckpt_id, transformer=transformer, torch_dtype=bnb_4bit_compute_dtype ) pipeline.enable_model_cpu_offload() image = pipeline( \"a puppy in a pond, alphonse mucha style\" , num_inference_steps= 28 , guidance_scale= 3.5 , height= 768 , width= 512 , generator=torch.manual_seed( 0 ) ).images[ 0 ] image.save( \"alphonse_mucha_merged.png\" )\n\nRunning on Google Colab\n\nWhile we showcased results on an RTX 4090, the same code can be run on more accessible hardware like the T4 GPU available in Google Colab for free.\n\nOn a T4, you can expect the fine-tuning process to take significantly longer around 4 hours for the same number of steps. This is a trade-off for accessibility, but it makes custom fine-tuning possible without high-end hardware. Be mindful of usage limits if running on Colab, as a 4-hour training run might push them.\n\nConclusion\n\nQLoRA, coupled with the diffusers library, significantly democratizes the ability to customize state-of-the-art models like FLUX.1-dev. As demonstrated on an RTX 4090, efficient fine-tuning is well within reach, yielding high-quality stylistic adaptations. Furthermore, for users with the latest NVIDIA hardware, torchao enables even faster training through FP8 precision.\n\nShare your creations on the Hub!\n\nSharing your fine-tuned LoRA adapters is a fantastic way to contribute to the open-source community. It allows others to easily try out your styles, build on your work, and helps create a vibrant ecosystem of creative AI tools.\n\nIf you've trained a LoRA for FLUX.1-dev, we encourage you to share it. The easiest way is to add the --push_to_hub flag to the training script. Alternatively, if you have already trained a model and want to upload it, you can use the following snippet.\n\nfrom huggingface_hub import create_repo, upload_folder repo_id = \"your-username/alphonse_mucha_qlora_flux\" create_repo(repo_id, exist_ok= True ) upload_folder( repo_id=repo_id, folder_path= \"alphonse_mucha_qlora_flux\" , commit_message= \"Add Alphonse Mucha LoRA adapter\" )\n\nCheck out our Mucha LoRA and the TorchAO FP8 LoRA. You can find both, plus other adapters, in this collection.\n\nWe can't wait to see what you create!",
    "link": "https://huggingface.co/blog/flux-qlora",
    "Summary": "(LoRA) Fine-Tuning FLUX.1-dev on Consumer HardwarePublished June 19, 2025 Update on GitHubIn our previous post, Exploring Quantization Backends in Diffusers, we dived into how various quantization techniques can shrink diffusion models like FLUX.1-dev, making them significantly more accessible for inference without drastically compromising performance.\nQLoRA Fine-tuning FLUX.1-dev with DiffusersWe used a diffusers training script (slightly modified from here designed for DreamBooth-style LoRA fine-tuning of FLUX models.\nBF16 LoRA: Running standard LoRA (with the base FLUX.1-dev in FP16) on the same setup consumed 26 GB VRAM.\nOption 1: Loading LoRA AdaptersOne approach is to load your trained LoRA adapters on top of the base model.\nfrom huggingface_hub import create_repo, upload_folder repo_id = \"your-username/alphonse_mucha_qlora_flux\" create_repo(repo_id, exist_ok= True ) upload_folder( repo_id=repo_id, folder_path= \"alphonse_mucha_qlora_flux\" , commit_message= \"Add Alphonse Mucha LoRA adapter\" )Check out our Mucha LoRA and the TorchAO FP8 LoRA.",
    "Keywords": [
      "import",
      "style",
      "flux1dev",
      "qlora",
      "training",
      "consumer",
      "hardware",
      "base",
      "finetuning",
      "lora",
      "mucha",
      "model",
      "memory"
    ]
  },
  {
    "Title": "Featherless AI on Hugging Face Inference Providers \ud83d\udd25",
    "Authors": [],
    "Publish Date": null,
    "Text": "Featherless AI on Hugging Face Inference Providers \ud83d\udd25\n\nPublished June 12, 2025 Update on GitHub\n\nWe're thrilled to share that Featherless AI is now a supported Inference Provider on the Hugging Face Hub! Featherless AI joins our growing ecosystem, enhancing the breadth and capabilities of serverless inference directly on the Hub\u2019s model pages. Inference Providers are also seamlessly integrated into our client SDKs (for both JS and Python), making it super easy to use a wide variety of models with your preferred providers.\n\nFeatherless AI supports a wide variety of text and conversational models, including the latest open-source models from DeepSeek, Meta, Google, Qwen, and much more.\n\nFeatherless AI is a serverless AI inference provider with unique model loading and GPU orchestration abilities that makes an exceptionally large catalog of models available for users. Providers often offer either a low cost of access to a limited set of models, or an unlimited range of models with users managing servers and the associated costs of operation. Featherless provides the best of both worlds offering unmatched model range and variety but with serverless pricing. Find the full list of supported models on the models page.\n\nWe're super excited to see what you'll build with this new provider!\n\nRead more about how to use Featherless as an Inference Provider in its dedicated documentation page.\n\nHow it works\n\nIn the website UI\n\nIn your user account settings, you are able to:\n\nSet your own API keys for the providers you\u2019ve signed up with. If no custom key is set, your requests will be routed through HF. Learn more about request types in the docs.\n\nOrder providers by preference. This applies to the widget and code snippets in the model pages.\n\nAs mentioned, there are two modes when calling Inference Providers:\n\nCustom key (calls go directly to the inference provider, using your own API key of the corresponding inference provider)\n\nRouted by HF (in that case, you don't need a token from the provider, and the charges are applied directly to your HF account rather than the provider's account)\n\nModel pages showcase third-party inference providers (the ones that are compatible with the current model, sorted by user preference)\n\nFrom the client SDKs\n\nfrom Python, using huggingface_hub\n\nThe following example shows how to use DeepSeek-R1 using Featherless AI as the inference provider. You can use a Hugging Face token for automatic routing through Hugging Face, or your own Featherless AI API key if you have one.\n\nInstall or upgrade huggingface_hub to ensure you have version v0.33.0 or better: pip install --upgrade huggingface-hub\n\nimport os from huggingface_hub import InferenceClient client = InferenceClient( provider= \"featherless-ai\" , api_key=os.environ[ \"HF_TOKEN\" ] ) messages = [ { \"role\" : \"user\" , \"content\" : \"What is the capital of France?\" } ] completion = client.chat.completions.create( model= \"deepseek-ai/DeepSeek-R1-0528\" , messages=messages, ) print (completion.choices[ 0 ].message)\n\nfrom JS using @huggingface/inference\n\nimport { InferenceClient } from \"@huggingface/inference\" ; const client = new InferenceClient (process. env . HF_TOKEN ); const chatCompletion = await client. chatCompletion ({ model : \"deepseek-ai/DeepSeek-R1-0528\" , messages : [ { role : \"user\" , content : \"What is the capital of France?\" } ], provider : \"featherless-ai\" , }); console . log (chatCompletion. choices [ 0 ]. message );\n\nBilling\n\nFor direct requests, i.e. when you use the key from an inference provider, you are billed by the corresponding provider. For instance, if you use a Featherless AI API key you're billed on your Featherless AI account.\n\nFor routed requests, i.e. when you authenticate via the Hugging Face Hub, you'll only pay the standard provider API rates. There's no additional markup from us, we just pass through the provider costs directly. (In the future, we may establish revenue-sharing agreements with our provider partners.)\n\nImportant Note \u203c\ufe0f PRO users get $2 worth of Inference credits every month. You can use them across providers. \ud83d\udd25\n\nSubscribe to the Hugging Face PRO plan to get access to Inference credits, ZeroGPU, Spaces Dev Mode, 20x higher limits, and more.\n\nWe also provide free inference with a small quota for our signed-in free users, but please upgrade to PRO if you can!\n\nFeedback and next steps\n\nWe would love to get your feedback! Share your thoughts and/or comments here: https://huggingface.co/spaces/huggingface/HuggingDiscussions/discussions/49",
    "link": "https://huggingface.co/blog/inference-providers-featherless",
    "Summary": "Featherless AI on Hugging Face Inference Providers \ud83d\udd25Published June 12, 2025 Update on GitHubWe're thrilled to share that Featherless AI is now a supported Inference Provider on the Hugging Face Hub!\nFeatherless AI joins our growing ecosystem, enhancing the breadth and capabilities of serverless inference directly on the Hub\u2019s model pages.\nFeatherless AI is a serverless AI inference provider with unique model loading and GPU orchestration abilities that makes an exceptionally large catalog of models available for users.\nYou can use a Hugging Face token for automatic routing through Hugging Face, or your own Featherless AI API key if you have one.\nFor instance, if you use a Featherless AI API key you're billed on your Featherless AI account.",
    "Keywords": [
      "inference",
      "models",
      "featherless",
      "face",
      "hugging",
      "providers",
      "ai",
      "key",
      "provider",
      "model"
    ]
  },
  {
    "Title": "Introducing Training Cluster as a Service - a new collaboration with NVIDIA",
    "Authors": [],
    "Publish Date": null,
    "Text": "Introducing Training Cluster as a Service - a new collaboration with NVIDIA\n\nPublished June 11, 2025 Update on GitHub\n\nMaking GPU Clusters Accessible\n\nToday at GTC Paris, we are excited to announce Training Cluster as a Service in collaboration with NVIDIA, to make large GPU clusters more easily accessible for research organizations all over the world, so they can train the foundational models of tomorrow in every domain.\n\nMany Gigawatt-size GPU supercluster projects are being built to train the next gen of AI models. This can make it seem that the compute gap between the \u201cGPU poor\u201d and the \u201cGPU rich\u201d is quickly widening. But the GPUs are out there, as hyperscalers, regional and AI-native cloud providers all quickly expand their capacity.\n\nHow do we then connect AI compute capacity with the researchers who need it? How do we enable universities, national research labs and companies all over the world to build their own models?\n\nThis is what Hugging Face and NVIDIA are tackling with Training Cluster as a Service - providing GPU cluster accessibility, with the flexibility to only pay for the duration of training runs.\n\nTo get started, any of the 250,000 organizations on Hugging Face can request the GPU cluster size they need, when they need it.\n\nHow it works\n\nTo get started, you can request a GPU cluster on behalf of your organization at hf.co/training-cluster\n\nTraining Cluster as a Service integrates key components from NVIDIA and Hugging Face into a complete solution:\n\nNVIDIA Cloud Partners provide capacity for the latest NVIDIA accelerated computing like NVIDIA Hopper and NVIDIA GB200 in regional datacenters, all centralized within NVIDIA DGX Cloud\n\nNVIDIA DGX Cloud Lepton - announced today at GTC Paris - provides easy access to the infrastructure provisioned for researchers, and enables training run scheduling and monitoring\n\nHugging Face developer resources and open source libraries make it easy to get training runs started.\n\nOnce your GPU cluster request is accepted, Hugging Face and NVIDIA will collaborate to source, price, provision and set up your GPU cluster per your size, region and duration requirements.\n\nClusters at Work\n\nAdvancing Rare Genetic Disease Research with TIGEM\n\nThe Telethon Institute of Genomics and Medicine - TIGEM for short - is a research center dedicated to understanding the molecular mechanisms behind rare genetic diseases and developing novel treatments. Training new AI models is a new path to predict the effect of pathogenic variants and for drug repositioning.\n\nAI offers new ways to research the causes of rare genetic diseases and to develop treatments, but our domain requires training new models. Training Cluster as a Service made it easy to procure the GPU capacity we needed, at the right time\n\n-- Diego di Bernardo, Coordinator of the Genomic Medicine program at TIGEM\n\nAdvancing AI for Mathematics with Numina\n\nNumina is a non-profit organization building open-source, open-dataset AI for reasoning in math - and won the 2024 AIMO progress prize.\n\nWe are tracking well on our objective to build open alternatives to the best closed-source models, such as Deepmind's AlphaProof. Computing resources is our bottleneck today - with Training Cluster as a Service we will be able to reach our goal!\n\n-- Yann Fleureau, cofounder of Project Numina\n\nAdvancing Material Science with Mirror Physics\n\nMirror Physics is a startup creating frontier AI systems for chemistry and materials science.\n\nTogether with the MACE team, we're working to push the limits of AI for chemistry. With Training Cluster as a Service, we're producing high-fidelity chemical models at unprecedented scale. This is going to be a significant step forward for the field.\n\n-- Sam Walton Norwood, CEO and founder at Mirror\n\nPowering the Diversity of AI Research\n\nTraining Cluster as a Service is a new collaboration between Hugging Face and NVIDIA to make AI compute more readily available to the global community of AI researchers.\n\nAccess to large-scale, high-performance compute is essential for building the next generation of AI models across every domain and language. Training Cluster as a Service will remove barriers for researchers and companies, unlocking the ability to train the most advanced models and push the boundaries of what\u2019s possible in AI.\n\n-- Cl\u00e9ment Delangue, cofounder and CEO of Hugging Face\n\nIntegrating DGX Cloud Lepton with Hugging Face\u2019s Training Cluster as a Service gives developers and researchers a seamless way to access high-performance NVIDIA GPUs across a broad network of cloud providers. This collaboration makes it easier for AI researchers and organizations to scale their AI training workloads while using familiar tools on Hugging Face.\n\n-- Alexis Bjorlin, vice president of DGX Cloud at NVIDIA\n\nEnabling AI Builders with NVIDIA\n\nWe are excited to collaborate with NVIDIA to offer Training Cluster as a Service to Hugging Face organizations - you can get started today at hf.co/training-cluster\n\nToday at GTC Paris, NVIDIA announced many new contributions for Hugging Face users, from agents to robots!",
    "link": "https://huggingface.co/blog/nvidia-training-cluster",
    "Summary": "This is what Hugging Face and NVIDIA are tackling with Training Cluster as a Service - providing GPU cluster accessibility, with the flexibility to only pay for the duration of training runs.\nOnce your GPU cluster request is accepted, Hugging Face and NVIDIA will collaborate to source, price, provision and set up your GPU cluster per your size, region and duration requirements.\nComputing resources is our bottleneck today - with Training Cluster as a Service we will be able to reach our goal!\nWith Training Cluster as a Service, we're producing high-fidelity chemical models at unprecedented scale.\nThis collaboration makes it easier for AI researchers and organizations to scale their AI training workloads while using familiar tools on Hugging Face.",
    "Keywords": [
      "models",
      "training",
      "cloud",
      "collaboration",
      "service",
      "face",
      "hugging",
      "ai",
      "nvidia",
      "gpu",
      "cluster",
      "introducing"
    ]
  },
  {
    "link": "https://huggingface.co/blog/screensuite",
    "_error": "Article `download()` failed with 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/screensuite on URL https://huggingface.co/blog/screensuite"
  },
  {
    "link": "https://huggingface.co/blog/smolvla",
    "_error": "Article `download()` failed with 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/smolvla on URL https://huggingface.co/blog/smolvla"
  },
  {
    "link": "https://huggingface.co/blog/kv-cache",
    "_error": "Article `download()` failed with 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/kv-cache on URL https://huggingface.co/blog/kv-cache"
  },
  {
    "link": "https://huggingface.co/blog/vllm-colocate",
    "_error": "Article `download()` failed with 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/vllm-colocate on URL https://huggingface.co/blog/vllm-colocate"
  },
  {
    "link": "https://huggingface.co/blog/structured-codeagent",
    "_error": "Article `download()` failed with 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/structured-codeagent on URL https://huggingface.co/blog/structured-codeagent"
  },
  {
    "link": "https://huggingface.co/blog/liger-grpo",
    "_error": "Article `download()` failed with 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/liger-grpo on URL https://huggingface.co/blog/liger-grpo"
  },
  {
    "link": "https://huggingface.co/blog/dell-ai-applications",
    "_error": "Article `download()` failed with 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/dell-ai-applications on URL https://huggingface.co/blog/dell-ai-applications"
  },
  {
    "link": "https://huggingface.co/blog/python-tiny-agents",
    "_error": "Article `download()` failed with 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/python-tiny-agents on URL https://huggingface.co/blog/python-tiny-agents"
  },
  {
    "link": "https://huggingface.co/blog/diffusers-quantization",
    "_error": "Article `download()` failed with 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/diffusers-quantization on URL https://huggingface.co/blog/diffusers-quantization"
  },
  {
    "link": "https://huggingface.co/blog/nanovlm",
    "_error": "Article `download()` failed with 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/nanovlm on URL https://huggingface.co/blog/nanovlm"
  },
  {
    "link": "https://huggingface.co/blog/transformers-model-definition",
    "_error": "Article `download()` failed with 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/transformers-model-definition on URL https://huggingface.co/blog/transformers-model-definition"
  },
  {
    "link": "https://huggingface.co/blog/azure-ai-foundry",
    "_error": "Article `download()` failed with 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/azure-ai-foundry on URL https://huggingface.co/blog/azure-ai-foundry"
  },
  {
    "link": "https://huggingface.co/blog/kaggle-integration",
    "_error": "Article `download()` failed with 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/kaggle-integration on URL https://huggingface.co/blog/kaggle-integration"
  },
  {
    "link": "https://huggingface.co/blog/fast-whisper-endpoints",
    "_error": "Article `download()` failed with 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/fast-whisper-endpoints on URL https://huggingface.co/blog/fast-whisper-endpoints"
  },
  {
    "link": "https://huggingface.co/blog/vlms-2025",
    "_error": "Article `download()` failed with 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/vlms-2025 on URL https://huggingface.co/blog/vlms-2025"
  }
]